# Data Engineer Collaboration Model
# How this skill works with other AI memory specialists

prerequisites:
  skills: []
  knowledge:
    - "SQL fundamentals (SELECT, JOIN, GROUP BY)"
    - "Understanding of ETL concepts"
    - "Basic Python or similar scripting"
    - "Understanding of data formats (JSON, CSV, Parquet)"

complementary_skills:
  - skill: postgres-wizard
    relationship: "Database optimization"
    brings: "Query optimization, indexing, source database tuning"

  - skill: event-architect
    relationship: "Streaming data"
    brings: "Kafka/NATS design, event schema, CDC setup"

  - skill: ml-memory
    relationship: "Memory data pipelines"
    brings: "Memory consolidation pipelines, retrieval optimization"

  - skill: observability-sre
    relationship: "Pipeline monitoring"
    brings: "Metrics, alerting, pipeline SLOs"

  - skill: infra-architect
    relationship: "Pipeline infrastructure"
    brings: "Kubernetes, Airflow/Dagster deployment"

  - skill: migration-specialist
    relationship: "Data migrations"
    brings: "Schema migration, data backfill strategies"

delegation:
  - trigger: "source database optimization"
    delegate_to: postgres-wizard
    pattern: parallel
    context: "Query patterns and data volumes"
    receive: "Index recommendations, query optimization"

  - trigger: "need streaming architecture"
    delegate_to: event-architect
    pattern: sequential
    context: "Data flow requirements and volumes"
    receive: "Kafka/NATS design and CDC setup"

  - trigger: "memory consolidation pipeline"
    delegate_to: ml-memory
    pattern: parallel
    context: "Memory data and consolidation rules"
    receive: "Consolidation logic and memory schemas"

  - trigger: "pipeline monitoring"
    delegate_to: observability-sre
    pattern: parallel
    context: "Pipeline metrics and failure modes"
    receive: "Dashboards and alerting"

  - trigger: "pipeline infrastructure"
    delegate_to: infra-architect
    pattern: parallel
    context: "Compute and storage requirements"
    receive: "Kubernetes manifests, orchestrator setup"

  - trigger: "schema migration"
    delegate_to: migration-specialist
    pattern: sequential
    context: "Current and target schemas"
    receive: "Migration plan and rollback strategy"

collaboration_patterns:
  sequential:
    - "event-architect designs CDC, then I build transformation pipeline"
    - "I design schema, then migration-specialist plans deployment"
    - "postgres-wizard optimizes source, then I tune extraction"

  parallel:
    - "I build pipeline while observability-sre sets up monitoring"
    - "I transform data while infra-architect provisions infrastructure"

  review:
    - "Review ml-memory's consolidation logic for pipeline integration"
    - "Review migration-specialist's schema changes for pipeline impact"
    - "Review event-architect's event schema for transformation needs"

cross_domain_insights:
  - domain: distributed-systems
    insight: "CAP theorem: partition tolerance is not optional"
    applies_when: "Designing pipeline failure handling"

  - domain: databases
    insight: "Indexes speed reads but slow writes"
    applies_when: "Optimizing sink table design"

  - domain: batch-processing
    insight: "Batch size trades latency for throughput"
    applies_when: "Tuning batch pipeline performance"

  - domain: streaming
    insight: "Backpressure prevents cascading failures"
    applies_when: "Handling producer faster than consumer"

  - domain: testing
    insight: "Test with production-scale data, not toy datasets"
    applies_when: "Validating pipeline before deployment"

ecosystem:
  primary_tools:
    - "dbt - SQL transformation framework"
    - "Airflow/Dagster - Pipeline orchestration"
    - "Debezium - Change Data Capture"
    - "Great Expectations - Data quality"
    - "Spark/Flink - Distributed processing"

  alternatives:
    - name: Prefect
      use_when: "Python-first, simpler than Airflow"
      avoid_when: "Need Airflow's maturity and ecosystem"

    - name: Fivetran
      use_when: "Need managed EL without custom code"
      avoid_when: "Custom transformations needed"

    - name: dlt
      use_when: "Python-native data loading, schema evolution"
      avoid_when: "dbt expertise already exists"

    - name: Polars
      use_when: "Need fast single-machine processing"
      avoid_when: "Data exceeds single machine memory"

    - name: DuckDB
      use_when: "Analytical queries on local files"
      avoid_when: "Need distributed processing"

  deprecated:
    - "Polling for changes (use CDC)"
    - "Silent exception handling"
    - "Non-idempotent pipelines"
    - "Processing time windows for late data"
    - "Hardcoded connection strings"
