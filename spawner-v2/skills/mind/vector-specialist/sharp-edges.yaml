# Vector Specialist Sharp Edges
# Battle scars from building embedding and retrieval systems at scale

sharp_edges:
  - id: embedding-model-mismatch
    summary: Query and document embedded with different models
    severity: critical
    situation: |
      You upgrade your embedding model for new documents but don't re-embed
      existing ones. Or you use a different model for query vs document.
      Search quality tanks.
    why: |
      Each embedding model defines its own vector space. Vectors from different
      models are incompatible - cosine similarity between them is meaningless.
      It's like comparing meters to pounds.
    solution: |
      # Rule 1: Same model for query and documents
      EMBEDDING_MODEL = "text-embedding-3-small"

      async def embed_query(query: str) -> List[float]:
          return await embed(query, model=EMBEDDING_MODEL)

      async def embed_document(doc: str) -> List[float]:
          return await embed(doc, model=EMBEDDING_MODEL)

      # Rule 2: Track model version with embeddings
      async def store_embedding(doc_id: str, content: str):
          embedding = await embed_document(content)
          await db.execute(
              """
              INSERT INTO embeddings (doc_id, vector, model_version)
              VALUES ($1, $2, $3)
              """,
              doc_id, embedding, EMBEDDING_MODEL
          )

      # Rule 3: Re-embed everything when upgrading
      async def migrate_embeddings(old_model: str, new_model: str):
          docs = await db.fetch("SELECT * FROM embeddings WHERE model_version = $1", old_model)
          for doc in docs:
              new_embedding = await embed(doc.content, model=new_model)
              await update_embedding(doc.id, new_embedding, new_model)
    symptoms:
      - "Search quality suddenly drops"
      - "New documents rank differently than old"
      - "Same query returns different results over time"
      - "Embeddings have different dimensions"
    detection_pattern: 'embed.*model.*!=|different.*model'
    version_range: ">=1.0.0"

  - id: qdrant-hnsw-build-time
    summary: HNSW build parameters dramatically affect index creation time
    severity: high
    situation: |
      You configure Qdrant with high-quality HNSW settings for better recall.
      Initial load of 1M vectors takes 12 hours instead of expected 30 minutes.
    why: |
      HNSW parameters (m, ef_construct) trade build time for search quality.
      High values mean each vector connects to many neighbors during build.
      With m=32 and ef_construct=400, index build is 10x slower than defaults.
    solution: |
      # Balance quality vs build time
      # Defaults: m=16, ef_construct=100
      # High quality: m=32, ef_construct=200 (4x slower build)
      # Max quality: m=64, ef_construct=400 (10x slower build)

      from qdrant_client.models import VectorParams, HnswConfigDiff

      # For initial load: use fast settings
      client.create_collection(
          collection_name="memories",
          vectors_config=VectorParams(
              size=1536,
              distance=Distance.COSINE,
          ),
          hnsw_config=HnswConfigDiff(
              m=16,
              ef_construct=100,
          )
      )

      # After load: optimize for quality
      client.update_collection(
          collection_name="memories",
          hnsw_config=HnswConfigDiff(
              m=32,
              ef_construct=200,
          )
      )
      # This triggers re-indexing in background

      # For queries: tune ef (search quality)
      client.search(
          collection_name="memories",
          query_vector=query_vec,
          limit=10,
          search_params=SearchParams(hnsw_ef=128)  # Higher = better recall
      )
    symptoms:
      - "Index build takes hours instead of minutes"
      - "Memory spikes during indexing"
      - "Initial data load blocks service"
      - "Build time grows non-linearly with data"
    detection_pattern: 'm=.*(?:32|64)|ef_construct=.*(?:200|400)'
    version_range: ">=1.0.0"

  - id: pgvector-hnsw-disk-space
    summary: pgvector HNSW indexes can be larger than the data
    severity: high
    situation: |
      You add an HNSW index to your pgvector table. Disk usage triples.
      Database backups fail due to size limits.
    why: |
      HNSW index stores graph structure on top of vectors. With 1536-dimension
      vectors and default m=16, index is often 2-3x the size of the vector data.
      Nobody warns you until your disk fills.
    solution: |
      # 1. Estimate size before creating index
      # Rule of thumb: index_size ≈ vectors * dimensions * 4 * m / 8
      # 1M vectors * 1536 dims * 4 bytes * 16 / 8 ≈ 12 GB index

      # 2. Consider ivfflat for smaller index (but slower queries)
      CREATE INDEX ON memories USING ivfflat (embedding vector_cosine_ops)
      WITH (lists = 1000);  # sqrt(n) to 2*sqrt(n) lists

      # 3. Use pgvectorscale's DiskANN for large scale
      CREATE INDEX ON memories USING diskann (embedding);

      # 4. Monitor index size
      SELECT
          pg_size_pretty(pg_relation_size('memories_embedding_idx')) as index_size,
          pg_size_pretty(pg_table_size('memories')) as table_size;

      # 5. Set maintenance_work_mem for faster builds
      SET maintenance_work_mem = '2GB';
    symptoms:
      - "Disk usage 3x expected"
      - "Index creation runs for hours"
      - "Backups exceed size limits"
      - "VACUUM takes forever"
    detection_pattern: 'USING hnsw'
    version_range: ">=1.0.0"

  - id: quantization-recall-loss
    summary: Scalar/binary quantization silently degrades recall
    severity: high
    situation: |
      You enable scalar quantization to save memory. Everything seems fine.
      Months later, users complain search quality is bad. You've been returning
      suboptimal results the whole time.
    why: |
      Quantization compresses vectors, losing precision. Scalar quantization
      loses 1-5% recall. Binary quantization can lose 5-15%. If you don't
      measure before/after, you won't notice until users complain.
    solution: |
      # ALWAYS measure recall before enabling quantization

      async def measure_recall(
          queries: List[str],
          ground_truth: Dict[str, List[str]],
          top_k: int = 10
      ) -> float:
          hits = 0
          total = 0

          for query in queries:
              results = await search(query, limit=top_k)
              result_ids = [r.id for r in results]
              true_ids = ground_truth[query][:top_k]

              hits += len(set(result_ids) & set(true_ids))
              total += len(true_ids)

          return hits / total

      # Before quantization
      baseline_recall = await measure_recall(test_queries, ground_truth)
      print(f"Baseline recall@10: {baseline_recall:.3f}")

      # Enable quantization
      client.update_collection(
          collection_name="memories",
          quantization_config=ScalarQuantization(
              scalar=ScalarQuantizationConfig(type=ScalarType.INT8)
          )
      )

      # Measure again
      quantized_recall = await measure_recall(test_queries, ground_truth)
      print(f"Quantized recall@10: {quantized_recall:.3f}")
      print(f"Recall loss: {baseline_recall - quantized_recall:.3f}")

      # Accept or reject based on requirements
      if baseline_recall - quantized_recall > 0.05:
          print("WARNING: >5% recall loss, reconsider quantization")
    symptoms:
      - "Search quality subjectively worse"
      - "Relevant documents not in top results"
      - "Memory savings seem free (they're not)"
      - "No baseline measurements exist"
    detection_pattern: 'ScalarQuantization|BinaryQuantization'
    version_range: ">=1.0.0"

  - id: chunking-too-large
    summary: Embedding entire documents loses specific information
    severity: high
    situation: |
      You embed full documents (2000+ tokens). Search returns the right document
      but users can't find the specific answer in it. "It says it's relevant
      but I can't find the part I need."
    why: |
      Long text embedding averages meaning across the whole document. Specific
      facts get diluted. A 2000-token document about many topics has a generic
      embedding that matches nothing specifically.
    solution: |
      # Optimal chunk size: 256-512 tokens
      # With 10-20% overlap to preserve context

      def chunk_document(text: str, chunk_size: int = 400, overlap: int = 50):
          tokens = tokenizer.encode(text)
          chunks = []

          start = 0
          while start < len(tokens):
              end = start + chunk_size
              chunk_tokens = tokens[start:end]
              chunk_text = tokenizer.decode(chunk_tokens)

              chunks.append({
                  "text": chunk_text,
                  "start_idx": start,
                  "end_idx": end,
                  "doc_id": doc_id
              })

              start = end - overlap  # Overlap for context

          return chunks

      # Store chunk relationship to parent document
      for chunk in chunk_document(document.text):
          embedding = await embed(chunk["text"])
          await store_chunk(
              parent_id=document.id,
              chunk_text=chunk["text"],
              embedding=embedding,
              position=chunk["start_idx"]
          )

      # Return parent doc with highlighted chunk
      async def search_with_context(query: str):
          chunks = await search_chunks(query)
          results = []
          for chunk in chunks:
              parent = await get_document(chunk.parent_id)
              results.append({
                  "document": parent,
                  "relevant_chunk": chunk.text,
                  "chunk_position": chunk.position
              })
          return results
    symptoms:
      - "Right document, wrong section"
      - "User can't find the answer in result"
      - "Long documents rank but aren't useful"
      - "Queries for specific facts fail"
    detection_pattern: 'chunk_size.*(?:1000|2000)|embed.*full.*doc'
    version_range: ">=1.0.0"

  - id: no-reranking
    summary: First-stage retrieval quality treated as final
    severity: high
    situation: |
      You return vector search results directly to users. Quality is "okay" but
      not great. Obviously relevant documents are sometimes ranked 5th or 6th.
    why: |
      First-stage retrieval (vector search, BM25) optimizes for speed, not
      precision. It finds candidates quickly but ranks them roughly. Cross-encoder
      reranking is 10x slower but 2x more accurate.
    solution: |
      from sentence_transformers import CrossEncoder

      reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

      async def search_with_rerank(
          query: str,
          limit: int = 10,
          rerank_top: int = 50  # Rerank more than you return
      ) -> List[Memory]:
          # Stage 1: Fast retrieval
          candidates = await vector_search(query, limit=rerank_top)

          # Stage 2: Cross-encoder reranking
          pairs = [(query, c.content) for c in candidates]
          scores = reranker.predict(pairs)

          # Sort by reranker scores
          ranked = sorted(
              zip(candidates, scores),
              key=lambda x: x[1],
              reverse=True
          )

          return [c for c, score in ranked[:limit]]

      # For even better quality: LLM reranking
      async def llm_rerank(query: str, candidates: List[Memory]) -> List[Memory]:
          prompt = f"""Given the query: "{query}"

          Rank these documents by relevance (most relevant first):
          {format_candidates(candidates)}

          Return just the document IDs in order of relevance."""

          response = await llm.complete(prompt)
          ordered_ids = parse_ids(response)
          return [c for id in ordered_ids for c in candidates if c.id == id]
    symptoms:
      - "Relevant results ranked 5th-10th"
      - "Quality good enough but not great"
      - "Simple queries work, complex fail"
      - "Precision@1 is low"
    detection_pattern: 'return.*search.*results(?!.*rerank)'
    version_range: ">=1.0.0"

  - id: embedding-api-costs
    summary: Embedding API costs explode with naive implementation
    severity: medium
    situation: |
      You embed documents with OpenAI API on every request. Monthly bill is
      $5000 when you expected $500. Same content is being re-embedded.
    why: |
      Embedding APIs charge per token. Re-embedding the same content wastes
      money. Without caching and batching, costs grow linearly with requests
      instead of with unique content.
    solution: |
      # 1. Cache embeddings by content hash
      async def get_embedding(text: str) -> List[float]:
          cache_key = f"emb:{sha256(text)}"
          cached = await redis.get(cache_key)
          if cached:
              return json.loads(cached)

          embedding = await openai.embed(text)
          await redis.setex(cache_key, 86400, json.dumps(embedding))
          return embedding

      # 2. Batch embedding requests
      async def embed_batch(texts: List[str]) -> List[List[float]]:
          # Check cache for each
          results = []
          to_embed = []
          to_embed_indices = []

          for i, text in enumerate(texts):
              cached = await redis.get(f"emb:{sha256(text)}")
              if cached:
                  results.append((i, json.loads(cached)))
              else:
                  to_embed.append(text)
                  to_embed_indices.append(i)

          # Batch API call for uncached
          if to_embed:
              new_embeddings = await openai.embed(to_embed)  # Single API call
              for idx, emb in zip(to_embed_indices, new_embeddings):
                  results.append((idx, emb))
                  await redis.setex(f"emb:{sha256(texts[idx])}", 86400, json.dumps(emb))

          # Return in original order
          results.sort(key=lambda x: x[0])
          return [emb for _, emb in results]

      # 3. Use smaller models for appropriate tasks
      # text-embedding-3-small: 62.3K tokens/$1
      # text-embedding-3-large: 9.6K tokens/$1
    symptoms:
      - "API costs higher than expected"
      - "Same text embedded repeatedly"
      - "No caching layer exists"
      - "One API call per document"
    detection_pattern: 'await.*embed\\((?!.*batch|.*cache)'
    version_range: ">=1.0.0"

  - id: semantic-gap
    summary: User queries use different words than documents
    severity: medium
    situation: |
      Documents say "hypertension treatment." Users search "high blood pressure
      medicine." Semantic similarity is lower than expected because vocabulary
      differs.
    why: |
      Embedding models learn from training data vocabulary. If your domain uses
      specific terminology but users don't, there's a semantic gap. The model
      may not know "hypertension" = "high blood pressure" strongly enough.
    solution: |
      # Strategy 1: Query expansion with synonyms
      async def expand_query(query: str) -> str:
          expanded = await llm.complete(
              f"""Rewrite this search query with synonyms and related terms.
              Original: {query}
              Expanded (include original terms plus synonyms):"""
          )
          return expanded

      # Strategy 2: Hybrid retrieval (vectors + keywords)
      async def hybrid_search(query: str) -> List[Memory]:
          # Vector search catches semantic similarity
          vector_results = await vector_search(query)

          # Keyword search catches exact terminology
          keyword_results = await bm25_search(query)

          # Combine with RRF
          return reciprocal_rank_fusion([vector_results, keyword_results])

      # Strategy 3: Domain fine-tuning (expensive but effective)
      # Fine-tune embedding model on domain-specific pairs
      training_pairs = [
          ("hypertension", "high blood pressure"),
          ("MI", "heart attack"),
          # ... domain synonyms
      ]

      # Strategy 4: Query understanding
      async def understand_query(query: str) -> Dict:
          return await llm.complete(
              f"""Extract from this query:
              - entities: specific things mentioned
              - intent: what the user wants
              - synonyms: other ways to say this
              Query: {query}"""
          )
    symptoms:
      - "Relevant documents not found"
      - "Domain terms not matching"
      - "General queries work, specific don't"
      - "Users complain 'I know it exists'"
    detection_pattern: null
    version_range: ">=1.0.0"

  - id: cold-cache-thundering-herd
    summary: All caches expire together, API gets hammered
    severity: medium
    situation: |
      You cache embeddings with same TTL. After restart or TTL expiry, all
      requests hit the embedding API simultaneously. Rate limits exceeded,
      requests fail.
    why: |
      Uniform TTL means synchronized expiry. When cache is cold (restart, expiry),
      every request triggers API call. This creates thundering herd that exceeds
      rate limits.
    solution: |
      # Strategy 1: Jittered TTL
      import random

      async def cache_embedding(key: str, embedding: List[float], base_ttl: int = 3600):
          # Add random jitter (±10%)
          jitter = int(base_ttl * 0.1 * (random.random() * 2 - 1))
          ttl = base_ttl + jitter
          await redis.setex(key, ttl, json.dumps(embedding))

      # Strategy 2: Background refresh before expiry
      async def get_embedding_with_refresh(text: str) -> List[float]:
          cache_key = f"emb:{sha256(text)}"
          result = await redis.get(cache_key)

          if result:
              ttl = await redis.ttl(cache_key)
              if ttl < 300:  # Refresh if < 5 min left
                  asyncio.create_task(refresh_embedding(text, cache_key))
              return json.loads(result)

          # Cache miss - fetch and store
          return await fetch_and_cache_embedding(text, cache_key)

      # Strategy 3: Rate limiting on API calls
      from asyncio import Semaphore

      embedding_semaphore = Semaphore(10)  # Max 10 concurrent calls

      async def rate_limited_embed(text: str) -> List[float]:
          async with embedding_semaphore:
              return await openai.embed(text)
    symptoms:
      - "API rate limits hit after restart"
      - "Bursts of embedding requests"
      - "All caches expire at once"
      - "Latency spikes periodically"
    detection_pattern: 'setex.*\\d+\\)'
    version_range: ">=1.0.0"
