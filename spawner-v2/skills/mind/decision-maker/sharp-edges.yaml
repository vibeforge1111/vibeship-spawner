# Decision Maker Sharp Edges
# Real gotchas from technical decision-making - the traps that delay projects and create regret

sharp_edges:
  - id: sunk-cost-trap
    title: The Sunk Cost Trap - "We've Already Invested So Much"
    severity: critical
    situation: |
      Team has spent 3 months on an approach. New information suggests it won't
      work. But the investment feels too large to abandon. Team doubles down,
      spending 6 more months before finally admitting failure.
    why: |
      Past investment is irrelevant to future decisions - it's already spent.
      But human psychology weighs it heavily. We feel we must "justify" past
      work by continuing, even when stopping is objectively better.
    solution: |
      1. Separate evaluation from investment:
         - "Ignore what we've built. If starting fresh today, what would we do?"
         - If answer differs from current path, current path is wrong

      2. Set kill criteria upfront:
         - "If X doesn't work by date Y, we stop"
         - Decide criteria before emotional investment builds

      3. Celebrate pivots, not persistence:
         - "We learned X didn't work and pivoted" = success
         - "We spent 9 months on X" = failure

      4. Reframe the investment:
         - Past work gave you information
         - That information says "stop"
         - Ignoring the lesson wastes the investment twice
    symptoms:
      - "We've come too far to turn back"
      - "Just a little more and it'll work"
      - Increasing investment with decreasing confidence
      - Team knows it's wrong but can't say it
    detection_pattern: "already invested|too far|just need|almost there|so much time"

  - id: reversibility-illusion
    title: The Reversibility Illusion - "We Can Always Change It Later"
    severity: critical
    situation: |
      Team chooses database, architecture, or API design assuming it's reversible.
      Two years later, with 500k lines of code depending on it, changing would
      require 6-month rewrite. Decision was actually one-way, not two-way.
    why: |
      Reversibility depends on what gets built on top. A foundation choice is
      reversible before the building exists, not after. Teams evaluate
      reversibility at decision time, ignoring future dependencies.
    solution: |
      1. Ask: "What will depend on this?"
         - Will other code call this API?
         - Will data be stored in this format?
         - Will teams build expertise around this?

      2. Dependency accumulation test:
         - 0 dependencies: Two-way door
         - 10 dependencies: Questionable
         - 100 dependencies: One-way door

      3. Time-lock evaluation:
         - "Could we reverse this in 6 months?"
         - "Could we reverse in 2 years with 10x code?"

      4. When in doubt, treat as one-way:
         - Extra analysis is cheap
         - Wrong architecture is expensive
    symptoms:
      - "We can refactor later"
      - New code building on the decision daily
      - Integration complexity increasing
      - Original authors have left
    detection_pattern: "refactor later|change if needed|not locked in|temporary"

  - id: consensus-paralysis
    title: Consensus Paralysis - Everyone Must Agree
    severity: high
    situation: |
      Team needs to choose a monitoring tool. Six people have opinions.
      Three months of "alignment meetings" later, still no decision.
      Meanwhile, production issues go unmonitored.
    why: |
      Consensus feels safe - no one can blame you if everyone agreed. But
      for two-way doors, consensus is overkill. The cost of deciding wrong
      is lower than the cost of not deciding at all.
    solution: |
      1. Match process to door type:
         - Two-way door: Individual or small team decides
         - One-way door: Broader input, but still single decision maker

      2. Assign decision owner:
         - "Person X decides by date Y"
         - Owner gathers input but doesn't need agreement
         - Others commit to supporting the decision

      3. Set default if no decision:
         - "If we can't decide by Friday, we use Option A"
         - Prevents indefinite delay

      4. Disagree and commit protocol:
         - Record disagreements for learning
         - But still ship the decision
    symptoms:
      - "Let's get more stakeholder input"
      - Meetings about meetings
      - Decision punted to next sprint repeatedly
      - Everyone waiting for someone else
    detection_pattern: "need alignment|get buy-in|stakeholder|more input|consensus"

  - id: analysis-perfectionism
    title: Analysis Perfectionism - "Need More Data"
    severity: high
    situation: |
      Team evaluating cloud providers. Create 50-page comparison doc. Still
      uncertain. Request more benchmarks. Run pilots. Three months pass.
      Competitors launched while you analyzed.
    why: |
      More information feels like risk reduction. But information has
      diminishing returns. The 10th hour of research rarely changes the
      decision that was clear after 2 hours.
    solution: |
      1. Timebox research:
         - Two-way door: 2-4 hours max
         - One-way door: 2-4 days max
         - After timebox, decide with available info

      2. Define "good enough":
         - What confidence level is needed?
         - 80%? 90%? 100% is impossible
         - Usually 80% is sufficient for two-way doors

      3. Identify the actual question:
         - Often "more research" means unclear criteria
         - Define what you'd need to know to decide
         - If you can't define it, you have enough

      4. Cost of delay:
         - Every week of delay has a cost
         - Compare research value to delay cost
    symptoms:
      - "Just one more benchmark"
      - Analysis doc growing but confidence not
      - Same options discussed repeatedly
      - Fear of making the wrong choice
    detection_pattern: "more research|more data|not sure yet|need benchmark|what if"

  - id: hippo-override
    title: The HiPPO Override - Highest Paid Person's Opinion Wins
    severity: high
    situation: |
      Team of experts analyzes options. Recommends Option B with clear
      rationale. VP walks in: "We're doing Option A." No discussion.
      Team demotivated, but complies. Option A fails predictably.
    why: |
      Hierarchy feels efficient - less debate, faster decisions. But
      senior people are often furthest from the code. Their intuition
      may be outdated. Overriding experts wastes their analysis.
    solution: |
      1. Separate input from decision rights:
         - Senior: Context, constraints, priorities
         - Technical team: Technical recommendation
         - Clear who decides what

      2. Make overrides explicit and documented:
         - "I'm overriding the team recommendation because X"
         - Forces articulation of reasoning
         - Creates accountability

      3. Review override outcomes:
         - Track when HiPPO overrides experts
         - Measure outcomes vs expert recommendations
         - Data helps calibrate future trust

      4. Escalation protocol:
         - If technical team strongly disagrees, they escalate
         - Not "comply silently then complain later"
    symptoms:
      - Decision changes after exec joins meeting
      - Team recommendation not in final decision doc
      - "Leadership decided" without rationale
      - Experts stop giving real opinions
    detection_pattern: "leadership decided|exec wants|from the top|just do it"

  - id: local-optimization
    title: Local Optimization - Winning the Battle, Losing the War
    severity: high
    situation: |
      Team optimizes their service for performance. Adds caching layer.
      Now every consuming team must handle cache invalidation. Total
      system complexity increased, even though one service improved.
    why: |
      Teams optimize for their metrics. But decisions have cross-team
      effects. A locally optimal choice can be globally suboptimal.
      No one owns the system-wide view.
    solution: |
      1. Map affected parties:
         - Who consumes this service/API?
         - Who depends on this behavior?
         - Include them in decision

      2. Consider total cost of ownership:
         - Your team: +1 complexity
         - Each consumer: +0.5 complexity
         - 10 consumers = +5 total complexity
         - Your "improvement" is a net negative

      3. Prefer pushing complexity down:
         - Better to have 1 complex service
         - Than 10 consumers handling complexity
         - Centralize where possible

      4. Document cross-team impacts in ADR:
         - "This affects team X, Y, Z"
         - "They accept the trade-off because..."
    symptoms:
      - Consumers complaining about your "improvement"
      - Integration bugs after your change
      - Other teams working around your design
      - Finger pointing about who caused complexity
    detection_pattern: "works for us|their problem|downstream|integration"

  - id: technology-fascination
    title: Technology Fascination - "New is Better"
    severity: medium
    situation: |
      Team proposes Kubernetes for 3-person startup with one service.
      Or GraphQL for internal tool with 2 consumers. Or microservices
      for MVP. Technology chosen for resume, not problem.
    why: |
      Engineers love learning. New tech is exciting. But new tech has
      learning curves, immature ecosystems, unknown failure modes.
      The boring choice often ships faster and fails less.
    solution: |
      1. Problem-first evaluation:
         - What problem are we solving?
         - Does current tech solve it?
         - What does new tech add?

      2. Team capability check:
         - Does anyone know this tech?
         - How long to become proficient?
         - Is learning cost justified?

      3. Boring technology principle:
         - Use well-understood tech by default
         - New tech needs explicit justification
         - "It's cool" is not justification

      4. Innovation tokens:
         - Team gets 2-3 "new tech" choices per project
         - Everything else must be boring
         - Forces prioritization
    symptoms:
      - "All the cool companies use X"
      - Tech chosen before problem defined
      - Learning curve causing delays
      - Team excited about tech, not product
    detection_pattern: "everyone uses|latest|modern|cutting edge|exciting"

  - id: false-urgency
    title: False Urgency - "We Need to Decide Now"
    severity: medium
    situation: |
      Someone declares decision is urgent. Team rushes, makes suboptimal
      choice. Later realize they had weeks, not hours. Urgency was
      artificial or misunderstood.
    why: |
      Urgency bypasses analysis. It feels decisive and action-oriented.
      But many "urgent" decisions aren't. Real urgency is rare -
      production down, legal deadline, customer waiting.
    solution: |
      1. Question urgency:
         - "What happens if we decide next week?"
         - "What deadline are we hitting?"
         - "Who is waiting on this?"

      2. Distinguish urgency types:
         - External deadline: Real urgency
         - Self-imposed deadline: Artificial
         - "We should just decide": Not urgent

      3. Fast decision â‰  rushed decision:
         - Urgent decisions still need clear criteria
         - Timebox analysis, don't skip it
         - 30 minutes of thought beats 0

      4. Create breathing room:
         - "Can we get an extra day?"
         - Often yes, no one asked
    symptoms:
      - "We need this yesterday"
      - No actual deadline when pressed
      - Urgency from one person, not situation
      - Post-decision regret
    detection_pattern: "urgent|asap|right now|can't wait|immediately"

  - id: decision-scope-creep
    title: Decision Scope Creep - "While We're At It"
    severity: medium
    situation: |
      Team deciding on logging library. Conversation drifts to
      "standardize all observability." Then "define platform strategy."
      Original simple decision becomes 6-month initiative.
    why: |
      Related decisions feel connected. It seems efficient to solve them
      together. But scope expansion delays all decisions. Small decisions
      get blocked by big strategy that never finishes.
    solution: |
      1. Define decision scope upfront:
         - "We are deciding X"
         - "We are NOT deciding Y, Z"
         - Write it down

      2. Parking lot related items:
         - "Good point, that's a separate decision"
         - Log it for later
         - Don't derail current decision

      3. Decide what you can now:
         - Make the small decision
         - Don't let perfect strategy block good progress
         - Strategy can change; make it changeable

      4. Timebox discussions:
         - "We have 30 min for this decision"
         - Scope creep becomes obvious when time runs out
    symptoms:
      - Meeting scope expanded since invite
      - "Related" topics consuming time
      - Simple decisions taking weeks
      - Original question forgotten
    detection_pattern: "while we're at it|related|also should|bigger picture"

  - id: decision-debt
    title: Decision Debt - "We'll Figure It Out Later"
    severity: medium
    situation: |
      Team defers hard decisions to "later." Later never comes. Implicit
      decisions are made by default - whoever codes first wins. System
      grows without coherent design.
    why: |
      Hard decisions are uncomfortable. Deferring feels like progress -
      we shipped! But undecided things don't stay undecided. They become
      implicit decisions with worse outcomes.
    solution: |
      1. Name deferred decisions:
         - "We are explicitly deferring X"
         - "We will decide by date Y"
         - "Default until then is Z"

      2. Track decision debt:
         - List of deferred decisions
         - Review regularly
         - Don't accumulate too many

      3. Decide or explicitly delegate:
         - "First implementer decides"
         - Better than implicit chaos
         - At least someone owns it

      4. Time-bound deferrals:
         - "We'll revisit after MVP"
         - If not revisited by date, default wins
    symptoms:
      - "We haven't decided that yet"
      - Different team members assume different answers
      - Inconsistent implementations
      - "I thought we were doing X?"
    detection_pattern: "figure out later|TBD|haven't decided|defer|revisit"

