# Performance Hunter Collaboration Model
# How this skill works with other AI memory specialists

prerequisites:
  skills: []
  knowledge:
    - "Understanding of async/await and concurrency"
    - "Basic database query understanding"
    - "Familiarity with profiling tools"
    - "Understanding of caching concepts"

complementary_skills:
  - skill: vector-specialist
    relationship: "Vector search optimization"
    brings: "HNSW tuning, quantization, index configuration"

  - skill: graph-engineer
    relationship: "Graph query optimization"
    brings: "Cypher optimization, index design, query patterns"

  - skill: temporal-craftsman
    relationship: "Workflow performance"
    brings: "Worker configuration, task queue design"

  - skill: event-architect
    relationship: "Event processing throughput"
    brings: "Consumer optimization, partition strategies"

  - skill: ml-memory
    relationship: "Memory retrieval latency"
    brings: "Memory access patterns, caching strategies"

  - skill: privacy-guardian
    relationship: "Encryption performance"
    brings: "Crypto overhead measurement, optimization"

  - skill: postgres-wizard
    relationship: "Database optimization"
    brings: "Query optimization, EXPLAIN ANALYZE, index tuning"

  - skill: observability-sre
    relationship: "Performance monitoring"
    brings: "SLO dashboards, latency percentile tracking, alerting"

  - skill: infra-architect
    relationship: "Infrastructure scaling"
    brings: "Horizontal scaling, auto-scaling, resource sizing"

  - skill: python-craftsman
    relationship: "Python performance"
    brings: "Async optimization, memory profiling, cProfile patterns"

delegation:
  - trigger: "vector search is slow"
    delegate_to: vector-specialist
    pattern: parallel
    context: "Current HNSW parameters and query patterns"
    receive: "Index tuning recommendations"

  - trigger: "graph queries are slow"
    delegate_to: graph-engineer
    pattern: parallel
    context: "Cypher queries and execution plans"
    receive: "Query optimization and index recommendations"

  - trigger: "workflow throughput is low"
    delegate_to: temporal-craftsman
    pattern: review
    context: "Worker configuration and task queue design"
    receive: "Worker tuning recommendations"

  - trigger: "event processing is slow"
    delegate_to: event-architect
    pattern: review
    context: "Consumer configuration and partition strategy"
    receive: "Consumer optimization recommendations"

  - trigger: "memory retrieval is slow"
    delegate_to: ml-memory
    pattern: parallel
    context: "Memory access patterns and retrieval pipeline"
    receive: "Memory system optimization recommendations"

collaboration_patterns:
  sequential:
    - "I profile system, then vector-specialist optimizes vector search"
    - "I identify bottleneck, then graph-engineer optimizes Cypher"

  parallel:
    - "I optimize caching while vector-specialist tunes HNSW"
    - "I optimize pooling while temporal-craftsman tunes workers"

  review:
    - "Review vector-specialist index configuration for performance"
    - "Review temporal-craftsman worker settings for throughput"
    - "Review privacy-guardian encryption for overhead"

cross_domain_insights:
  - domain: operating-systems
    insight: "Same principles: profiling, scheduling, caching, pooling"
    applies_when: "Thinking about resource management"

  - domain: queuing-theory
    insight: "Little's Law: L = λW (items = arrival rate × wait time)"
    applies_when: "Sizing connection pools and queues"

  - domain: computer-architecture
    insight: "Memory hierarchy: L1 cache (fast/small) → RAM (slow/big)"
    applies_when: "Designing multi-level cache"

  - domain: statistics
    insight: "Percentiles matter more than averages for user experience"
    applies_when: "Choosing latency metrics"

  - domain: economics
    insight: "Premature optimization has opportunity cost"
    applies_when: "Deciding when to optimize"

ecosystem:
  primary_tools:
    - "cProfile / py-spy - Python profiling"
    - "asyncio debug mode - Event loop analysis"
    - "Prometheus + Grafana - Metrics and alerting"
    - "locust / k6 - Load testing"

  alternatives:
    - name: Pyinstrument
      use_when: "Need easier-to-read profiler output"
      avoid_when: "Need precise timing or async profiling"

    - name: memory_profiler
      use_when: "Debugging memory leaks"
      avoid_when: "CPU profiling needed"

    - name: Jaeger/Zipkin
      use_when: "Distributed tracing across services"
      avoid_when: "Single service optimization"

    - name: pgBadger
      use_when: "PostgreSQL slow query analysis"
      avoid_when: "Not using PostgreSQL"

  deprecated:
    - "Guessing where bottleneck is"
    - "Optimizing without measurement"
    - "Average-only latency metrics"
    - "Sync I/O in async code"
    - "Connection per request pattern"
