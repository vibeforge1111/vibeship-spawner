id: ai-image-generation-sharp-edges
skill: ai-image-generation
version: 1.0.0

edges:
  - id: model-style-mismatch
    summary: Using wrong model for desired aesthetic wastes generations
    severity: high
    situation: |
      Using Midjourney for photorealism when DALL-E 3 or Flux excels, or using
      DALL-E for stylized illustration when Midjourney dominates.
    why: |
      Each model has strengths. Wrong choice = 5-10x more iterations to get
      acceptable output, or never getting what you want. Budget and time wasted.
    solution: |
      MODEL SELECTION BY AESTHETIC:

      PHOTOREALISM (products, people, scenes):
      → Flux Pro/Dev - Best photorealism, great faces
      → DALL-E 3 - Good realism, excellent prompt following
      → Imagen 3 - Google's photorealistic option

      STYLIZED/ARTISTIC:
      → Midjourney v6 - Best for stylized, artistic, fantasy
      → Ideogram - Best for text in images
      → Leonardo - Good variety of styles

      ILLUSTRATION/DESIGN:
      → Midjourney - Strong for illustrations
      → DALL-E 3 - Clean, design-friendly
      → Ideogram - When text integration needed

      SPEED/ITERATION:
      → Flux Schnell - Fastest, good for prototyping
      → DALL-E 3 - Fast, reliable

      # MCP tool check
      spawner_tools({ action: "recommend", use_case: "product photography" })
    symptoms:
      - Multiple re-generations
      - '"This looks wrong" feedback'
      - Switching models mid-project
      - Style inconsistency in series
    detection_pattern: null

  - id: prompt-structure-chaos
    summary: Unstructured prompts produce unpredictable results
    severity: high
    situation: |
      Writing prompts as stream of consciousness without clear structure.
      "A cool looking person in a city with neon lights maybe at night"
    why: |
      AI models parse prompts in order. Front-loaded important elements
      get more weight. Unstructured prompts = random interpretation.
    solution: |
      PROMPT STRUCTURE (front-to-back importance):

      1. MEDIUM: "Oil painting of", "Professional photograph of", "3D render of"
      2. SUBJECT: Main subject with key details
      3. ENVIRONMENT: Setting, background, context
      4. LIGHTING: Light source, quality, color
      5. STYLE: Artist reference, aesthetic, mood
      6. TECHNICAL: Camera, lens, aspect ratio

      # BAD
      "a city at night with neon and a person"

      # GOOD
      "Professional photograph of a woman in her 30s standing on
      a rain-slicked Tokyo street. Neon signs reflect on wet pavement.
      Night scene, dramatic side lighting from storefronts. Cyberpunk
      aesthetic, cinematic color grade. Shot on Sony A7IV, 35mm lens,
      f/2.0, shallow depth of field."
    symptoms:
      - Random, unpredictable outputs
      - Subject not emphasized
      - Wrong medium/style applied
      - '"The AI doesn''t understand me"'
    detection_pattern: 'prompt.*"[^"]{1,100}"(?!.*\b(photograph|render|painting|illustration)\b)'

  - id: text-in-image-failures
    summary: Most models fail at generating readable text
    severity: medium
    situation: |
      Adding text like brand names, labels, or signs to image prompts
      and getting gibberish or misspelled words.
    why: |
      AI image models were not designed for typography. They generate
      patterns that look like text but aren't readable. This has improved
      but is still unreliable except for Ideogram.
    solution: |
      TEXT HANDLING OPTIONS:

      1. USE IDEOGRAM (best at text):
         - Ideogram 2.0 generates readable text
         - Use for logos, signs, labels

      2. TEXT IN POST-PRODUCTION:
         - Generate image without text
         - Add text in Photoshop/Figma/Canva
         - More control, always correct

      3. SIMPLE TEXT ONLY:
         - 1-3 word text works better
         - Common words more reliable
         - Stylized text (graffiti) more forgiving

      4. INPAINTING:
         - Generate gibberish text
         - Inpaint with correct text using Ideogram

      # NEVER rely on AI for:
      - Product labels
      - Legal text
      - Brand names
      - URLs or codes
    symptoms:
      - Gibberish text
      - Misspellings
      - '"Almost right" text'
      - Text appearing where not wanted
    detection_pattern: 'text.*says|say.*"|reading|label|sign|logo.*"'

  - id: anatomy-disasters
    summary: Human anatomy errors especially in hands and limbs
    severity: high
    situation: |
      Generating humans with extra fingers, merged limbs, impossible poses,
      or anatomically incorrect features.
    why: |
      AI models struggle with human anatomy due to training data complexity.
      Hands are notoriously difficult. This has improved but still occurs.
    solution: |
      ANATOMY PROTECTION:

      1. NEGATIVE PROMPTS (always include):
         - '"no extra fingers, no deformed hands, anatomically correct"'
         - '"no merged limbs, proper anatomy"'

      2. POSE STRATEGY:
         - Avoid complex hand poses initially
         - Use ControlNet with pose reference
         - Generate hands separately and composite

      3. MODEL SELECTION:
         - Flux Pro has best anatomy
         - Midjourney v6 improved significantly
         - DALL-E 3 decent but not perfect

      4. INPAINTING FIX:
         - Generate image
         - Mask problematic areas
         - Inpaint with specific fix prompt
         - '"correct human hand with five fingers"'

      5. CROPPING:
         - Compose shots to avoid problematic areas
         - Close-up avoiding hands if not essential
    symptoms:
      - Extra fingers
      - Missing limbs
      - Merged body parts
      - '"Uncanny valley" figures'
    detection_pattern: null

  - id: consistency-across-series
    summary: Same prompt produces different results each generation
    severity: high
    situation: |
      Generating a series of images for campaign/brand and each one
      looks completely different in style, color, lighting.
    why: |
      AI generation is inherently random. Without consistency measures,
      each generation is independent. A series looks like random images.
    solution: |
      CONSISTENCY TECHNIQUES:

      1. SEED CONTROL:
         - Use same seed for related images
         - Note: Same prompt + seed = similar result
         - Vary only specific elements

      2. STYLE REFERENCE (IP-Adapter):
         - Generate hero image first
         - Use as style reference for all others
         - '"in the style of [reference image]"'

      3. PROMPT ANCHORING:
         - Create base prompt with all style elements
         - Only change subject/action per image
         - Lock: lighting, color palette, camera, mood

      4. LORA/FINE-TUNING:
         - Train LoRA on brand style
         - Apply to all generations
         - Most consistent but requires setup

      5. CHARACTER SHEETS:
         - Generate character reference sheet
         - Use for all character appearances
         - Front, side, 3/4 views

      # ComfyUI workflow for consistency
      Use comfyui tool with style_reference workflow
    symptoms:
      - '"These don''t look like a series"'
      - Random color grading
      - Inconsistent lighting
      - Character doesn't look the same
    detection_pattern: null

  - id: resolution-upscaling-confusion
    summary: Generating at low res expecting magic upscaling
    severity: medium
    situation: |
      Generating thumbnails or low-resolution images, then expecting
      upscaling to create print-quality output.
    why: |
      Upscaling enhances existing detail, doesn't create new detail.
      Low-res generation has less information. Upscaling it just makes
      the lack of detail larger.
    solution: |
      RESOLUTION STRATEGY:

      Generate at NATIVE MAX RESOLUTION:
      - Midjourney: --quality 1 --stylize default (1024x1024+)
      - DALL-E 3: 1792x1024 or 1024x1792
      - Flux: 1024x1024 native, can do higher
      - Ideogram: Up to 1.5 megapixels

      THEN UPSCALE:
      - Magnific AI - Best quality, "reimagines" detail
      - Topaz Gigapixel - Clean, faithful upscaling
      - Real-ESRGAN - Free, good for illustrations

      FOR PRINT:
      - Minimum 300 DPI at final size
      - 8.5x11" print = 2550x3300px minimum
      - Generate high, upscale 2-4x, then output

      # Never: Generate 512x512, upscale 8x, expect quality
    symptoms:
      - Blurry prints
      - Visible artifacts at size
      - '"{content}" looking details'
      - AI artifacts become visible
    detection_pattern: null

  - id: copyright-training-data
    summary: Generating content that too closely mimics copyrighted works
    severity: critical
    situation: |
      Prompting "in the style of [living artist]" or generating characters
      that clearly replicate copyrighted IP.
    why: |
      Legal risk is real. Getty sued Stability AI. Artists are suing.
      Using specific artist names or generating recognizable IP can
      expose you or your client to legal action.
    solution: |
      SAFE PROMPTING:

      AVOID:
      - '"[Living artist name] style"'
      - Specific copyrighted characters
      - Recognizable IP (Disney, Marvel, etc.)
      - Trademarked logos or designs

      INSTEAD:
      - Describe the style: "impressionist brushstrokes"
      - Use historical/deceased artists: "Rembrandt lighting"
      - Combine multiple influences
      - Create original characters inspired by, not copying

      COMMERCIAL SAFETY:
      - Use models with cleared training data
      - Adobe Firefly (licensed stock training)
      - Shutterstock AI (their own library)
      - Document your process
    symptoms:
      - Output too similar to known work
      - Recognizable copyrighted elements
      - Legal cease and desist
      - Platform content removal
    detection_pattern: 'style of (Greg Rutkowski|Artgerm|Wlop|Disney|Marvel|specific-artist)'

  - id: api-cost-overrun
    summary: Burning through API credits without tracking
    severity: high
    situation: |
      Running many generations during iteration without tracking spend.
      Hitting billing limits or getting surprise invoices.
    why: |
      AI image generation costs $0.01-0.10+ per image. Seems cheap until
      you've run 500 iterations. Teams can burn $50-100/day easily.
    solution: |
      COST MANAGEMENT:

      KNOW YOUR RATES:
      - DALL-E 3: ~$0.04-0.12/image
      - Midjourney: $10-60/month unlimited
      - Flux via Replicate: ~$0.003/image (cheap!)
      - Ideogram: ~$0.02/image

      TRACKING SYSTEM:
      ```typescript
      async function generateWithTracking(prompt) {
        const cost = estimateCost(model);
        await logCost({ model, cost, prompt });
        return generate(prompt);
      }

      // Daily report
      spawner_analyze({ action: "cost-report", date: "today" })
      ```

      BUDGET RULES:
      - Set daily/weekly limits
      - Use cheap models for iteration
      - Save premium for finals
      - Alert at 80% budget
    symptoms:
      - Surprise bills
      - '"We''re out of credits"'
      - Billing alerts
      - Rate limiting
    detection_pattern: null

  - id: prompt-injection-in-user-input
    summary: User input can manipulate AI behavior unexpectedly
    severity: critical
    situation: |
      Building an app where users provide input that becomes part of
      the image prompt. User injects instructions.
    why: |
      Users can inject "ignore previous instructions" or harmful content
      into prompts. Can bypass content filters, generate unwanted content,
      or break your application.
    solution: |
      PROMPT INJECTION PREVENTION:

      1. INPUT SANITIZATION:
         ```typescript
         function sanitize(input: string): string {
           // Remove instruction-like patterns
           return input
             .replace(/ignore.*instruction/gi, '')
             .replace(/system.*prompt/gi, '')
             .replace(/\n/g, ' ')
             .slice(0, 200); // Limit length
         }
         ```

      2. STRUCTURED PROMPTS:
         ```typescript
         // User input goes in specific slot only
         const prompt = `Professional photo of a ${sanitize(userProduct)}.
                        Clean white background. Studio lighting.`;
         ```

      3. CONTENT FILTERING:
         - Pre-filter user input for banned terms
         - Use provider's safety filters
         - Post-filter generated content

      4. MODERATION API:
         - Run input through moderation first
         - Block or flag before generation
    symptoms:
      - Unexpected content generated
      - Filters bypassed
      - Harmful content appears
      - Prompt structure broken
    detection_pattern: 'userInput|user_input|req\.body|params\.'

  - id: batch-generation-without-deduplication
    summary: Generating duplicates in batch without checking similarity
    severity: medium
    situation: |
      Running batch generations for variants and getting many near-identical
      outputs, wasting credits.
    why: |
      Without varied seeds or prompt variations, you might generate the same
      image multiple times. Batch efficiency is lost.
    solution: |
      BATCH VARIATION STRATEGY:

      1. SEED VARIATION:
         ```typescript
         const seeds = Array.from({ length: 10 }, () =>
           Math.floor(Math.random() * 1000000)
         );
         const results = await Promise.all(
           seeds.map(seed => generate({ prompt, seed }))
         );
         ```

      2. PROMPT VARIATION:
         - Slight wording changes
         - Different angles/perspectives
         - Color/lighting variations
         - '"{content}"', "Alternative angle of"

      3. DEDUPLICATION:
         - Calculate perceptual hash of outputs
         - Filter out >90% similar images
         - Retry for more variety

      4. QUALITY FILTERING:
         - Not all generations are usable
         - Auto-filter for artifacts
         - Human review for finals
    symptoms:
      - Near-identical outputs
      - Wasted generation credits
      - '"These all look the same"'
      - Low variety in batches
    detection_pattern: 'Promise\.all.*generate|batch.*generate|for.*generate'

  - id: ignoring-safety-filters
    summary: Trying to bypass content safety filters
    severity: critical
    situation: |
      Attempting to generate content that trips safety filters by
      using synonyms, creative spelling, or jailbreak techniques.
    why: |
      Content filters exist for legal and ethical reasons. Bypassing them
      risks account termination, legal issues, and platform bans.
      Providers actively detect and ban bypass attempts.
    solution: |
      WORK WITH FILTERS, NOT AGAINST:

      1. UNDERSTAND LIMITS:
         - Violence, explicit content blocked
         - Real people with certain scenarios blocked
         - Copyrighted characters blocked

      2. LEGITIMATE ALTERNATIVES:
         - Use NSFW-allowing platforms with proper licensing
         - Commission traditional artists for restricted content
         - Stock photography for real people
         - Create original characters

      3. APPEAL PROCESS:
         - False positives happen
         - Use official appeal channels
         - Document legitimate use case

      4. NEVER:
         - Share bypass techniques
         - Use in production apps
         - Risk client/employer reputation
    symptoms:
      - Account warnings
      - Generation failures
      - Account suspension
      - Content removal
    detection_pattern: null

  - id: metadata-loss
    summary: Losing generation metadata and unable to reproduce results
    severity: medium
    situation: |
      Getting a perfect generation but not saving prompt, seed, model
      version, and settings. Unable to iterate or recreate.
    why: |
      AI generation has randomness. Without exact parameters, you cannot
      recreate results, create variations, or maintain consistency for
      future work in the same style.
    solution: |
      METADATA LOGGING:

      Save for every generation:
      ```json
      {
        "id": "gen_abc123",
        "timestamp": "2024-01-15T10:30:00Z",
        "model": "midjourney-v6",
        "prompt": "Full prompt...",
        "negative_prompt": "...",
        "seed": 42,
        "settings": {
          "quality": 1,
          "stylize": 100,
          "aspect": "16:9"
        },
        "output_url": "...",
        "cost": 0.04
      }
      ```

      # MCP Integration
      spawner_remember({
        type: "generation",
        data: { prompt, seed, model, output_url }
      })

      # Use version control for prompt libraries
      # Tag successful prompts for reuse
    symptoms:
      - '"How did we make that?"'
      - Can't recreate success
      - Inconsistent follow-up images
      - Lost institutional knowledge
    detection_pattern: null
