id: ai-ad-creative-sharp-edges
skill: ai-ad-creative
version: 1.0.0

edges:
  - id: platform-spec-blindness
    summary: Ignoring platform-specific format requirements
    severity: critical
    situation: |
      Creating ads without considering platform-specific requirements like
      safe zones, aspect ratios, text limits, or format restrictions.
    why: |
      Each platform has strict requirements. Meta has 20% text rule (softer now
      but still impacts delivery). TikTok needs vertical. LinkedIn has different
      expectations. Wrong format = rejected or underperforming ads.
    solution: |
      PLATFORM REQUIREMENTS:

      META (Facebook/Instagram):
      - Feed: 1:1 or 4:5 (best), 16:9 (okay)
      - Stories/Reels: 9:16
      - Text on image: <20% for best delivery
      - Video: 1-240 seconds
      - Caption: 125 chars visible, 2200 max

      TIKTOK:
      - Vertical 9:16 only
      - Video: 5-60 seconds sweet spot
      - Top 20% and bottom 20% safe zones
      - CTA button covers bottom
      - Sound-on assumed

      GOOGLE (YouTube, Display):
      - YouTube: 16:9, 6-15 second skippable
      - Display: Multiple sizes needed
      - Performance Max: Many formats
      - Text limits vary by placement

      LINKEDIN:
      - Sponsored content: 1200x627 (1.91:1)
      - Video: 3 seconds to 30 minutes
      - Text: 150 chars recommended
      - Professional context

      # Check specs BEFORE generating
    symptoms:
      - Ads rejected by platform
      - Poor delivery/reach
      - Key elements cropped
      - Low quality warnings
    detection_pattern: null

  - id: hook-missing-or-weak
    summary: First 3 seconds don't capture attention
    severity: critical
    situation: |
      Creating video ads without a strong hook in the first 1-3 seconds,
      or static ads without an immediate visual hook.
    why: |
      Users scroll fast. You have 1-3 seconds max to stop the scroll. Without
      a pattern-interrupt hook, your ad is invisible. All the conversion copy
      after the hook doesn't matter if no one sees it.
    solution: |
      HOOK TYPES THAT WORK:

      VIDEO HOOKS (first 3 seconds):
      1. PATTERN INTERRUPT:
         - Unexpected visual
         - Weird angle
         - Contrast/movement
         - "Wait, what?"

      2. QUESTION HOOKS:
         - "Did you know...?"
         - "What if I told you...?"
         - Problem statement as question

      3. BOLD STATEMENT:
         - Controversial claim
         - Surprising statistic
         - Direct promise

      4. NATIVE HOOKS:
         - "POV:" (TikTok)
         - "I found this..."
         - "Nobody asked but..."

      STATIC HOOKS:
      - High contrast focal point
      - Face looking at camera
      - Unusual composition
      - Clear product + emotion

      AI PROMPT EXAMPLE:
      "Create ad with [HOOK TYPE] that stops the scroll.
      First frame should [specific attention-grabbing element]"

      # Test 5+ different hooks per concept
    symptoms:
      - Low view-through rate
      - High cost per engagement
      - Quick scrolls past ad
      - Poor video retention
    detection_pattern: null

  - id: generic-ai-aesthetic
    summary: AI-generated ads look obviously AI-generated
    severity: high
    situation: |
      Using default AI image outputs that have the recognizable "AI look"—
      too perfect, hyperreal, or with that uncanny polish users recognize.
    why: |
      Users are learning to recognize AI content. When they identify an ad
      as AI-generated, trust drops. The ad feels fake or lazy. Authenticity
      matters more than polish.
    solution: |
      AVOID THE AI AESTHETIC:

      1. IMPERFECTION IS AUTHENTIC:
         - Add slight imperfections in post
         - Lower contrast/saturation slightly
         - Avoid hyperreal lighting
         - Match platform native content style

      2. UGC-STYLE GENERATION:
         - Prompt for "iPhone photo" or "casual snapshot"
         - Not "professional photography" or "8K"
         - "Real person" not "model"
         - Candid > posed

      3. MIX WITH REAL:
         - AI backgrounds + real product photos
         - AI people + real environments
         - Hybrid approaches less detectable

      4. PLATFORM MATCHING:
         - TikTok: Lower quality = more native
         - Instagram: Polished but not perfect
         - LinkedIn: Professional but authentic

      PROMPT ADJUSTMENTS:
      Instead of: "Professional advertisement, 8K, perfect"
      Use: "Authentic, candid, natural lighting, mobile photo style"
    symptoms:
      - Comments calling out AI
      - Lower trust signals
      - Poor engagement
      - '"Fake" feeling'
    detection_pattern: 'prompt.*(8K|professional|perfect|hyperreal)'

  - id: ctr-over-conversion
    summary: Optimizing for clicks instead of actual conversions
    severity: high
    situation: |
      Creating ads that get high click-through rates but don't convert,
      often through clickbait or misleading creative.
    why: |
      CTR is a vanity metric. An ad that gets 5% CTR but 0.1% conversion
      is worse than 1% CTR with 2% conversion. Clickbait attracts wrong
      audience and wastes budget.
    solution: |
      CONVERSION-FOCUSED CREATIVE:

      1. MATCH PROMISE TO LANDING PAGE:
         - Ad promise = landing page headline
         - No surprise at click-through
         - Consistent visual style

      2. PRE-QUALIFY IN AD:
         - Mention price if not competitive
         - Show real product/service
         - Set accurate expectations
         - Let wrong-fit people NOT click

      3. CONVERSION ELEMENTS:
         - Clear product/benefit visibility
         - Social proof in ad
         - Urgency if authentic
         - CTA matching action needed

      4. MEASURE CORRECTLY:
         Track: CPA (Cost Per Acquisition)
         Not just: CTR, CPC
         Goal: Lowest cost per actual customer

      AI PROMPT:
      "Create ad that clearly shows [product] with [benefit],
      featuring [social proof], honest representation"

      # Attract the right clickers, not all clickers
    symptoms:
      - High CTR, low conversion
      - Landing page bounce
      - Wasted ad spend
      - Poor ROAS
    detection_pattern: null

  - id: test-one-thing-fail
    summary: Testing multiple variables simultaneously
    severity: high
    situation: |
      Changing hook, image, copy, and format all at once when testing,
      making it impossible to know what caused the result.
    why: |
      When you change 5 things and performance changes, you don't know which
      variable mattered. Statistical significance requires isolated variables.
      Multi-variable tests need massive sample sizes.
    solution: |
      CONTROLLED TESTING:

      1. VARIABLE ISOLATION:
         Test A: Hook variation only
         Test B: Image variation only
         Test C: Copy variation only
         Never: Hook + image + copy

      2. TESTING HIERARCHY:
         Order by impact (test first):
         1. Hook (biggest impact)
         2. Offer/promise
         3. Creative format
         4. Visual style
         5. CTA

      3. STATISTICAL RIGOR:
         - Same audience for test
         - Equal budget split
         - Enough volume (100+ conversions per variant)
         - Let it run (3-7 days minimum)

      4. LEARNING DOCUMENTATION:
         Winner: Hook A
         Why: Higher stop-scroll rate
         Insight: Questions outperform statements
         Action: Apply question hook to next batch

      # One variable at a time for real learning
    symptoms:
      - Unclear winners
      - Conflicting test results
      - No accumulated learning
      - Always guessing
    detection_pattern: null

  - id: creative-fatigue-blindness
    summary: Running same creative until it dies
    severity: high
    situation: |
      Not monitoring for creative fatigue and running winning ads until
      performance collapses instead of proactively refreshing.
    why: |
      Every ad fatigues. Frequency increases, CTR drops, CPM rises, returns
      diminish. If you wait for collapse, you're already behind on refresh.
      Proactive refresh maintains performance.
    solution: |
      FATIGUE MONITORING:

      SIGNALS TO WATCH:
      - CTR declining 20%+ from peak
      - CPM rising without auction changes
      - Frequency above 2-3
      - Conversion rate dropping
      - Negative comment sentiment

      REFRESH STRATEGY:
      - High spend: New variants weekly
      - Medium spend: Every 2 weeks
      - Low spend: Monthly

      PROACTIVE PIPELINE:
      Always have in queue:
      - 10+ untested variants
      - 5+ variations of current winners
      - Fresh concepts in development

      AI ADVANTAGE:
      Traditional refresh: 1-2 weeks to create
      AI refresh: Same day generation

      WORKFLOW:
      Monday: Review performance, flag fatigue
      Tuesday: Generate 10 new variants
      Wednesday: QA and upload
      Thursday: Launch tests
      Friday: Analyze, plan next week

      # Never let the cupboard go empty
    symptoms:
      - Sudden performance drop
      - "Ad stopped working"
      - Scrambling to create new ads
      - Lost momentum
    detection_pattern: null

  - id: brand-disconnect
    summary: AI ads don't feel like the brand
    severity: medium
    situation: |
      AI-generated ads that convert but don't match brand voice, visual
      identity, or overall brand experience.
    why: |
      Short-term: Conversions may work
      Long-term: Brand dilution, customer confusion, trust erosion
      The ad and landing page feel like different companies.
    solution: |
      BRAND-LOCKED AI CREATIVE:

      1. BRAND PROMPT PREFIX:
         "In [Brand] style: [detailed description]
         Colors: [specific hex codes]
         Typography feel: [description]
         Photography style: [description]"

      2. REFERENCE LIBRARY:
         - Approved brand assets as reference
         - "Like this image but for [new use]"
         - Style consistency across generations

      3. BRAND GUARDRAILS:
         - Color checker in post (match palette)
         - Typography overlay (brand fonts)
         - Logo placement rules
         - Do not use list (competitors, banned words)

      4. APPROVAL PROCESS:
         - Brand check before launch
         - Side-by-side with brand examples
         - Consistent feel even if performance-focused

      # Conversion creative can still be on-brand
    symptoms:
      - '"Doesn''t look like us" feedback'
      - Customer confusion
      - Inconsistent experience
      - Brand team pushback
    detection_pattern: null

  - id: legal-compliance-gaps
    summary: AI ads with compliance issues
    severity: critical
    situation: |
      AI-generating ads that make claims, show results, or include
      elements that create legal/regulatory exposure.
    why: |
      AI doesn't know advertising regulations. Claims need substantiation.
      Results need disclaimers. Some industries (finance, health, alcohol)
      have strict rules. Violations = account bans, fines, lawsuits.
    solution: |
      COMPLIANCE CHECKLIST:

      CLAIMS:
      - No unsubstantiated claims
      - "Results typical" disclaimers
      - No guaranteed outcomes
      - Evidence for statistics

      TESTIMONIALS:
      - Must be genuine (even AI avatars saying real reviews)
      - Results not typical disclaimers
      - Relationship disclosure if paid

      REGULATED INDUSTRIES:
      - Finance: APR disclosures, risk warnings
      - Health: FDA compliance, no cure claims
      - Alcohol: Age requirements, responsibility
      - Gambling: License numbers, responsible gambling

      AI-SPECIFIC:
      - Disclosure if AI-generated faces required
      - No deepfake concerns
      - Clear labeling where required

      PROCESS:
      - Legal review for new claim types
      - Approval checklist before launch
      - Regular audit of running ads

      # When in doubt, get legal review
    symptoms:
      - Ad rejections
      - Account warnings
      - Legal letters
      - Platform bans
    detection_pattern: 'prompt.*(guarantee|proven|cure|will make you)'

  - id: budget-waste-testing
    summary: Spending too much on testing unproven creative
    severity: medium
    situation: |
      Giving new, unproven creative too much budget too quickly, wasting
      money before signal emerges.
    why: |
      New creative needs time for algorithms to optimize. Spending $1000/day
      on untested creative burns money. Small tests find winners cheaply.
    solution: |
      BUDGET STRUCTURE:

      TESTING BUDGET (20% of total):
      - Small budget per variant ($20-50/day)
      - Many variants tested
      - Kill fast (2-3 days if clearly losing)
      - Graduate winners to scaling

      SCALING BUDGET (60% of total):
      - Proven winners only
      - Incremental scale (20% increases)
      - Monitor for fatigue signals
      - Maintain as long as efficient

      EXPLORATION (20% of total):
      - New concepts, formats, audiences
      - Accept higher CPA for learning
      - Systematic not random

      TESTING RULES:
      - Minimum 100 impressions for signal
      - 3-7 days before decision
      - Kill bottom 50% of batch
      - Scale top 10-20%

      # Test cheap, scale winners
    symptoms:
      - High CPA on new creative
      - Budget exhausted on losers
      - Not enough winning creative found
      - Inefficient spend
    detection_pattern: null

  - id: single-platform-creative
    summary: Not adapting creative for each platform
    severity: medium
    situation: |
      Using same creative across Meta, TikTok, LinkedIn, Google without
      platform-specific adaptation.
    why: |
      User behavior differs by platform. What works on TikTok fails on
      LinkedIn. Native content outperforms generic. Resizing is not adapting.
    solution: |
      PLATFORM ADAPTATION:

      META:
      - Sound-off viewing (text overlays)
      - UGC/authentic feel
      - 4:5 or 1:1 format
      - Hook in first frame

      TIKTOK:
      - Creator-style content
      - Trending audio references
      - 9:16 vertical only
      - Platform-native hooks

      YOUTUBE:
      - Longer attention span
      - Sound-on assumed
      - Skip in 5 seconds
      - Story arc possible

      LINKEDIN:
      - Professional tone
      - Authority-based
      - Less entertainment
      - B2B sensibility

      GOOGLE DISPLAY:
      - Multiple sizes required
      - Static works well
      - Clear CTA
      - Brand visibility

      DON'T: Resize same creative
      DO: Recreate for each platform's context
    symptoms:
      - Underperformance vs benchmarks
      - Ads feel out of place
      - Native content wins
      - Lower engagement rates
    detection_pattern: null

  - id: no-learning-loops
    summary: Testing without systematic learning documentation
    severity: medium
    situation: |
      Running many tests but not documenting learnings, leading to
      repeated mistakes and no accumulated knowledge.
    why: |
      Without documentation, each test is isolated. You repeat failures,
      forget what worked, can't train team members. Learning compounds
      when documented and applied.
    solution: |
      LEARNING SYSTEM:

      FOR EACH TEST:
      ```
      Test: [Name/ID]
      Date: [Start] - [End]
      Variable tested: [Specific element]
      Hypothesis: [What you expected]
      Result: [What happened, with data]
      Winner: [Which variant]
      Learning: [Why it won]
      Action: [How to apply going forward]
      ```

      LEARNING LIBRARY:
      - Searchable database of tests
      - Tagged by variable type, product, platform
      - Reviewed before new test design
      - Shared with team

      PATTERN RECOGNITION:
      After 10+ tests, identify:
      - What hooks work best for us?
      - What visual styles convert?
      - What messaging resonates?
      - What formats work?

      # Test → Document → Learn → Apply → Repeat
    symptoms:
      - Repeating failed approaches
      - No accumulated knowledge
      - Can't answer "what works for us"
      - Team reinvents wheel
    detection_pattern: null
