edges:
  - title: Brand dilution through uncontrolled AI generation
    severity: critical
    detection:
      context:
        - multiple team members generating brand content
        - no central prompt library or brand guidelines
        - inconsistent visual or voice output across channels
      intent:
        - generate brand content
        - create marketing materials
        - produce social media posts
      code:
        - custom GPT without brand training
        - ad-hoc prompts in ChatGPT/Claude
        - no version control on prompts
    problem: |
      When team members use AI to generate brand content without centralized
      prompts and guidelines, each person creates their own interpretation.
      Marketing's brand voice differs from sales. Social media visuals don't
      match website. Every output is inconsistent. Brand equity erodes.

      The team thinks they're being efficient using AI, but they're actually
      fragmenting the brand across dozens of individual AI sessions with no
      shared context.
    consequence: |
      - Brand becomes unrecognizable across touchpoints
      - Customers experience inconsistent messaging and visuals
      - Brand equity and trust decline
      - Expensive rebrand or cleanup required
      - Team wastes time debating "what sounds like us?"
    solution: |
      1. Create centralized prompt library in version control
      2. Build Custom GPTs or Claude Projects with embedded brand knowledge
      3. Require all brand content to use approved prompts
      4. Implement approval workflow for AI-generated content
      5. Monthly brand audits to catch drift early
      6. Track which prompts produce best results, deprecate poor ones

  - title: Prompt library chaos - no versioning or governance
    severity: critical
    detection:
      context:
        - prompts scattered across Google Docs, Notion, Slack
        - no version numbers or changelog
        - team members modifying prompts without review
      intent:
        - reuse prompts
        - update brand guidelines
        - share prompts with team
      code:
        - prompts in documents, not code repository
        - no git history or version tags
        - no approval process for prompt changes
    problem: |
      Prompts are treated as throwaway text instead of critical brand assets.
      Stored in random documents. Modified without review. No history of what
      changed or why. When a prompt stops working, nobody knows what version
      worked or who changed it.

      A team member "improves" a prompt but actually breaks brand consistency.
      Change propagates across team. By the time anyone notices, dozens of
      off-brand assets have been created.
    consequence: |
      - Can't reproduce successful results (don't know which prompt version worked)
      - Can't rollback bad changes
      - Prompts drift without oversight
      - Onboarding new team members is chaos
      - Knowledge loss when team members leave
    solution: |
      1. Move prompts to Git repository
      2. Use semantic versioning for prompts (v1.0, v1.1, v2.0)
      3. Require pull requests and approval for prompt changes
      4. Document changelog in each prompt file
      5. Tag major versions for rollback capability
      6. Archive deprecated prompts (don't delete - reference value)

  - title: Style drift - no quality monitoring over time
    severity: high
    detection:
      context:
        - AI output from 3 months ago looks different than today
        - same prompts producing different results
        - no systematic quality checking
      intent:
        - maintain brand consistency
        - ensure quality over time
        - detect model changes
      code:
        - no benchmarking against gold standards
        - no regular brand audits
        - no drift detection metrics
    problem: |
      AI models update constantly. Midjourney v6.0 → v6.1 changes output style.
      ChatGPT model updates shift voice patterns. Your prompts stay the same
      but results drift. By the time you notice, months of off-brand content
      have been created.

      Teams assume "set and forget" - create prompts once, use forever. But
      AI is a moving target. What worked perfectly in January produces
      mediocre results by June.
    consequence: |
      - Brand consistency degrades silently
      - Quality drops without anyone noticing
      - Expensive content rework required
      - Can't pinpoint when/why drift started
      - Loss of audience trust
    solution: |
      1. Define 5-10 "gold standard" examples for each content type
      2. Monthly comparison: current AI output vs. gold standards
      3. Track quality scores over time (1-10 scale)
      4. When drift detected, investigate: model change? prompt issue? brand evolution?
      5. Update prompts to compensate for model changes
      6. Document model versions used for all content

  - title: No negative prompts - AI fills gaps with off-brand choices
    severity: high
    detection:
      context:
        - AI generates elements you'd never choose
        - stock photo aesthetics appearing
        - generic corporate speak in copy
      intent:
        - generate brand-consistent content
        - avoid off-brand elements
      code:
        - prompts only describe what to include
        - no explicit forbidden elements list
        - no negative examples in training
    problem: |
      When you don't tell AI what NOT to do, it makes assumptions based on
      training data. Your brand avoids gradients, but you never said so -
      gradients appear everywhere. You hate "synergy" and "game-changer" but
      didn't block them - AI uses them constantly.

      AI doesn't know your brand's anti-patterns unless you explicitly teach
      them. Positive examples show what to do; negative prompts prevent what
      to avoid.
    consequence: |
      - AI generates off-brand elements you never wanted
      - Every output requires editing to remove bad elements
      - Style drift toward generic/stock aesthetics
      - Brand differentiation lost
      - Constant frustration with AI output
    solution: |
      1. For every positive brand guideline, create negative counterpart
      2. Visual negatives: gradients, drop shadows, stock photo clichés
      3. Voice negatives: corporate jargon, buzzwords, clichés
      4. Include negative prompts in all generation requests
      5. Build blocklists of forbidden phrases/elements
      6. Train AI on negative examples: "This is NOT our brand"

  - title: Cultural blindness - brand optimized for one context only
    severity: high
    detection:
      context:
        - expanding to new markets or languages
        - diverse user complaints about content
        - accessibility issues flagged
      intent:
        - create globally-relevant brand
        - ensure inclusive content
        - meet accessibility standards
      code:
        - prompts with cultural assumptions
        - no locale-specific variations
        - accessibility as afterthought
    problem: |
      Brand prompts created for US English market. Metaphors, idioms, cultural
      references baked in. When expanding to other regions, AI generates
      content that doesn't translate, confuses, or offends. Color meanings
      differ across cultures. Humor doesn't land. Visual symbols mean
      different things.

      Accessibility requirements ignored in prompts. AI generates images with
      poor contrast, copy without structure for screen readers, videos without
      captions.
    consequence: |
      - Content fails in international markets
      - Exclude users with disabilities
      - Damage brand reputation through cultural insensitivity
      - Legal exposure (accessibility lawsuits)
      - Expensive localization rework
    solution: |
      1. Create locale-specific prompt variations for global brands
      2. Include cultural consultants in brand AI training
      3. Build accessibility into all prompts: contrast, alt text, structure
      4. Test AI output with diverse users before launch
      5. Include cultural and accessibility review in approval workflow
      6. Update prompts based on international feedback

  - title: Over-constraining creativity - prompts become straitjacket
    severity: medium
    detection:
      context:
        - team complains prompts are too rigid
        - AI output feels robotic or formulaic
        - can't adapt to new contexts
      intent:
        - maintain brand consistency
        - enable creative variation
        - adapt to context
      code:
        - prompts specify exact structures
        - no room for contextual adaptation
        - "always" and "never" overused
    problem: |
      In pursuit of consistency, prompts become so prescriptive they kill
      creativity. "Always use this exact sentence structure." "Never vary
      from 3-paragraph format." AI can't adapt to different contexts or
      opportunities.

      Brand guidelines should enable creativity within constraints, not
      prevent it. A social media post needs different structure than email
      newsletter. Urgent product announcement needs different tone than
      evergreen content.
    consequence: |
      - AI output feels generic and repetitive
      - Can't adapt to new use cases or opportunities
      - Team circumvents prompts to be creative (breaks consistency)
      - Brand becomes boring and predictable
      - Competitive disadvantage
    solution: |
      1. Define brand constraints (voice, visual style) but allow variation
      2. Create context-specific prompts vs. one-size-fits-all
      3. Include "variation dimensions" in prompts (what CAN vary within brand)
      4. Encourage experimentation with new prompt approaches
      5. Monthly prompt review: promote successful innovations to library
      6. Balance consistency with creative freedom

  - title: Under-investing in AI brand training
    severity: medium
    detection:
      context:
        - using 2-3 examples to train AI on brand
        - expecting AI to infer brand from brief description
        - every AI output requires heavy editing
      intent:
        - train AI on brand quickly
        - get AI generating on-brand content
      code:
        - minimal training examples provided
        - no custom GPTs or Claude Projects
        - no style references for visual AI
    problem: |
      Team wants AI to understand brand from a few examples and brief
      description. But AI learns from patterns across many examples, not
      inference from few. Visual style needs 15-25 anchor images. Voice
      needs 50+ examples across contexts.

      Without sufficient training, AI lacks context to generate consistently.
      Every output requires editing. Team gets frustrated, declares "AI can't
      do our brand," and abandons effort. But problem wasn't AI - it was
      insufficient training.
    consequence: |
      - AI output quality is poor and inconsistent
      - Every piece requires heavy editing (negates efficiency)
      - Team frustration with AI tools
      - Missed opportunity to scale brand content
      - Competitive disadvantage vs. brands that invest in training
    solution: |
      1. Visual: Curate 15-25 brand anchor images minimum
      2. Voice: Collect 50+ examples across content types
      3. Build Custom GPTs with full brand knowledge base
      4. Create Midjourney style references (--sref)
      5. Train Flux LoRAs for critical visual use cases
      6. Invest time upfront in training to save editing time later

  - title: No quality benchmarks - "good enough" becomes "good"
    severity: medium
    detection:
      context:
        - unclear what "on brand" means
        - subjective debates about quality
        - acceptance of mediocre AI output
      intent:
        - define quality standards
        - maintain high bar for brand
      code:
        - no gold standard examples defined
        - no scoring rubric for quality
        - no comparison against best work
    problem: |
      Without defined quality benchmarks, team has no objective standard for
      "on brand" vs. "off brand." Quality debates become subjective. Over
      time, standards drift downward - mediocre AI output accepted because
      it's "good enough" and editing takes effort.

      Best practice: Define 5-10 "gold standard" examples for each content
      type. These are your best performing brand content. New AI output must
      match or exceed these benchmarks.
    consequence: |
      - Quality standards drift downward over time
      - No objective criteria for "good" vs. "bad"
      - Team wastes time in subjective debates
      - Brand dilution through acceptance of mediocrity
      - Loss of competitive edge
    solution: |
      1. Define 5-10 gold standard examples per content type
      2. Create scoring rubric (1-10) for brand alignment
      3. Compare all new AI output against benchmarks
      4. Reject output below quality threshold
      5. Update prompts when output consistently underperforms
      6. Raise benchmarks over time as quality improves

  - title: Forgetting brand evolution - treating brand as static
    severity: medium
    detection:
      context:
        - brand has evolved but AI still uses old guidelines
        - prompts haven't been updated in 6+ months
        - AI output feels dated compared to new brand direction
      intent:
        - evolve brand over time
        - keep AI aligned with current brand
      code:
        - no versioning for brand guidelines
        - prompts created once, never updated
        - no changelog for brand changes
    problem: |
      Brands aren't static - they evolve. Positioning shifts. Visual style
      modernizes. Voice matures. But AI brand training often gets "set and
      forget." Prompts created at launch, never updated. AI keeps generating
      in old brand while team has moved on.

      New employees join, see AI output, assume THAT'S the current brand.
      Perpetuates outdated brand instead of current one. Creates confusion
      and inconsistency.
    consequence: |
      - AI generates outdated brand content
      - Inconsistency between AI output and current brand
      - New team members confused about real brand
      - Can't leverage AI for latest brand direction
      - Manual rework required for all AI content
    solution: |
      1. Version brand guidelines like code (v1.0, v1.1, v2.0)
      2. Document changelog: what changed and why
      3. Create parallel v2 prompt library when brand evolves
      4. Migrate AI training: update GPTs, style references, voice corpus
      5. A/B test old vs. new before full switch
      6. Set sunset dates for deprecated versions

  - title: No feedback loop - not learning from AI output performance
    severity: low
    detection:
      context:
        - generating AI content but not tracking performance
        - no data on which prompts produce best results
        - repeating same approaches regardless of outcomes
      intent:
        - improve AI output quality over time
        - optimize prompts based on results
      code:
        - no performance tracking on AI-generated content
        - no metadata linking content to prompts used
        - no systematic prompt optimization
    problem: |
      Team generates AI content but doesn't track which prompts produce best
      performing results. No connection between prompt used and business
      outcomes (engagement, conversions, sentiment). Can't identify which
      approaches work.

      Best practice: Tag all AI content with metadata (prompt version, model
      used, date). Track performance. Identify top performers. Analyze what
      made those prompts effective. Update prompt library with learnings.
    consequence: |
      - Can't optimize prompts based on real performance
      - Repeat same approaches even when they underperform
      - Miss insights about what resonates with audience
      - Slower improvement over time
      - Competitors who track feedback pull ahead
    solution: |
      1. Tag AI content with metadata: prompt version, model, date, approver
      2. Track performance: engagement, conversions, sentiment
      3. Monthly analysis: which prompts produced top performers?
      4. Extract patterns from winners
      5. Update prompt library with learnings
      6. Deprecate consistently underperforming prompts
      7. A/B test prompt variations to optimize
