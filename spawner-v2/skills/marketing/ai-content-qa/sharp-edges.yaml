id: ai-content-qa-sharp-edges
skill: ai-content-qa
version: 1.0.0

edges:
  - id: reviewing-own-work
    summary: Being your own QA
    severity: critical
    situation: |
      Creator reviews their own content before publishing. "I'll just give it a
      quick look" or "I know what to check for." Even experienced reviewers miss
      issues in their own work.
    why: |
      Your brain reads what you intended to write, not what's actually there.
      You know the context, so you fill in gaps readers won't. You made
      intentional choices, so you can't see them as potential problems.
      Fresh eyes find what tired eyes miss—every time.
    solution: |
      NEVER QA YOUR OWN WORK:

      FOR INDIVIDUALS:
      - Take a 24-hour break, then review (if no peer available)
      - Read aloud (forces different processing)
      - Change format (print it, change font)
      - Use text-to-speech to listen
      - Still not as good as fresh eyes

      FOR TEAMS:
      - Mandatory peer review before publish
      - Rotate QA assignments
      - Different person creates vs. reviews
      - Even quick 2-minute peer check helps

      MINIMUM VIABLE QA:
      If truly no option, at minimum:
      1. Write → Walk away (hours/overnight)
      2. Read on different device
      3. Read backward (last paragraph first)
      4. Read aloud

      # Fresh eyes are not optional.
    symptoms:
      - "I reviewed it myself"
      - Obvious errors make it to publish
      - Typos despite "proofreading"
      - Inconsistencies nobody caught
    detection_pattern: null

  - id: subjective-blocking
    summary: Killing content based on preference, not problems
    severity: critical
    situation: |
      QA rejects content because they "don't like it" or "would have done it
      differently." No objective issues identified—just subjective preference
      masquerading as quality control.
    why: |
      QA's job is quality, not creative direction. When QA becomes subjective,
      it kills creator confidence, slows velocity, and doesn't actually improve
      outcomes. "I don't like it" is not a QA finding. "It doesn't meet the
      brief" is.
    solution: |
      OBJECTIVE QA STANDARDS:

      VALID QA REJECTIONS:
      - Factually incorrect
      - Off-brief
      - Off-brand
      - Platform non-compliant
      - Grammatically incorrect
      - Legally problematic
      - Missing required elements

      INVALID QA REJECTIONS:
      - "I don't like it"
      - "I would have said it differently"
      - "It's not how I'd approach it"
      - "It's just not quite right" (without specifics)
      - Stylistic preferences

      THE TEST:
      Can you point to a specific standard, guideline,
      or requirement that's violated?
      - Yes → Valid finding
      - No → Preference, not QA

      IF YOU HAVE OPINIONS:
      - Separate "required changes" from "suggestions"
      - Label suggestions as optional
      - Explain why (learning opportunity)
      - Accept creator may decline

      # QA = quality standards, not personal preferences
    symptoms:
      - "I just don't like it"
      - No specific issues cited
      - Different reviewer = different result
      - Creator frustration at unclear feedback
    detection_pattern: null

  - id: missing-brief-check
    summary: Reviewing without reference to the original brief
    severity: high
    situation: |
      QA reviews content based on their interpretation of what it should be,
      rather than checking against the documented brief. "Looks good to me"
      without verifying it delivers on requirements.
    why: |
      Content can be beautifully written and completely wrong. The brief exists
      to define success criteria. Without checking against it, you're reviewing
      for general quality, not for purpose. Good content that misses the brief
      is bad content.
    solution: |
      BRIEF-BASED REVIEW:

      BEFORE REVIEWING:
      1. Get the brief (always)
      2. Read the brief (actually read it)
      3. List key requirements
      4. Build review checklist from brief

      REVIEW CHECKLIST:
      □ Primary objective addressed?
      □ Target audience appropriate?
      □ Key message present?
      □ Required elements included?
      □ Restrictions observed?
      □ Platform requirements met?

      IF NO BRIEF EXISTS:
      - Request one before review
      - Or document assumptions and verify
      - Never assume you know the goal

      BRIEF → CHECKLIST → REVIEW:
      Every time. No exceptions.

      # Can't check compliance without requirements
    symptoms:
      - '"Looks good to me" (no specifics)'
      - Content approved that misses goals
      - '"That''s not what we wanted" at stakeholder review'
      - Vague QA feedback
    detection_pattern: null

  - id: ai-hallucination-blindness
    summary: Trusting AI-generated facts without verification
    severity: high
    situation: |
      Reviewing AI-generated content and accepting claims, statistics, or facts
      without verification. "The AI said it, it must be right." AI hallucinates
      confidently.
    why: |
      AI models confabulate—they generate plausible-sounding but false information
      with complete confidence. Numbers, quotes, statistics, even company
      information can be entirely fabricated. Publishing false claims damages
      credibility and can have legal consequences.
    solution: |
      AI FACT-CHECK PROTOCOL:

      VERIFY EVERYTHING:
      □ Statistics have sources
      □ Quotes are real and accurate
      □ Company/product claims are true
      □ Awards/recognitions exist
      □ Dates and timelines are correct
      □ Named people exist
      □ Testimonials are from real customers

      HIGH-RISK CLAIMS:
      - "Studies show..." → Which study? Link?
      - "Experts say..." → Which expert? Quote?
      - "[X]% of..." → Source for number?
      - "Award-winning..." → What award?
      - "Customers love..." → Documented testimonial?

      QUICK VERIFICATION:
      - Google the specific claim
      - Check company website
      - Search for the cited source
      - Ask creator for verification

      WHEN IN DOUBT:
      - Remove the claim
      - Rephrase without specific numbers
      - Add "can help" vs "will achieve"

      # AI confidence ≠ accuracy
    symptoms:
      - Impressive but unverified statistics
      - Quotes that can't be sourced
      - Too-good-to-be-true claims
      - Facts that don't match reality
    detection_pattern: null

  - id: platform-blindness
    summary: Not verifying platform-specific requirements
    severity: high
    situation: |
      Content passes QA but fails on the platform—truncated headlines, broken
      links, wrong aspect ratios, exceeded character limits. "It looked fine in
      the doc."
    why: |
      Each platform has specific technical requirements. Content that looks
      perfect in a document can break completely in context. Truncation wastes
      spend. Wrong specs waste production. QA must check actual platform
      constraints.
    solution: |
      PLATFORM VERIFICATION:

      META ADS:
      □ Primary text: Hook in first 125 chars
      □ Headline: Under 40 chars (25 mobile)
      □ Description: Under 30 chars
      □ Image: 1080x1080 or 1200x628
      □ Video: 1:1 or 4:5 aspect ratio
      □ 20% text rule (check tool)

      GOOGLE ADS:
      □ Headlines: 30 chars each (max 15)
      □ Descriptions: 90 chars each (max 4)
      □ Display URL: 15 chars each path
      □ No exclamation in headline
      □ No all caps words

      LINKEDIN:
      □ Intro text: 150 chars visible
      □ Headline: Under 70 chars
      □ Image: 1200x627

      ALWAYS:
      □ Preview in platform (or simulator)
      □ Check mobile rendering
      □ Test all links
      □ Verify UTM parameters

      # Check in context, not just in docs
    symptoms:
      - "It got truncated"
      - Broken links in live ads
      - Wrong aspect ratio
      - "It looked different in the doc"
    detection_pattern: null

  - id: bottleneck-qa
    summary: QA as a traffic jam, not quality checkpoint
    severity: medium
    situation: |
      QA becomes the slowest part of the process. Content backs up waiting for
      review. Deadlines missed because QA couldn't keep up. "We're waiting on QA."
    why: |
      QA that slows production is failing. Quality can't be an excuse for
      blocking velocity—you need systems that scale. If QA is the bottleneck,
      either process or staffing needs to change.
    solution: |
      SCALING QA:

      TIER YOUR REVIEW:
      - Tier 1: Speed review (2 min) - low risk
      - Tier 2: Standard review (10 min) - medium risk
      - Tier 3: Deep review (30 min) - high risk

      CLASSIFY CONTENT:
      Low risk: Social posts, variations, minor edits
      Medium risk: New campaigns, landing pages
      High risk: High spend, legal, new positioning

      AUTOMATION:
      - Spell check (automated)
      - Grammar check (automated)
      - Character counts (automated)
      - Link check (automated)
      - Brand term check (automated)

      Human review for judgment calls only.

      SLA SYSTEM:
      - Define turnaround times by tier
      - Track and report on SLAs
      - Staff to meet SLAs

      SELF-SERVE QA:
      - Checklists creators can run
      - Templates pre-validated
      - Style guides accessible

      # Fast AND good, not fast OR good
    symptoms:
      - "Waiting on QA"
      - Content backs up
      - Rush reviews become standard
      - QA blamed for missed deadlines
    detection_pattern: null

  - id: catching-not-teaching
    summary: Finding errors without building capability
    severity: medium
    situation: |
      QA catches the same errors repeatedly from the same creators. Issues are
      fixed but not documented. Patterns aren't shared. Creators keep making the
      same mistakes.
    why: |
      QA that only catches errors is a patch, not a solution. The goal is to
      raise quality at the source. If creators keep making the same mistakes,
      QA isn't teaching—it's just gatekeeping. Systemic improvement > error
      correction.
    solution: |
      TEACHING QA:

      TRACK PATTERNS:
      - Log common errors by creator
      - Log common errors by content type
      - Identify systemic issues

      SHARE LEARNINGS:
      - Weekly "common issues" roundup
      - Document in style guide
      - Create checklists from patterns

      FEEDBACK LOOP:
      - Don't just fix—explain why
      - Show impact of the error
      - Provide correct example
      - Check if understood

      BUILD CAPABILITY:
      - Self-serve checklists
      - Pre-flight tools
      - Training on common issues
      - Templates that prevent errors

      MEASURE IMPROVEMENT:
      - Track error rates over time
      - Goal: same error shouldn't repeat
      - Celebrate improvement

      # Teach the system, not just catch the errors
    symptoms:
      - Same errors repeated
      - No documentation of patterns
      - No training from QA findings
      - Error rate stays flat
    detection_pattern: null

  - id: perfectionism-trap
    summary: Holding content to impossible standards
    severity: medium
    situation: |
      QA rejects content that's "good enough" in pursuit of perfect. Endless
      revision cycles. "It could be better." Nothing ships because nothing is
      ever quite right.
    why: |
      Perfect is the enemy of shipped. Every piece of content has diminishing
      returns on revision. The difference between 95% and 100% often doesn't
      impact outcomes—but the delay does. "Good enough" needs to be defined and
      defended.
    solution: |
      DEFINING GOOD ENOUGH:

      BY CONTENT TYPE:
      - Quick social: 80% good enough
      - Ad campaign: 90% good enough
      - Hero content: 95% good enough
      - Legal/compliance: 100% required

      GOOD ENOUGH = :
      □ Meets brief requirements
      □ On brand
      □ No errors
      □ Platform compliant
      □ CTA works

      NOT REQUIRED:
      - Perfect word choice
      - Optimal structure
      - Reviewer would love it
      - Best possible version

      THE TIME TEST:
      Will 2 more hours of revision meaningfully
      improve outcomes?
      - Yes → Worth it
      - No → Ship it

      DECISION AUTHORITY:
      QA can require: Must meet standards
      QA can suggest: Could be better
      QA cannot block: "Just don't love it"

      # Done > perfect. Always.
    symptoms:
      - Endless revision cycles
      - "It's not quite there yet"
      - Nothing ever ships
      - Creators lose motivation
    detection_pattern: null

  - id: inconsistent-standards
    summary: Different reviewers apply different criteria
    severity: medium
    situation: |
      Content passes with one reviewer, fails with another. Standards seem to
      depend on who's reviewing. "It depends on who you get." No consistent
      quality bar.
    why: |
      Inconsistent QA is worse than no QA—it creates confusion, frustration,
      and unpredictability. Creators can't learn what "good" means. Quality
      becomes political. Trust in the process erodes.
    solution: |
      STANDARDIZING QA:

      WRITTEN STANDARDS:
      - Document what passes/fails
      - Include examples (good and bad)
      - Specify required vs. optional
      - Define edge cases

      CALIBRATION:
      - Regular calibration sessions
      - Review same content, compare notes
      - Align on edge cases
      - Update standards from learnings

      QA RUBRIC:
      - Objective criteria checklist
      - Scoring system if needed
      - Clear pass/fail thresholds

      ESCALATION PATH:
      - If reviewers disagree, process for resolution
      - Senior reviewer or documented standard wins
      - Not "louder opinion wins"

      QUALITY OF QA:
      - Audit QA decisions periodically
      - Track consistency across reviewers
      - Address outliers

      # Same standards, regardless of reviewer
    symptoms:
      - "Depends who reviews it"
      - Reviewer shopping
      - Inconsistent feedback
      - No clear standard
    detection_pattern: null

  - id: surface-only-review
    summary: Catching typos but missing strategic errors
    severity: low
    situation: |
      QA catches spelling errors but misses that the headline buries the benefit,
      the message conflicts with another campaign, or the CTA doesn't match the
      objective. Polished content that fails strategically.
    why: |
      Surface quality is necessary but not sufficient. Content can be perfectly
      spelled and completely ineffective. Strategic errors are more expensive
      than tactical ones—they waste entire campaigns, not just impressions.
    solution: |
      MULTI-LEVEL REVIEW:

      REVIEW ORDER (big to small):
      1. BRAND: Does it belong to us?
      2. STRATEGY: Does it achieve the goal?
      3. TECHNICAL: Will it work on platform?
      4. TACTICAL: Is it polished?

      WHY THIS ORDER:
      Don't proofread content that fails strategically.
      Fix big problems before small ones.

      STRATEGIC CHECKS:
      □ Key message clear and prominent
      □ Benefit-led, not feature-led
      □ Single message (not multiple)
      □ CTA matches objective
      □ Audience appropriate
      □ Consistent with other content

      DON'T STOP AT SPELLING:
      - Read for meaning, not just errors
      - Ask "will this work?" not just "is this correct?"
      - Check against goals, not just grammar

      # Perfect spelling on a bad ad is still a bad ad
    symptoms:
      - "It passed QA but didn't perform"
      - Strategic issues in published content
      - Perfect grammar, wrong message
      - QA only catching surface issues
    detection_pattern: null
