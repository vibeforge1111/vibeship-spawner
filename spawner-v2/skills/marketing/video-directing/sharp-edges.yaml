id: video-directing
version: 1.0.0

sharp_edges:
  - id: crossing-the-line
    summary: The 180-degree rule creates spatial confusion when broken unintentionally
    severity: high
    situation: When cutting between shots in a dialogue or action sequence
    why: |
      The 180-degree rule maintains spatial consistency for viewers. An imaginary
      line connects the subjects—staying on one side keeps screen direction consistent.
      Cross it without intent, and characters suddenly seem to swap positions, confusing
      the audience about who's where.
    solution: |
      1. Establish your axis (the line between subjects)
      2. All cameras stay on one side of the line
      3. If you MUST cross: use a neutral shot (straight on), a moving shot
         that crosses during movement, or cut to a close insert

      For AI Generation:
      - Specify camera position in prompts: "camera at 45 degrees left of subject"
      - Generate matching angles: if one character faces right, other faces left
      - When in doubt, generate frontal angles that work with any cut
    detection_pattern:
      context: [dialogue, conversation, two-shot, reverse, over-the-shoulder]
      intent: [cut, edit, sequence, coverage]

  - id: jump-cut-jarring
    summary: Jump cuts within the same angle are jarring without 30-degree rule
    severity: medium
    situation: When cutting between takes of the same subject/angle
    why: |
      If you cut between two shots of similar angle (less than 30 degrees apart),
      the subject appears to "jump" in frame. This is jarring unless intentionally
      stylized (French New Wave, YouTube talking head style).
    solution: |
      1. Change angle by at least 30 degrees between cuts
      2. Or change shot size significantly (wide to close-up)
      3. Or use insert/cutaway to bridge the cut
      4. Or embrace it as style (match with music/energy)

      For AI Generation:
      - Generate 3 versions: wide, medium, close-up for each moment
      - Use angle variations in prompts explicitly
      - Generate B-roll specifically for covering jump cuts
    detection_pattern:
      context: [same shot, same angle, talking head, interview]
      intent: [cut, trim, edit, sequence]

  - id: eyeline-mismatch
    summary: Mismatched eyelines break the illusion of subjects looking at each other
    severity: high
    situation: When cutting between characters in dialogue or reaction shots
    why: |
      If Character A looks screen-right, Character B must look screen-left for them
      to appear to be looking at each other. Mismatched eyelines make characters
      seem to be looking past each other—breaking the scene's reality.
    solution: |
      1. During shooting: mark eyeline targets on set
      2. A looks right + up = B must look left + down (and vice versa)
      3. For AI: specify eyeline direction in every prompt
      4. Generate reaction shots with consistent eyeline to master

      AI Prompt Template:
      "Subject looking [direction] at [height], matching eyeline for dialogue"
    detection_pattern:
      context: [dialogue, conversation, reaction, eye contact]
      intent: [match, cut, reverse, coverage]

  - id: empty-frames
    summary: AI video generation often creates too much empty space
    severity: medium
    situation: When generating AI video for cinematic use
    why: |
      AI models often default to centered subjects with lots of empty space.
      Professional cinematography uses deliberate framing—rule of thirds, leading
      room, balanced headroom. Empty frames feel amateurish.
    solution: |
      1. Specify framing in prompts: "rule of thirds", "looking room left"
      2. Request specific headroom: "close-up with minimal headroom"
      3. Post-process: crop and reframe generated footage
      4. Use wider generation, crop to intention

      Framing Terms for AI Prompts:
      - "extreme close-up, face filling frame"
      - "medium shot, rule of thirds positioning"
      - "subject positioned left with looking room right"
    detection_pattern:
      context: [AI generate, create video, footage, shot]
      intent: [composition, framing, cinematic]

  - id: pacing-uniformity
    summary: AI-generated clips often have uniform pacing that feels robotic
    severity: medium
    situation: When using AI video for sequences that need dynamic rhythm
    why: |
      AI models generate clips at consistent pace. Real cinematography varies—
      slow moments build tension, fast cuts create energy, pauses let emotion land.
      Uniform pacing feels mechanical and loses the viewer.
    solution: |
      1. Generate clips at varying lengths intentionally
      2. Use speed ramping in post (slow-mo, fast-mo)
      3. Cut AI footage to match music/emotional beats
      4. Intercut with static moments for contrast

      Pacing by Emotion:
      - Tension building: longer shots, slow push-ins
      - Action/excitement: shorter shots, faster cuts
      - Emotional beats: hold on faces, let moments breathe
      - Resolution: return to longer shots, calm rhythm
    detection_pattern:
      context: [AI video, generated footage, sequence, edit]
      intent: [rhythm, pacing, tempo, cut]

  - id: lighting-inconsistency
    summary: AI-generated shots often have inconsistent lighting across a sequence
    severity: high
    situation: When generating multiple AI video shots for the same scene
    why: |
      Each AI generation creates its own lighting conditions. Cut between them,
      and the light direction/color/intensity jumps—destroying the illusion that
      these shots exist in the same space and time.
    solution: |
      1. Specify identical lighting in every prompt for a scene
      2. Use reference images with consistent lighting
      3. Prompt: "consistent with previous shot, same golden hour lighting"
      4. Post-process: color grade to match across shots

      Lighting Prompt Template:
      "[shot description], [specific light direction e.g., key light from left],
      [color temperature e.g., warm 5600K], [time of day e.g., golden hour],
      [style reference e.g., Roger Deakins style]"
    detection_pattern:
      context: [AI generate, multiple shots, sequence, scene]
      intent: [lighting, match, consistent, continuity]

  - id: temporal-inconsistency
    summary: AI video can have physics glitches, temporal stutters, and impossible movements
    severity: high
    situation: When using AI video for realistic footage
    why: |
      AI models don't understand physics—objects morph, gravity fails, movements
      skip frames. These artifacts immediately break immersion and signal "AI-generated"
      to viewers, even unconsciously.
    solution: |
      1. Review every frame, not just beginning and end
      2. Use shorter generations (more control, fewer artifacts)
      3. Cut away before artifacts (use B-roll to hide them)
      4. Regenerate problem sections, don't try to fix
      5. Composite: use AI for backgrounds, real footage for critical action

      Quality Checklist:
      □ Do hands/fingers render correctly throughout?
      □ Does motion flow without stutters?
      □ Do objects maintain consistent form?
      □ Does physics make sense (gravity, momentum)?
    detection_pattern:
      context: [AI video, generated, footage, realistic]
      intent: [review, quality, check, artifacts]

  - id: coverage-gaps
    summary: Shooting without comprehensive coverage leaves editors with no options
    severity: high
    situation: When planning shots for a scene that will be edited
    why: |
      Directors often shoot the "hero shots" they envision but miss the connective
      tissue—the insert shots, reactions, cutaways. In edit, you discover you can't
      cut around problems because you have no coverage.
    solution: |
      1. Always shoot: wide master, medium, close-ups, inserts
      2. Get "safety shots"—neutral footage that can bridge any cut
      3. Shoot 3x more B-roll than you think you need
      4. Get reaction shots even when dialogue is finished

      Coverage Minimum per Scene:
      □ Wide master (whole scene in one shot)
      □ Medium two-shot
      □ Singles on each character
      □ Inserts (hands, objects, details)
      □ Reaction shots (nodding, listening, thinking)
      □ Entry and exit shots
    detection_pattern:
      context: [shot list, coverage, scene, shooting]
      intent: [plan, shoot, capture, film]

  - id: actor-direction-vague
    summary: Vague direction to performers leads to generic, uncompelling performances
    severity: medium
    situation: When directing talent (or prompting AI digital humans)
    why: |
      Telling an actor "be sad" or "look happy" gives them nothing to work with.
      Great directors give specific, playable actions—what does the character WANT
      in this moment? What are they DOING to get it?
    solution: |
      1. Direct actions, not emotions: "Try to make them laugh" vs "be happy"
      2. Give obstacles: "You need to tell them, but you don't want to hurt them"
      3. Use sense memory: "Remember the last time you lost something precious"
      4. Be specific: "On this word, you realize she knows"

      For AI/Digital Humans:
      - Prompt with specific micro-expressions
      - Describe the thought behind the look, not just the emotion
      - "Subject suppressing a smile, trying to stay serious"
      - "Eyes searching for hope but expecting disappointment"
    detection_pattern:
      context: [perform, act, expression, direction, digital human]
      intent: [direct, instruct, emotional, react]

  - id: scope-before-story
    summary: Prioritizing visual spectacle over emotional story
    severity: medium
    situation: When planning ambitious visual sequences
    why: |
      It's tempting—especially with AI tools—to lead with "cool shots" rather than
      story necessity. But spectacle without emotional stakes is hollow. Audiences
      don't remember the shot—they remember how it made them feel.
    solution: |
      1. For every "big shot," ask: what emotion does this serve?
      2. Earn the spectacle with story setup
      3. James Cameron's rule: the most expensive shot must be the most emotional
      4. Use spectacle to reveal character, not replace it

      Story Questions Before Any Shot:
      - What does the audience need to feel?
      - What does the character want?
      - How does this shot advance that?
      - Would the story work without this shot?
    detection_pattern:
      context: [epic, spectacle, ambitious, scale, visual]
      intent: [plan, design, create, shoot]

  - id: ai-over-reliance
    summary: Letting AI make creative decisions that should be the director's
    severity: high
    situation: When using AI video generation without clear directorial intent
    why: |
      AI generates possibilities—but it doesn't understand story, emotion, or what
      the sequence needs. Using AI without vision creates generic content. The director
      must know what they want BEFORE asking AI to create it.
    solution: |
      1. Write the shot list before touching AI tools
      2. Describe each shot's purpose in one sentence
      3. Reject AI generations that don't serve the vision
      4. AI is a tool—you're still the director

      Before Any AI Generation:
      □ What emotion should viewer feel?
      □ Where does this shot sit in the sequence?
      □ What comes before and after?
      □ What specific visual serves this moment?
      □ What should I reject, no matter how "cool" it looks?
    detection_pattern:
      context: [AI generate, create, footage, video]
      intent: [make, create, generate, produce]

  - id: forgetting-audio
    summary: Treating audio as secondary to visuals in planning
    severity: high
    situation: When planning visual sequences without audio consideration
    why: |
      Directors obsess over shots but forget that audio carries half the experience.
      A mediocre shot with perfect audio beats a beautiful shot with bad audio.
      Sound design, music, and silence are directing tools as powerful as the camera.
    solution: |
      1. Plan audio alongside visuals from pre-production
      2. Know where music hits, where silence matters
      3. Consider practical sound in every location
      4. For AI: generate audio references before visuals

      Audio Director Questions:
      - What should the audience HEAR in this moment?
      - Is there music? Does it start/stop/shift?
      - What's the ambient sound of this world?
      - Where is deliberate silence powerful?
    detection_pattern:
      context: [scene, sequence, shot, plan, storyboard]
      intent: [plan, design, create, shoot]
