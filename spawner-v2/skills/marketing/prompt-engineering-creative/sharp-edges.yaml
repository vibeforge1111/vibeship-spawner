id: prompt-engineering-creative-sharp-edges
skill: prompt-engineering-creative
version: 1.0.0

edges:
  - id: model-agnostic-prompting
    summary: Using same prompt across different models expecting same results
    severity: high
    situation: |
      Writing one prompt and expecting it to work identically across Midjourney,
      DALL-E, Flux, Suno, etc. Each model has different prompt languages.
    why: |
      Models are trained differently, parse prompts differently, and have
      different strengths. A prompt optimized for Midjourney fails on DALL-E.
      You waste generations and budget trying to force one prompt everywhere.
    solution: |
      MODEL-SPECIFIC PROMPT LANGUAGES:

      MIDJOURNEY:
      - Understands artistic terms well
      - ::weights for emphasis (subject::2 background::0.5)
      - --ar, --v, --style parameters
      - Artist names work but legal risk
      - "in the style of" effective

      DALL-E 3:
      - Natural language prompts
      - Detailed descriptions work well
      - System message for context
      - Avoids copyrighted content automatically

      FLUX:
      - Technical photography terms
      - Camera/lens specifications effective
      - Natural language but precise
      - Good with lighting descriptions

      STABLE DIFFUSION:
      - Weighted tokens: (word:1.2)
      - Negative prompts critical
      - Embedding names if using
      - LoRA triggers

      SUNO/MUSIC:
      - Genre + subgenre specific
      - BPM and tempo terms
      - Instrument lists
      - Mood vocabulary

      # Create prompt variants per model
      # Don't assume transferability
    symptoms:
      - Inconsistent results across models
      - "Works on X but not Y"
      - Wasted generations
      - Confusion about what's wrong
    detection_pattern: null

  - id: front-loading-failure
    summary: Burying important elements at end of prompt
    severity: high
    situation: |
      Writing prompts with subject/important elements at the end, after
      lengthy descriptions of style, mood, or setting.
    why: |
      Most models weight earlier tokens more heavily. If your main subject
      is at position 80 in a 100-token prompt, it gets less attention.
      The model may focus on whatever came first instead.
    solution: |
      FRONT-LOADING PRINCIPLE:

      Order of importance = order in prompt

      STRUCTURE:
      1. Subject (what/who is this)
      2. Action (what are they doing)
      3. Setting (where is this)
      4. Style (how does it look)
      5. Technical (camera, aspect ratio)

      # BAD
      "In a beautiful cinematic style with golden hour lighting
      and bokeh background, there is a portrait of a woman"

      # GOOD
      "Portrait of a woman with auburn hair. Golden hour lighting.
      Cinematic style. Bokeh background. 85mm lens."

      EXCEPTIONS:
      - "In the style of X" can lead if style is most important
      - Some models (Midjourney) allow flexible ordering
      - Test and adjust per model
    symptoms:
      - Wrong element emphasized
      - Subject unclear or secondary
      - Style overpowering subject
      - "It focused on the wrong thing"
    detection_pattern: 'prompt.*\.\s*(portrait|photo|image|shot|scene) of'

  - id: vague-adjectives
    summary: Using vague adjectives instead of specific descriptions
    severity: medium
    situation: |
      Using words like "beautiful", "cool", "nice", "amazing", "perfect"
      that have no concrete visual or auditory meaning.
    why: |
      AI models can't interpret subjective adjectives. "Beautiful" means
      different things to different people. The model will apply its
      training bias, which may not match your intention.
    solution: |
      SPECIFIC OVER VAGUE:

      Instead of:          Use:
      ─────────────────────────────────────
      "beautiful"       → "symmetrical features, clear skin"
      "cool"           → "cyberpunk aesthetic, neon lighting"
      "nice"           → "warm, inviting, soft lighting"
      "perfect"        → "flawless, high-detail, crisp"
      "epic"           → "grand scale, dramatic perspective"
      "amazing"        → [describe what makes it amazing]
      "interesting"    → [describe the specific interest point]

      ASK: "What would this look/sound like specifically?"

      EXCEPTION:
      - Some models understand "beautiful" as a style
      - But pair with specifics: "beautiful in the style of..."
    symptoms:
      - Generic output
      - "Not what I imagined"
      - Random interpretation
      - Inconsistent results
    detection_pattern: 'prompt.*(beautiful|amazing|cool|nice|perfect|epic)\b(?!ly)'

  - id: negative-prompt-neglect
    summary: Not using negative prompts to exclude unwanted elements
    severity: medium
    situation: |
      Getting unwanted elements (text, watermarks, artifacts, wrong style)
      without specifying what to avoid in negative prompts.
    why: |
      Negative prompts are as important as positive. Without them, you get
      model defaults which often include unwanted elements. Garbage prevention
      is as important as beauty creation.
    solution: |
      NEGATIVE PROMPT ESSENTIALS:

      UNIVERSAL NEGATIVES:
      - "blurry, low quality, distorted"
      - "watermark, text, logo, signature"
      - "cropped, cut off, out of frame"

      PEOPLE NEGATIVES:
      - "bad anatomy, extra limbs, deformed hands"
      - "extra fingers, mutated, disfigured"
      - "ugly, duplicate, morbid"

      STYLE NEGATIVES:
      - For photorealism: "cartoon, illustration, painting, anime"
      - For illustration: "photorealistic, photograph, 3d render"
      - For professional: "amateur, snapshot, low resolution"

      MODEL-SPECIFIC:
      - Midjourney: --no parameter
      - SD/Flux: Separate negative prompt field
      - DALL-E: Include in prompt "without X" or "no X"

      # Always have negative prompts ready
      # Build a library for common use cases
    symptoms:
      - Unwanted elements appear
      - Random text/watermarks
      - Artifacts and distortions
      - Wrong style mixing
    detection_pattern: null

  - id: over-prompting
    summary: Writing prompts that are too long and confusing
    severity: medium
    situation: |
      Writing 500-word prompts trying to specify every possible detail,
      causing the model to get confused or ignore parts.
    why: |
      Models have token limits and attention limits. Too much information
      causes parts to be ignored or conflicting instructions to create
      confused output. More is not always better.
    solution: |
      PROMPT LENGTH GUIDELINES:

      IMAGE MODELS (typical):
      - Sweet spot: 50-150 words
      - Max effective: ~200 words
      - Beyond: diminishing returns

      VIDEO MODELS:
      - Shorter is better: 30-75 words
      - Focus on key elements
      - Action over description

      MUSIC MODELS:
      - Genre + mood + instruments: 20-50 words
      - Structure if needed: +20 words
      - Keep focused

      PRUNING TECHNIQUE:
      1. Write detailed prompt
      2. Cut anything repeated
      3. Remove vague adjectives
      4. Prioritize top 5 elements
      5. Test shorter version

      # If output is confused, try LESS not MORE
    symptoms:
      - Confused output
      - Elements missing
      - Contradictory results
      - Model ignoring parts
    detection_pattern: 'prompt.*"[^"]{800,}"'

  - id: style-reference-without-context
    summary: Using artist names or style references without specificity
    severity: medium
    situation: |
      Using "in the style of X" without specifying which aspects of that
      style you want, getting inconsistent interpretations.
    why: |
      Artists have multiple periods, styles, and aspects. "Picasso style"
      could mean cubism, blue period, or rose period. The model picks
      randomly without guidance.
    solution: |
      SPECIFIC STYLE REFERENCES:

      Instead of: "in the style of Picasso"
      Use: "cubist style with geometric fragmentation, multiple
      perspectives, muted earth tones, like Picasso's analytical
      cubism period"

      STYLE ELEMENTS TO SPECIFY:
      - Color palette
      - Line quality (bold, delicate, sketchy)
      - Composition approach
      - Lighting style
      - Texture/material quality
      - Time period if relevant

      SAFE APPROACH:
      - Describe the style without naming the artist
      - Use movement names (impressionism, art nouveau)
      - Combine multiple influences

      # Describe the aesthetic, not just the name
    symptoms:
      - Wrong style interpretation
      - Inconsistent style application
      - "Not what I meant"
      - Random period/aspect selected
    detection_pattern: 'style of \w+(?!\s+(period|era|movement|technique|approach))'

  - id: ignoring-aspect-ratio-composition
    summary: Not considering how aspect ratio affects composition
    severity: medium
    situation: |
      Generating images without considering how the aspect ratio affects
      what can fit in frame and how it will be composed.
    why: |
      A horizontal prompt in a vertical frame won't work. "Person running
      across a field" needs wide aspect. Portrait needs vertical. The
      composition changes what's possible.
    solution: |
      ASPECT RATIO COMPOSITION:

      16:9 / 3:2 (Landscape):
      - Horizontal scenes
      - Multiple subjects side by side
      - Panoramic views
      - Action flowing left-right

      9:16 / 2:3 (Portrait):
      - Vertical subjects (people standing)
      - Tall elements (buildings, trees)
      - Social media stories/reels
      - Top-to-bottom flow

      1:1 (Square):
      - Centered subjects
      - Circular compositions
      - Symmetrical designs
      - Instagram posts

      PROMPTING:
      - "vertical composition" or "portrait orientation"
      - "wide panoramic view" or "landscape orientation"
      - "centered, symmetrical composition"

      # Match prompt content to aspect ratio
    symptoms:
      - Awkward cropping
      - Subjects cut off
      - Wasted space
      - Compositional failure
    detection_pattern: null

  - id: prompt-injection-vulnerability
    summary: User input concatenated into prompts without sanitization
    severity: critical
    situation: |
      Building apps where user input becomes part of the prompt, allowing
      users to inject instructions that override your prompt.
    why: |
      Users can inject "ignore previous instructions" or harmful content.
      Can bypass safety filters, generate unwanted content, or break your
      application. Security vulnerability.
    solution: |
      PROMPT INJECTION PREVENTION:

      1. INPUT SANITIZATION:
         ```typescript
         function sanitize(input: string): string {
           return input
             .replace(/ignore.*instruction/gi, '')
             .replace(/system.*prompt/gi, '')
             .replace(/\n/g, ' ')
             .slice(0, 200);
         }
         ```

      2. STRUCTURED PROMPTS:
         ```typescript
         // User input in specific slot only
         const prompt = `Create a ${sanitize(productType)} on white background.`;
         ```

      3. CONTENT FILTERING:
         - Pre-filter user input
         - Block known attack patterns
         - Use provider safety filters

      4. SEPARATION:
         - Don't concatenate directly
         - Use template with placeholders
         - Validate against schema
    symptoms:
      - Unexpected content
      - Filters bypassed
      - Prompt structure broken
      - Security incidents
    detection_pattern: 'prompt.*\\+.*user|`.*\\$\\{.*user'

  - id: temperature-misunderstanding
    summary: Not understanding how randomness/temperature settings affect output
    severity: medium
    situation: |
      Using default or wrong temperature/randomness settings, getting either
      too random or too boring/repetitive outputs.
    why: |
      Temperature controls creativity vs consistency. High = more random,
      creative, but possibly incoherent. Low = more predictable, consistent,
      but possibly boring. Wrong setting for use case fails.
    solution: |
      TEMPERATURE GUIDELINES:

      IMAGE MODELS (where applicable):
      - Low (0.1-0.3): Consistent, reproducible
      - Medium (0.5-0.7): Balanced
      - High (0.8-1.0): Creative, varied

      TEXT/LLM PROMPTS:
      - 0: Deterministic
      - 0.3-0.5: Focused, reliable
      - 0.7: Balanced (common default)
      - 1.0+: Creative, unpredictable

      MUSIC:
      - Low: More conventional structure
      - High: More experimental

      USE CASES:
      - Consistency needed: Low temperature
      - Variety needed: High temperature
      - Single best output: Low temp, multiple attempts
      - Exploration: High temp, curate best
    symptoms:
      - Too similar outputs
      - Too random outputs
      - '"Boring" or "chaotic"'
      - Wrong level of creativity
    detection_pattern: null

  - id: seed-confusion
    summary: Not understanding how seeds affect reproducibility
    severity: low
    situation: |
      Trying to reproduce a result without using the same seed, or
      expecting same seed to produce identical results across contexts.
    why: |
      Seeds enable reproducibility but with caveats. Same seed + same prompt
      + same settings = similar result. Change anything and the seed alone
      won't save you.
    solution: |
      SEED USAGE:

      FOR REPRODUCIBILITY:
      - Save: prompt + seed + model + all settings
      - Same everything = reproducible
      - Different prompt + same seed = different result

      FOR VARIATIONS:
      - Keep prompt and settings
      - Change seed for variations
      - Batch with sequential seeds

      LIMITATIONS:
      - Model updates may break reproducibility
      - Different model versions = different results
      - Some randomness may exist regardless

      WORKFLOW:
      ```typescript
      const generation = {
        prompt: "...",
        seed: 42,
        model: "flux-dev",
        settings: {...}
      };
      // Save this entire object, not just seed
      ```
    symptoms:
      - Can't reproduce result
      - Seed not working as expected
      - Lost generation settings
      - Inconsistent series
    detection_pattern: null

  - id: copy-paste-prompts
    summary: Using others' prompts without understanding structure
    severity: low
    situation: |
      Copying prompts from galleries or communities without understanding
      why they work, then being unable to modify or debug them.
    why: |
      Prompts are specific to context, model version, and intent. A copied
      prompt may work differently for you or stop working when the model
      updates. Without understanding, you can't adapt.
    solution: |
      LEARNING FROM PROMPTS:

      1. DECONSTRUCT:
         - Identify each element's purpose
         - What's the subject? Style? Technical?
         - Why those specific words?

      2. UNDERSTAND MODEL:
         - What model was this for?
         - Does it work on your model?
         - What's different?

      3. ADAPT, DON'T COPY:
         - Take principles, not exact text
         - Modify for your use case
         - Build your own vocabulary

      4. BUILD LIBRARY:
         - Create personal prompt templates
         - Document what works and why
         - Iterate and refine

      # Learn to fish, don't just copy fish
    symptoms:
      - Can't debug when it fails
      - Can't modify for new needs
      - Dependent on others' prompts
      - No improvement over time
    detection_pattern: null
