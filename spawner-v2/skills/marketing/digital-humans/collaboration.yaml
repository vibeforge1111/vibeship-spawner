id: digital-humans-collaboration
skill: digital-humans
version: 1.0.0

# ============================================================================
# PREREQUISITES
# ============================================================================
prerequisites:
  required:
    - name: Digital Human Platform (at least one)
      options:
        - provider: HeyGen
          signup: https://heygen.com/
          env_var: HEYGEN_API_KEY
          pricing: $24+/month, ~$0.10-0.20/minute
          best_for: Best lip sync, most natural movement
        - provider: Synthesia
          signup: https://synthesia.io/
          env_var: SYNTHESIA_API_KEY
          pricing: $22+/month, ~$0.20-0.40/minute
          best_for: Enterprise, 140+ languages
        - provider: D-ID
          signup: https://d-id.com/
          env_var: DID_API_KEY
          pricing: Pay per video, ~$0.05-0.15/minute
          best_for: Photo animation, API flexibility
        - provider: Tavus
          signup: https://tavus.io/
          env_var: TAVUS_API_KEY
          pricing: Custom/enterprise
          best_for: Personalized video at scale

  recommended:
    - name: Voice synthesis API
      options:
        - ElevenLabs (best quality)
        - Platform built-in voices
      reason: Custom voice often better than platform defaults

    - name: Video editing software
      options:
        - DaVinci Resolve (free)
        - Premiere Pro
        - CapCut
      reason: Avatar footage often needs editing, compositing

# ============================================================================
# MCP TOOL CONFIGURATIONS
# ============================================================================
mcp_tools:
  # HeyGen MCP
  - name: heygen
    description: AI avatar video generation
    install: npx -y @anthropic/mcp-installer install heygen
    config:
      server:
        command: npx
        args: ["-y", "@heygen/mcp-server"]
        env:
          HEYGEN_API_KEY: "${HEYGEN_API_KEY}"
    capabilities:
      - avatar-video-generation
      - custom-avatar-creation
      - voice-selection
      - multi-language
      - batch-personalization
    example_usage: |
      Use heygen to generate avatar video:
      - Avatar: Stock avatar ID or custom
      - Script: "Welcome to our product..."
      - Voice: Select from library or ElevenLabs
      - Background: Green screen or preset

  # D-ID MCP
  - name: d-id
    description: Photo animation and talking avatars
    install: npx -y @anthropic/mcp-installer install d-id
    config:
      server:
        command: npx
        args: ["-y", "@d-id/mcp-server"]
        env:
          DID_API_KEY: "${DID_API_KEY}"
    capabilities:
      - photo-to-video
      - talking-portrait
      - voice-synthesis
      - emotion-control
    example_usage: |
      Use d-id to animate a photo:
      - Source: Upload photo or use preset
      - Script: Text to speak
      - Voice: Select or provide audio
      - Emotion: Neutral, happy, etc.

  # Synthesia MCP (if available)
  - name: synthesia
    description: Enterprise AI video generation
    install: npx -y @anthropic/mcp-installer install synthesia
    config:
      server:
        command: npx
        args: ["-y", "@synthesia/mcp-server"]
        env:
          SYNTHESIA_API_KEY: "${SYNTHESIA_API_KEY}"
    capabilities:
      - avatar-video-generation
      - 140+-languages
      - custom-avatars
      - brand-templates
      - team-collaboration
    example_usage: |
      Use synthesia to create training video:
      - Template: Corporate training
      - Avatar: Professional presenter
      - Script: Training content
      - Language: Select target language

# ============================================================================
# API INTEGRATIONS
# ============================================================================
api_integrations:
  heygen:
    base_url: https://api.heygen.com/v2
    auth_header: "X-Api-Key: ${HEYGEN_API_KEY}"
    sdk: "heygen"
    example: |
      // HeyGen API example
      const response = await fetch("https://api.heygen.com/v2/video/generate", {
        method: "POST",
        headers: {
          "X-Api-Key": process.env.HEYGEN_API_KEY,
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          video_inputs: [{
            character: {
              type: "avatar",
              avatar_id: "avatar_id_here",
              avatar_style: "normal"
            },
            voice: {
              type: "text",
              input_text: "Hello, welcome to our demo...",
              voice_id: "voice_id_here"
            },
            background: {
              type: "color",
              value: "#ffffff"
            }
          }],
          dimension: { width: 1920, height: 1080 }
        })
      });

      // Poll for completion
      const videoId = response.video_id;
      // ... polling logic

  d_id:
    base_url: https://api.d-id.com
    auth_header: "Authorization: Basic ${DID_API_KEY}"
    sdk: "d-id"
    example: |
      // D-ID API example
      const response = await fetch("https://api.d-id.com/talks", {
        method: "POST",
        headers: {
          "Authorization": `Basic ${Buffer.from(process.env.DID_API_KEY).toString('base64')}`,
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          source_url: "https://path-to-image.jpg",
          script: {
            type: "text",
            input: "Hello, this is a talking portrait demo.",
            provider: {
              type: "microsoft",
              voice_id: "en-US-JennyNeural"
            }
          },
          config: {
            stitch: true,
            result_format: "mp4"
          }
        })
      });

  synthesia:
    base_url: https://api.synthesia.io/v2
    auth_header: "Authorization: ${SYNTHESIA_API_KEY}"
    example: |
      // Synthesia API example
      const response = await fetch("https://api.synthesia.io/v2/videos", {
        method: "POST",
        headers: {
          "Authorization": process.env.SYNTHESIA_API_KEY,
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          input: [{
            avatar: "avatar_id",
            avatarSettings: {
              voice: "voice_id",
              horizontalAlign: "center"
            },
            scriptText: "Welcome to this training video...",
            background: "corporate_office"
          }],
          aspectRatio: "16:9"
        })
      });

  tavus:
    base_url: https://api.tavus.io/v1
    auth_header: "x-api-key: ${TAVUS_API_KEY}"
    example: |
      // Tavus personalized video
      const response = await fetch("https://api.tavus.io/v1/videos", {
        method: "POST",
        headers: {
          "x-api-key": process.env.TAVUS_API_KEY,
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          replica_id: "your_trained_replica",
          script: "Hi {{first_name}}, I wanted to personally...",
          variables: {
            first_name: "John"
          }
        })
      });

# ============================================================================
# DELEGATION TRIGGERS
# ============================================================================
delegation_triggers:
  - trigger: "voice|audio|narration|voiceover"
    delegate_to: ai-audio-production
    pattern: sequential
    context: "Need voice audio for avatar"
    handoff_data:
      - "Consider ElevenLabs for higher quality voices"
      - "Match voice characteristics to avatar appearance"
      - "Provide script with pronunciation guides"
    receive: "Audio file for lip sync"

  - trigger: "script|copy|messaging|what to say"
    delegate_to: copywriting
    pattern: sequential
    context: "Need script for avatar to speak"
    handoff_data:
      - "Scripts should be simple, clear sentences"
      - "Avoid sarcasm, complex humor"
      - "Include pronunciation guides for unusual words"
    receive: "Optimized script for avatar delivery"

  - trigger: "B-roll|visuals|footage|background video"
    delegate_to: ai-video-generation
    pattern: parallel
    context: "Need supporting video content"
    handoff_data:
      - "B-roll to intercut with avatar"
      - "Background replacement options"
      - "Visual demonstrations to show"
    receive: "Video assets to composite with avatar"

  - trigger: "brand universe|character design|style"
    delegate_to: ai-world-building
    pattern: sequential
    context: "Avatar needs brand world integration"
    handoff_data:
      - "Avatar as character in brand world"
      - "Consistent styling requirements"
      - "Character personality guidelines"
    receive: "Brand guidelines for avatar design"

  - trigger: "synthetic influencer|AI persona|virtual character"
    delegate_to: synthetic-influencers
    pattern: sequential
    context: "Avatar needs full persona development"
    handoff_data:
      - "Visual design is foundation"
      - "Need personality, voice, content strategy"
      - "Social media presence planning"
    receive: "Persona bible for avatar character"

  - trigger: "localization|translation|multi-language"
    delegate_to: ai-localization
    pattern: parallel
    context: "Avatar content for multiple languages"
    handoff_data:
      - "Source script and language"
      - "Target languages needed"
      - "Voice/avatar matching per language"
    receive: "Localized scripts and voice recommendations"

  - trigger: "ad|commercial|advertisement"
    delegate_to: ai-ad-creative
    pattern: parallel
    context: "Avatar for advertising"
    handoff_data:
      - "Platform requirements"
      - "Call-to-action needs"
      - "Performance goals"
    receive: "Ad strategy for avatar content"

  - trigger: "orchestrate|full production|campaign"
    delegate_to: ai-creative-director
    pattern: sequential
    context: "Complex production coordination"
    handoff_data:
      - "Avatar role in larger production"
      - "Other assets being created"
      - "Timeline and dependencies"
    receive: "Production plan"

# ============================================================================
# CROSS-DOMAIN INSIGHTS
# ============================================================================
cross_domain_insights:
  - domain: Acting/Performance
    insight: |
      Performance concepts improve avatar direction:
      - Script intention matters for delivery
      - Pacing affects audience engagement
      - Less is often more with AI delivery
      - Think newsreader energy levels
    applies_when: "Writing scripts or directing avatar generation"

  - domain: Television production
    insight: |
      TV production techniques apply:
      - Framing (headshots, medium shots)
      - Lighting setups for professionalism
      - B-roll cutaways to maintain interest
      - Lower third graphics for context
    applies_when: "Planning avatar video production"

  - domain: Corporate communications
    insight: |
      Corporate video standards apply:
      - Professional tone and appearance
      - Consistent branding
      - Clear messaging hierarchy
      - Accessibility considerations (captions)
    applies_when: "Creating training or corporate content"

  - domain: Psychology/Trust
    insight: |
      Trust psychology affects avatar design:
      - Eye contact creates connection
      - Appropriate gestures build credibility
      - Voice characteristics affect trust perception
      - Disclosure actually builds trust (don't hide AI nature)
    applies_when: "Selecting or designing avatars"

  - domain: Animation
    insight: |
      Animation principles apply:
      - Anticipation before actions
      - Natural movement arcs
      - Avoiding "dead" moments
      - Personality through movement
    applies_when: "Evaluating avatar quality"

# ============================================================================
# COMMON COMBINATIONS
# ============================================================================
common_combinations:
  - name: Training Video
    skills:
      - digital-humans
      - copywriting
      - ai-video-generation
      - ai-visual-effects
    workflow: |
      1. Script development (copywriting)
      2. Avatar generation (digital-humans)
      3. B-roll and demonstrations (ai-video-generation)
      4. Composite and polish (ai-visual-effects)
      5. Add graphics, captions, branding

  - name: Personalized Outreach
    skills:
      - digital-humans
      - ai-localization
      - marketing
    workflow: |
      1. Create template video with variables (digital-humans)
      2. Generate personalized versions via API
      3. Localize for different markets (ai-localization)
      4. Distribute through campaigns (marketing)

  - name: Social Media Presence
    skills:
      - digital-humans
      - synthetic-influencers
      - content-strategy
      - real-time-content
    workflow: |
      1. Define persona (synthetic-influencers)
      2. Content calendar (content-strategy)
      3. Generate avatar content (digital-humans)
      4. Trend-reactive content (real-time-content)

  - name: Multi-language Explainer
    skills:
      - digital-humans
      - ai-localization
      - ai-audio-production
    workflow: |
      1. Create primary language version (digital-humans)
      2. Translate and localize scripts (ai-localization)
      3. Generate voice in each language (ai-audio-production)
      4. Generate avatar in each language (digital-humans)

# ============================================================================
# VERSION COMPATIBILITY
# ============================================================================
version_compatibility:
  platforms:
    heygen: "v2 API"
    synthesia: "v2 API"
    d_id: "v1 API"
    tavus: "v1 API"

  features:
    lip_sync: "All platforms"
    custom_avatars: "HeyGen, Synthesia"
    photo_animation: "D-ID primary"
    personalization: "Tavus primary"
    multi_language: "All, Synthesia best"

  notes: |
    Digital human platforms evolve rapidly.
    Check for:
    - New avatar options
    - Improved lip sync
    - New language support
    - Pricing changes
    - API updates

    Current as of December 2024.
