id: ai-audio-production-sharp-edges
skill: ai-audio-production
version: 1.0.0

edges:
  - id: copyright-music-generation
    summary: AI-generated music may still have copyright issues
    severity: critical
    situation: |
      Generating music with AI and assuming it's copyright-free for commercial use.
      Some outputs may closely resemble copyrighted works.
    why: |
      AI music models trained on copyrighted music can produce similar outputs.
      Content ID systems may flag your AI music. Legal risk exists. Platforms
      may mute or remove your content.
    solution: |
      COPYRIGHT SAFETY:

      1. USE COMMERCIAL-SAFE TOOLS:
         - Suno (commercial license on paid plans)
         - Soundraw (royalty-free by design)
         - AIVA (clear commercial licensing)
         - Avoid training your own on copyrighted music

      2. CONTENT ID CHECK:
         - Run output through YouTube Content ID before publishing
         - Check against major distributors
         - Keep generation metadata as proof of origin

      3. LICENSING:
         - Read terms carefully for each platform
         - Free tiers often exclude commercial use
         - Enterprise tiers include indemnification

      4. DOCUMENTATION:
         - Save prompts and generation IDs
         - Screenshot licensing terms
         - Prepare for disputes
    symptoms:
      - Content ID claims
      - Video muted on platforms
      - DMCA takedown notices
      - Licensing disputes
    detection_pattern: null

  - id: model-mismatch-for-genre
    summary: Using wrong AI music model for specific genre
    severity: high
    situation: |
      Using Suno for classical orchestration (Udio better) or Udio for
      pop hooks (Suno better). Each model has strengths.
    why: |
      Models trained differently. Wrong choice = multiple generations to get
      acceptable output. Wastes credits and time.
    solution: |
      MODEL SELECTION BY GENRE:

      SUNO (v3.5+):
      → Pop, rock, electronic, hip-hop
      → Strong hooks and melodies
      → Lyrics integration
      → Radio-ready productions

      UDIO:
      → Classical, jazz, complex arrangements
      → More nuanced dynamics
      → Longer-form compositions
      → Better instrument separation

      SOUNDRAW:
      → Background music, corporate
      → Customizable moods
      → Safe, neutral productions
      → Easy length adjustment

      AIVA:
      → Orchestral, cinematic
      → Classical composition
      → Emotional soundscapes

      ELEVENLABS MUSIC:
      → Sound effects, short stings
      → Voice-integrated audio
      → Quick iterations
    symptoms:
      - '"Doesn''t sound right" feedback'
      - Genre feels off
      - Multiple regenerations
      - Switching tools mid-project
    detection_pattern: null

  - id: prompt-vagueness-music
    summary: Vague music prompts produce generic output
    severity: high
    situation: |
      Prompts like "happy music" or "background song" without specifics
      about tempo, instruments, mood, genre, structure.
    why: |
      AI interprets vague prompts randomly. You get generic, forgettable
      music that doesn't fit your content.
    solution: |
      MUSIC PROMPT STRUCTURE:

      1. GENRE + SUBGENRE:
         "Lo-fi hip hop" not just "chill music"
         "80s synthwave" not just "electronic"

      2. TEMPO + ENERGY:
         "120 BPM, high energy, building intensity"
         "Slow ballad, 70 BPM, intimate feel"

      3. INSTRUMENTS:
         "Acoustic guitar, soft piano, subtle strings"
         "Heavy 808s, trap hi-hats, synth pads"

      4. MOOD + EMOTION:
         "Nostalgic, bittersweet, hopeful ending"
         "Tense, suspenseful, unresolved"

      5. STRUCTURE:
         "32-bar intro, verse, chorus, verse, chorus, outro"
         "Ambient, no beats, evolving textures"

      # Example prompt:
      "Upbeat indie pop, 110 BPM, acoustic guitar and synths,
      optimistic and adventurous feel, verse-chorus structure,
      similar energy to Vampire Weekend, 2 minutes"
    symptoms:
      - Generic output
      - Doesn''t fit content
      - '"Background music syndrome"'
      - No emotional impact
    detection_pattern: 'music.*"[^"]{1,30}"(?!.*(BPM|tempo|genre|instrument))'

  - id: audio-video-sync-failure
    summary: Music doesn't sync with video edits or beats
    severity: high
    situation: |
      Generating music separately from video, then trying to make them
      work together. Beats don't align with cuts, energy mismatches.
    why: |
      Music and video need to be married. Random music + video = amateur feel.
      Professionals cut to music or compose to picture.
    solution: |
      SYNC STRATEGIES:

      1. MUSIC FIRST:
         - Generate music first
         - Edit video TO the music
         - Cut on beats, transitions on changes
         - Let music drive pacing

      2. VIDEO FIRST (harder):
         - Lock video edit
         - Analyze video timing (cuts, beats)
         - Specify duration exactly: "2:34"
         - Generate music matching structure

      3. STEM APPROACH:
         - Generate music with stems (Suno/Udio)
         - Adjust levels for different sections
         - Rebuild around video needs

      4. SOUNDRAW ADAPTIVE:
         - Use Soundraw's timeline editor
         - Adjust intensity to match video
         - Length-adjust without regenerating
    symptoms:
      - Awkward timing
      - Beats miss cuts
      - Energy mismatches
      - Amateur feel
    detection_pattern: null

  - id: voice-cloning-ethics
    summary: Cloning voices without consent is illegal in many jurisdictions
    severity: critical
    situation: |
      Cloning a celebrity's voice, a client's voice without consent,
      or any voice for deceptive purposes.
    why: |
      Voice cloning without consent is illegal in many places (California,
      EU, etc.). Using someone's voice without permission is fraud, defamation,
      or identity theft. Platforms ban this. Legal consequences are real.
    solution: |
      VOICE CLONING RULES:

      LEGAL:
      - Your own voice (obviously)
      - Consented voices (written agreement)
      - Licensed voice models (ElevenLabs voices)
      - Clearly fictional characters

      ILLEGAL/FORBIDDEN:
      - Celebrity voices without license
      - Real people without consent
      - Deepfake/deceptive content
      - Impersonation for fraud

      CONSENT DOCUMENTATION:
      ```
      I, [Name], consent to having my voice cloned by
      [Company] for [Specified Uses] from [Date] to [Date].
      Signature, date, witness.
      ```

      DISCLOSURE:
      - Label AI voices as AI-generated
      - Don't deceive audiences
      - Platform-specific requirements
    symptoms:
      - Legal threats
      - Account suspension
      - Content removal
      - Reputation damage
    detection_pattern: 'clone.*voice.*(celebrity|famous|[A-Z][a-z]+ [A-Z][a-z]+)'

  - id: audio-quality-assumptions
    summary: Expecting AI audio to be production-ready without mastering
    severity: medium
    situation: |
      Using AI-generated audio directly in production without any
      post-processing, EQ, compression, or mastering.
    why: |
      AI audio is a starting point, not a final product. Without proper
      processing, it sounds amateur, thin, or inconsistent with other
      audio elements.
    solution: |
      POST-PRODUCTION PIPELINE:

      1. LEVEL NORMALIZATION:
         - Normalize to -14 LUFS for streaming
         - -1 dB true peak maximum
         - Consistent levels across tracks

      2. EQ:
         - High-pass filter (remove rumble)
         - De-ess if vocals present
         - Match to reference track

      3. COMPRESSION:
         - Light compression for consistency
         - Parallel compression for punch
         - Limit peaks

      4. STEREO:
         - Check mono compatibility
         - Proper stereo width
         - No phase issues

      TOOLS:
      - iZotope RX (cleanup)
      - FabFilter Pro-Q (EQ)
      - LANDR (automated mastering)
      - Adobe Podcast (AI enhancement)
    symptoms:
      - Thin sound
      - Level inconsistency
      - Doesn't match other audio
      - Amateur quality feel
    detection_pattern: null

  - id: voice-naturalness-uncanny
    summary: AI voices sound robotic or uncanny without proper settings
    severity: medium
    situation: |
      Using default ElevenLabs settings and getting robotic, unnatural,
      or uncanny valley voice output.
    why: |
      AI voices need tuning. Default stability/similarity settings may be
      wrong for your use case. Wrong settings = detectable AI voice.
    solution: |
      ELEVENLABS TUNING:

      STABILITY (lower = more expressive):
      - 0.3-0.5: Expressive, emotional
      - 0.5-0.7: Balanced (default)
      - 0.7-0.9: Stable, professional

      SIMILARITY (higher = more like original):
      - Use higher for cloned voices
      - Lower for more variation

      STYLE (0-1):
      - Add emotion and expression
      - 0.2-0.4 for narration
      - 0.5+ for dramatic reading

      SPEAKING RATE:
      - Adjust speed to content
      - Slower for technical content
      - Natural pace: 140-160 WPM

      POST-PROCESSING:
      - Add subtle room reverb
      - Light compression
      - De-ess if needed
    symptoms:
      - '"Sounds like AI"'
      - Robotic delivery
      - Uncanny valley effect
      - Monotone output
    detection_pattern: null

  - id: music-length-limitations
    summary: AI music has length constraints that cause awkward loops
    severity: medium
    situation: |
      Needing 5-minute background music but AI generates 2-minute tracks,
      leading to obvious loops or abrupt endings.
    why: |
      Most AI music models generate 2-4 minute tracks. Looping sounds
      repetitive. Extending can create artifacts. Planning needed.
    solution: |
      LENGTH SOLUTIONS:

      1. MULTIPLE GENERATIONS:
         - Generate variations
         - Crossfade between them
         - Create seamless transitions

      2. SOUNDRAW (best for length):
         - Generates any length
         - Adjustable in real-time
         - Built for background music

      3. EXTENSION PROMPTS:
         - Suno: Use extend feature
         - Udio: Generate continuation
         - Maintain style consistency

      4. LOOPING TECHNIQUE:
         - Find natural loop points
         - Crossfade 2-4 seconds
         - Use ambient sections for loops

      5. DAW ARRANGEMENT:
         - Take AI stems
         - Rearrange in DAW
         - Create custom length
    symptoms:
      - Obvious loops
      - Abrupt endings
      - Repetitive feel
      - Length doesn't fit content
    detection_pattern: null

  - id: mixing-ai-sources
    summary: Mixing multiple AI audio sources creates inconsistency
    severity: medium
    situation: |
      Combining AI music from Suno, AI voice from ElevenLabs, and AI SFX
      from different sources. They don't sound cohesive.
    why: |
      Different AI models have different sonic characteristics, noise floors,
      and processing. Without proper mixing, the combination sounds disjointed.
    solution: |
      MIXING AI SOURCES:

      1. REFERENCE MATCHING:
         - Pick a reference track
         - Match all elements to it
         - Use iZotope Ozone or similar

      2. FREQUENCY ALLOCATION:
         - Voice: 200Hz-4kHz focus
         - Music: Duck under voice
         - SFX: Clear frequency slots

      3. ROOM TONE:
         - Add subtle shared reverb
         - Same "space" for all elements
         - Ties disparate sources together

      4. NOISE FLOOR:
         - Match noise floors
         - RX de-noise if needed
         - Consistent quality baseline

      5. DYNAMICS:
         - Voice loudest (-14 LUFS)
         - Music bed under (-20 to -24 LUFS)
         - SFX contextual levels
    symptoms:
      - Sources sound separate
      - Quality inconsistency
      - '"Obvious AI" detection'
      - Amateur mix
    detection_pattern: null

  - id: api-cost-overruns
    summary: AI audio APIs can rack up costs quickly
    severity: high
    situation: |
      Using ElevenLabs, Suno, or other APIs without tracking character/
      generation counts. Surprise bills or hitting limits.
    why: |
      AI audio is priced per character (voice) or per generation (music).
      Long content or many iterations = high costs. Easy to overspend.
    solution: |
      COST MANAGEMENT:

      ELEVENLABS:
      - Free: 10k chars/month
      - Starter: 30k chars ($5)
      - Creator: 100k chars ($22)
      - Track characters: 1 min VO ≈ 1000 chars

      SUNO:
      - Free: 50 songs/month
      - Pro: 500 songs ($10)
      - Premier: 2000 songs ($30)

      UDIO:
      - Similar tier structure
      - Check current pricing

      TRACKING:
      ```typescript
      async function generateWithTracking(text: string) {
        const chars = text.length;
        await logCost({ service: 'elevenlabs', chars });
        return await generate(text);
      }
      ```

      BUDGET RULES:
      - Set daily/weekly limits
      - Alert at 80% usage
      - Use local TTS for drafts
    symptoms:
      - Surprise bills
      - '"Out of credits"'
      - Rate limiting
      - Project delays
    detection_pattern: null

  - id: sfx-matching-failure
    summary: AI sound effects don't match visuals realistically
    severity: medium
    situation: |
      Generating sound effects that are technically correct but don't
      match the specific visual action, environment, or scale.
    why: |
      Sound design is contextual. A "door slam" sounds different in a
      hallway vs. a studio. Generic SFX = amateur production feel.
    solution: |
      SFX MATCHING:

      1. ENVIRONMENT CONTEXT:
         - Add room characteristics
         - '"Door slam in large hallway with echo"'
         - Not just "door slam"

      2. SCALE MATCHING:
         - Small objects = higher pitch
         - Large objects = lower, more bass
         - Distance affects volume and clarity

      3. LAYERING:
         - Base SFX from AI
         - Layer with room tone
         - Add Foley for texture

      4. SYNC PRECISION:
         - Sub-frame accuracy for impacts
         - Pre-lap for anticipation
         - Post-tail for resonance

      5. LIBRARY HYBRID:
         - AI for unique sounds
         - Libraries for common sounds
         - Best of both worlds
    symptoms:
      - SFX feel "off"
      - Disconnected from visuals
      - Amateur production feel
      - Audience notices sync issues
    detection_pattern: null

  - id: overprocessing-artifacts
    summary: Heavy AI enhancement creates audible artifacts
    severity: medium
    situation: |
      Running audio through multiple AI enhancement tools (noise removal,
      voice enhancement, mastering) and creating artifacts.
    why: |
      Each AI processing pass adds potential artifacts. Stacking multiple
      enhancers can create "watery" sound, pumping, or unnatural quality.
    solution: |
      PROCESSING HYGIENE:

      1. MINIMAL PASSES:
         - One tool per problem
         - Don't stack enhancers
         - Prefer single-pass solutions

      2. QUALITY FIRST:
         - Capture/generate cleanly
         - Less processing needed
         - Start with quality source

      3. A/B TESTING:
         - Compare before/after
         - If can't hear improvement, skip
         - Less is often more

      4. ORDER MATTERS:
         - Noise removal first
         - EQ second
         - Compression third
         - Effects last

      5. MASTERING LAST:
         - Don't master then process
         - Master is final step
         - Don't stack mastering tools
    symptoms:
      - Watery/phasing sound
      - Pumping artifacts
      - Unnatural quality
      - '"Over-processed" feedback'
    detection_pattern: null
