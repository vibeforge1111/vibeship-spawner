id: ai-visual-effects-collaboration
skill: ai-visual-effects
version: 1.0.0

# ============================================================================
# PREREQUISITES
# ============================================================================
prerequisites:
  required:
    - name: Compositing Software
      description: Professional compositing capability
      options:
        - software: After Effects
          best_for: Motion graphics integration, general VFX
          cost: Creative Cloud subscription
        - software: DaVinci Resolve
          best_for: Color grading, Fusion for compositing
          cost: Free version available
        - software: Nuke
          best_for: Film/TV VFX, node-based
          cost: Enterprise licensing
        - software: Blender
          best_for: 3D integration, free
          cost: Free

    - name: AI VFX Tools
      description: AI-powered VFX capabilities
      options:
        - tool: ComfyUI
          best_for: Custom AI workflows, precise control
          cost: Free (local GPU required)
        - tool: Runway ML
          best_for: Quick AI tools, browser-based
          cost: Subscription
        - tool: Topaz Video AI
          best_for: Video upscaling, enhancement
          cost: One-time purchase

  recommended:
    - name: Upscaling Suite
      options:
        - Topaz Gigapixel AI (images)
        - Topaz Video AI (video)
        - Magnific AI (creative enhancement)
      reason: Different upscalers for different content types

    - name: Local GPU
      options:
        - NVIDIA RTX 3080+ (ComfyUI, local processing)
        - Apple Silicon M2+ (some tools)
      reason: Local processing faster and cheaper at scale

# ============================================================================
# MCP TOOL CONFIGURATIONS
# ============================================================================
mcp_tools:
  # Runway ML MCP
  - name: runway-vfx
    description: AI-powered VFX tools
    install: npx -y @anthropic/mcp-installer install runway
    config:
      server:
        command: npx
        args: ["-y", "@runway/mcp-server"]
        env:
          RUNWAY_API_KEY: "${RUNWAY_API_KEY}"
    capabilities:
      - inpainting
      - outpainting
      - background-removal
      - video-inpainting
      - frame-interpolation
    example_usage: |
      Use runway for VFX:
      - Inpaint: Remove objects from image/video
      - Green screen: Extract subjects
      - Expand: Outpaint to extend frames

  # Fal.ai for VFX models
  - name: fal-vfx
    description: Various AI VFX models
    install: npx -y @anthropic/mcp-installer install fal-ai
    config:
      server:
        command: npx
        args: ["-y", "@fal-ai/mcp-server"]
        env:
          FAL_KEY: "${FAL_KEY}"
    capabilities:
      - real-esrgan (upscaling)
      - controlnet (guided generation)
      - inpainting
      - style-transfer
    example_usage: |
      Use fal-ai for VFX processing:
      - Upscale with Real-ESRGAN
      - ControlNet for guided generation
      - Style transfer for look dev

  # Replicate for VFX models
  - name: replicate-vfx
    description: Access to VFX AI models
    install: npx -y @anthropic/mcp-installer install replicate
    config:
      server:
        command: npx
        args: ["-y", "@replicate/mcp-server"]
        env:
          REPLICATE_API_TOKEN: "${REPLICATE_API_TOKEN}"
    models_available:
      - codeformer (face restoration)
      - gfpgan (face enhancement)
      - real-esrgan (upscaling)
      - rembg (background removal)
      - film (frame interpolation)
    example_usage: |
      Use replicate for VFX:
      - Face restoration with CodeFormer
      - Background removal with rembg
      - Frame interpolation with FILM

# ============================================================================
# API INTEGRATIONS
# ============================================================================
api_integrations:
  runway:
    base_url: https://api.runwayml.com/v1
    auth_header: "Authorization: Bearer ${RUNWAY_API_KEY}"
    example: |
      // Runway inpainting
      const response = await fetch("https://api.runwayml.com/v1/inpaint", {
        method: "POST",
        headers: {
          "Authorization": `Bearer ${process.env.RUNWAY_API_KEY}`,
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          image: imageBase64,
          mask: maskBase64,
          prompt: "clean background"
        })
      });

  replicate_vfx:
    base_url: https://api.replicate.com/v1
    auth_header: "Authorization: Token ${REPLICATE_API_TOKEN}"
    sdk: "replicate"
    example: |
      import Replicate from "replicate";

      const replicate = new Replicate();

      // Face restoration
      const restored = await replicate.run(
        "sczhou/codeformer:latest",
        {
          input: {
            image: imageUrl,
            fidelity: 0.7,
            background_enhance: true
          }
        }
      );

      // Upscaling
      const upscaled = await replicate.run(
        "nightmareai/real-esrgan:latest",
        {
          input: {
            image: imageUrl,
            scale: 4,
            face_enhance: true
          }
        }
      );

      // Background removal
      const nobg = await replicate.run(
        "cjwbw/rembg:latest",
        {
          input: {
            image: imageUrl
          }
        }
      );

  fal_vfx:
    base_url: https://fal.run
    auth_header: "Authorization: Key ${FAL_KEY}"
    example: |
      import * as fal from "@fal-ai/serverless-client";

      fal.config({ credentials: process.env.FAL_KEY });

      // Real-ESRGAN upscaling
      const result = await fal.subscribe("fal-ai/real-esrgan", {
        input: {
          image_url: imageUrl,
          scale: 4
        }
      });

      // ControlNet for guided generation
      const controlled = await fal.subscribe("fal-ai/controlnet-sdxl", {
        input: {
          prompt: "enhance detail",
          control_image: imageUrl,
          controlnet_type: "depth"
        }
      });

  topaz:
    type: desktop_application
    notes: |
      Topaz products are desktop applications with CLI/scripting:

      # Topaz Video AI CLI
      tvai_cli --input video.mp4 --output upscaled.mp4 \
        --model artemis --scale 2

      # Topaz Gigapixel AI
      tgpai_cli --input image.jpg --output upscaled.jpg \
        --scale 4 --model standard

      Batch processing available through CLI.

# ============================================================================
# DELEGATION TRIGGERS
# ============================================================================
delegation_triggers:
  - trigger: "generate|create|new"
    delegate_to: ai-image-generation
    pattern: sequential
    context: "Need source content before VFX"
    handoff_data:
      - "What to generate"
      - "Quality requirements"
      - "Style direction"
    receive: "AI-generated source for VFX"

  - trigger: "video|footage|motion"
    delegate_to: ai-video-generation
    pattern: sequential
    context: "Need video content before VFX"
    handoff_data:
      - "Video requirements"
      - "Duration and style"
      - "Resolution needed"
    receive: "AI video for enhancement"

  - trigger: "color grade|look dev|DI"
    delegate_to: video-production
    pattern: parallel
    context: "Need professional color work"
    handoff_data:
      - "Source footage"
      - "Look references"
      - "Delivery specs"
    receive: "Color graded footage"

  - trigger: "motion graphics|animation|kinetic"
    delegate_to: motion-graphics
    pattern: parallel
    context: "Need animated elements"
    handoff_data:
      - "Animation requirements"
      - "Style direction"
      - "Integration specs"
    receive: "Motion graphics elements"

  - trigger: "orchestrate|production|campaign"
    delegate_to: ai-creative-director
    pattern: sequential
    context: "VFX is part of larger production"
    handoff_data:
      - "What VFX is needed"
      - "Other assets involved"
      - "Timeline"
    receive: "Production coordination"

  - trigger: "ad|advertising|commercial"
    delegate_to: ai-ad-creative
    pattern: parallel
    context: "VFX for advertising"
    handoff_data:
      - "Platform requirements"
      - "Brand guidelines"
      - "Performance goals"
    receive: "Ad creative direction"

# ============================================================================
# CROSS-DOMAIN INSIGHTS
# ============================================================================
cross_domain_insights:
  - domain: Traditional VFX
    insight: |
      Classic VFX principles apply to AI workflows:
      - Lighting match is critical
      - Edge quality sells the composite
      - Color continuity unifies elements
      - Grain/texture matching is essential
      AI tools are new, but fundamentals unchanged.
    applies_when: "Compositing AI with real footage"

  - domain: Cinematography
    insight: |
      Camera/lens knowledge improves VFX:
      - Depth of field affects edge treatment
      - Lens distortion should match
      - Motion blur characteristics differ by camera
      - Sensor characteristics (rolling shutter, etc.)
    applies_when: "Matching AI to camera footage"

  - domain: Color Science
    insight: |
      Color science applies to VFX matching:
      - Color spaces (Rec709, P3, Rec2020)
      - Log vs linear workflows
      - Color temperature and tint
      - LUT application order matters
    applies_when: "Color matching and grading"

  - domain: Image Processing
    insight: |
      Digital image fundamentals apply:
      - Bit depth and dynamic range
      - Compression artifact types
      - Resolution and nyquist
      - Anti-aliasing and edge quality
    applies_when: "Processing and upscaling"

  - domain: Animation
    insight: |
      Animation principles help with AI motion:
      - Temporal coherence
      - Motion blur expectations
      - Frame rate and interpolation
      - Keyframe concepts for consistency
    applies_when: "Working with AI video/animation"

# ============================================================================
# COMMON COMBINATIONS
# ============================================================================
common_combinations:
  - name: AI Video Enhancement Pipeline
    skills:
      - ai-visual-effects
      - ai-video-generation
      - ai-audio-production
    workflow: |
      1. Generate base video (ai-video-generation)
      2. Upscale to final resolution
      3. Color correct and grade
      4. Fix any artifacts (inpaint)
      5. Add grain/texture matching
      6. Add audio (ai-audio-production)
      7. Final export

  - name: AI-Live Action Composite
    skills:
      - ai-visual-effects
      - ai-image-generation
      - video-production
    workflow: |
      1. Shoot live action plate
      2. Generate AI elements (ai-image-generation)
      3. Color match to plate
      4. Composite with proper edges
      5. Add grain matching
      6. Light wrap and integration
      7. Final grade

  - name: Video Restoration
    skills:
      - ai-visual-effects
    workflow: |
      1. Analyze source quality
      2. Denoise/cleanup
      3. Face restoration if needed
      4. Upscale to target resolution
      5. Color correction
      6. Grain matching to era
      7. Export

  - name: Full Campaign VFX
    skills:
      - ai-visual-effects
      - ai-image-generation
      - ai-video-generation
      - ai-creative-director
    workflow: |
      1. Creative direction (ai-creative-director)
      2. Generate source assets
      3. VFX enhancement pass
      4. Composite all elements
      5. Color continuity pass
      6. Final QA and delivery

# ============================================================================
# TECHNICAL SPECIFICATIONS
# ============================================================================
technical_specs:
  recommended_codecs:
    intermediate:
      - ProRes 422 HQ (macOS)
      - DNxHR HQ (cross-platform)
      - EXR (for VFX, per-frame)
    delivery:
      - H.264/H.265 (web)
      - ProRes 4444 (broadcast)
      - DPX/EXR (film)

  color_spaces:
    working: "Linear or ACEScg for VFX"
    delivery: "Rec709 or P3 depending on output"

  resolution_guidelines:
    work_resolution: "Delivery resolution or higher"
    upscale_limit: "2-4x reasonable, 8x pushing it"
    intermediate_resolution: "Native, no scaling"

  gpu_requirements:
    minimum: "8GB VRAM for basic ComfyUI"
    recommended: "12-24GB VRAM for complex workflows"
    cloud_alternative: "Replicate, Fal.ai for heavy processing"

# ============================================================================
# VERSION COMPATIBILITY
# ============================================================================
version_compatibility:
  software:
    after_effects: "2024+"
    davinci_resolve: "18+"
    comfyui: "Latest stable"
    topaz_video_ai: "4.x"

  models:
    real_esrgan: "RealESRGAN_x4plus"
    codeformer: "Latest"
    stable_diffusion: "SDXL, SD 1.5 for ControlNet"

  notes: |
    AI VFX tools evolve rapidly.

    Check for:
    - New model releases
    - Better upscalers
    - Faster processing
    - New capabilities

    Current as of December 2024.
