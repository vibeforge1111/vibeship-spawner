id: ai-visual-effects-sharp-edges
skill: ai-visual-effects
version: 1.0.0

edges:
  - id: upscaling-miracle-expectation
    summary: Expecting upscaling to create detail that doesn't exist
    severity: high
    situation: |
      Taking a heavily compressed or very low resolution source and
      expecting AI upscaling to create photorealistic detail.
    why: |
      AI upscaling hallucinate plausible detail, but it's invented.
      A 144p video won't become 4K broadcast quality. The source
      information simply isn't there. "Enhance" only works in CSI.
    solution: |
      UPSCALING REALITY:

      WHAT AI UPSCALERS DO:
      - Intelligently interpolate existing data
      - Add plausible texture and detail
      - Reduce artifacts from scaling

      WHAT THEY CAN'T DO:
      - Recover information that's not there
      - Fix severe compression artifacts
      - Create true detail from nothing

      QUALITY EXPECTATIONS:
      - 720p → 1080p: Excellent results
      - 480p → 1080p: Good, some hallucination
      - 240p → 1080p: Visible artifacts, invented detail
      - Lower: Not recommended

      BEST PRACTICE:
      - Generate at highest resolution possible
      - Use upscaling for final polish, not rescue
      - Test upscale on sample before full batch
      - Multiple passes with conservative settings

      # Generate high, upscale higher—not generate low, pray
    symptoms:
      - Mushy, painterly output
      - Invented details that look wrong
      - Doesn't match source quality expectations
      - \'"It looks AI" feedback\'
    detection_pattern: null

  - id: temporal-inconsistency
    summary: Frame-by-frame processing causing flickering
    severity: high
    situation: |
      Processing video frames individually without temporal consistency,
      causing flickering, jumping, or inconsistent style between frames.
    why: |
      Each frame processed independently can produce slightly different
      results. These differences cause visible flickering at playback.
      Video needs frame-to-frame consistency.
    solution: |
      TEMPORAL CONSISTENCY:

      FOR UPSCALING:
      - Use video-specific upscalers (Topaz Video AI)
      - They analyze frame sequences
      - Motion-compensated processing

      FOR STYLE TRANSFER:
      - Process key frames, interpolate between
      - Use video-specific models
      - ComfyUI with temporal attention

      FOR INPAINTING:
      - Propagate mask across frames
      - Use video inpainting models
      - Ebsynth for style propagation

      COMFYUI SPECIFICS:
      - AnimateDiff for consistent motion
      - Temporal LoRAs
      - Frame interpolation nodes

      CHECKING:
      - Watch at full speed
      - Look for flicker
      - Check on problem areas (edges, faces)
    symptoms:
      - Flickering between frames
      - Style jumps
      - \'"Boiling" edges\'
      - Inconsistent color/detail
    detection_pattern: null

  - id: edge-quality-neglect
    summary: Poor edge quality destroying composite believability
    severity: high
    situation: |
      Compositing AI elements with visible edge artifacts—hard edges,
      halos, fringing—that scream "composited."
    why: |
      Edge quality is what sells a composite. The human eye is extremely
      sensitive to edge anomalies. Bad edges make everything look fake,
      no matter how good the elements.
    solution: |
      EDGE QUALITY CHECKLIST:

      EXTRACTION:
      - Use AI rotoscoping (Runway, After Effects)
      - Multiple passes if needed
      - Manual cleanup on hero shots

      EDGE TREATMENT:
      - Feather appropriately for depth of field
      - Edge blur should match lens characteristics
      - No hard pixel edges unless intentional

      FRINGING:
      - Remove color fringing (chroma spill)
      - Despill on green screen pulls
      - Color correction on edges

      MATCHING:
      - Edge softness matches other scene elements
      - Grain/noise continues across edge
      - Lighting wraps around edge correctly

      VERIFICATION:
      - View at 100% zoom
      - View against different backgrounds
      - Motion test—edges worse in motion
    symptoms:
      - Visible halos
      - Hard cutout look
      - Color fringing
      - \'"Pasted on" appearance\'
    detection_pattern: null

  - id: color-mismatch-composite
    summary: Color mismatch between AI elements and footage
    severity: high
    situation: |
      Compositing AI-generated elements into footage without matching
      color grade, resulting in obvious misalignment.
    why: |
      Even perfect edges can't save a composite where colors don't match.
      AI elements often have different color characteristics than real
      footage. The mismatch breaks the illusion.
    solution: |
      COLOR MATCHING WORKFLOW:

      1. ANALYZE FOOTAGE:
         - Sample key colors (shadows, mids, highlights)
         - Identify color temperature
         - Note contrast ratio

      2. MATCH AI ELEMENT:
         - Color correct to match samples
         - Match black point and white point
         - Match saturation levels

      3. LIGHTING DIRECTION:
         - Identify key light in footage
         - Adjust AI element lighting to match
         - Add shadows/highlights as needed

      4. UNIFYING GRADE:
         - Apply same LUT to all elements
         - Final adjustment layer
         - Slight desaturation often helps integration

      5. GRAIN/TEXTURE:
         - Match film grain/sensor noise
         - Add subtle texture overlay
         - Helps blend elements

      TOOLS:
      - DaVinci Resolve (best color)
      - After Effects (Lumetri)
      - Premiere Pro (quick)
    symptoms:
      - Elements look separate
      - Color temperature mismatch
      - \'"Different cameras" look\'
      - Obvious composite
    detection_pattern: null

  - id: destructive-workflow
    summary: Making irreversible changes without saving originals
    severity: medium
    situation: |
      Applying edits destructively, overwriting original files, or
      flattening layers before the project is truly finished.
    why: |
      VFX work is iterative. What looks good today may need changes
      tomorrow. Client feedback requires revisions. Without originals
      and layered files, you're starting over.
    solution: |
      NON-DESTRUCTIVE WORKFLOW:

      1. NEVER OVERWRITE:
         - Save original files separately
         - Version naming: file_v1, file_v2
         - Date-stamped backups

      2. LAYER EVERYTHING:
         - Keep adjustments on separate layers
         - Use adjustment layers, not direct edits
         - Preserve ability to adjust

      3. PROJECT FILES:
         - Save .aep, .drp, .prproj files
         - Include all assets referenced
         - Archive completed projects

      4. NAMING CONVENTION:
         [Project]_[Shot]_[Version]_[Date]
         CampaignQ1_Shot01_v3_240115.psd

      5. FOLDER STRUCTURE:
         /Project
           /01_Source (never modify)
           /02_Working
           /03_Renders
           /04_Archive

      # Disk space is cheap. Recreating work is expensive.
    symptoms:
      - Can't make requested changes
      - Starting over for revisions
      - "We need the original"
      - Lost work
    detection_pattern: null

  - id: wrong-upscaler-choice
    summary: Using wrong upscaling model for content type
    severity: medium
    situation: |
      Using a photo upscaler on anime, or an anime upscaler on photos,
      getting poor results due to model mismatch.
    why: |
      Upscaling models are trained on specific content types. Models
      optimized for photos add wrong textures to illustrations.
      Anime models smooth away realistic detail.
    solution: |
      UPSCALER MATCHING:

      PHOTOREALISTIC CONTENT:
      - Topaz Gigapixel/Video AI
      - Real-ESRGAN (general models)
      - Magnific AI (creative enhancement)

      ANIME/ILLUSTRATION:
      - Real-ESRGAN (anime models)
      - waifu2x
      - Anime4K (for video)

      AI-GENERATED IMAGES:
      - Magnific AI (enhances detail)
      - Topaz (faithful upscale)
      - Match to original style

      VIDEO (ANY TYPE):
      - Topaz Video AI (multiple models)
      - Select model matching content
      - Test on sample frames

      CG/3D RENDERS:
      - Can often re-render at higher res
      - Upscaling adds unwanted texture
      - Use supersampling if possible
    symptoms:
      - Wrong texture added
      - Style doesn't match
      - Details look artificial
      - Smoothed away details
    detection_pattern: null

  - id: comfyui-workflow-chaos
    summary: Messy ComfyUI workflows that can't be reproduced
    severity: medium
    situation: |
      Building complex ComfyUI workflows without organization,
      making them impossible to understand or modify later.
    why: |
      ComfyUI's power comes from its flexibility. But unorganized
      workflows become unmaintainable. Can't reproduce results,
      can't fix issues, can't share with team.
    solution: |
      COMFYUI ORGANIZATION:

      1. NODE GROUPS:
         - Group related nodes
         - Clear group labels
         - Color coding by function

      2. NAMING:
         - Rename nodes descriptively
         - Input/output labels
         - Notes on non-obvious settings

      3. WORKFLOW STRUCTURE:
         Left → Right flow:
         Input → Processing → Output

         Vertical grouping:
         Similar operations stacked

      4. DOCUMENTATION:
         - Text nodes with explanations
         - README with workflow purpose
         - Screenshot of expected output

      5. VERSION CONTROL:
         - Save workflow versions
         - Name with purpose and date
         - Archive working versions

      6. MODULARITY:
         - Reusable sub-workflows
         - Standard patterns documented
         - Team shared libraries
    symptoms:
      - "How does this work again?"
      - Can't reproduce results
      - Breaking changes
      - Can't onboard teammates
    detection_pattern: null

  - id: grain-texture-mismatch
    summary: AI elements lack grain/noise of real footage
    severity: medium
    situation: |
      Compositing AI-generated elements that are perfectly clean into
      footage that has film grain or sensor noise, making the AI
      elements look obviously synthetic.
    why: |
      Real cameras produce grain/noise. AI generates clean images.
      This difference is a dead giveaway. Adding matching grain
      is essential for believable composites.
    solution: |
      GRAIN MATCHING:

      1. ANALYZE SOURCE:
         - What type? (film grain, digital noise)
         - How much? (ISO level, film stock)
         - Character? (color vs luminance noise)

      2. REMOVE THEN ADD:
         - AI elements often need slight denoise
         - Then add matched grain
         - More control than trying to match

      3. GRAIN SOURCES:
         - Film grain overlays (free/paid)
         - Noise generators (After Effects)
         - Match from source footage directly

      4. APPLICATION:
         - Add as overlay layer
         - Match intensity and size
         - Track if camera moving

      5. VERIFICATION:
         - Zoom to 100%
         - Compare to adjacent footage
         - Should be indistinguishable

      TOOLS:
      - After Effects: Add Grain effect
      - Resolve: Film Grain node
      - Premiere: Noise effect + adjustment
    symptoms:
      - Elements look "too clean"
      - Obvious synthetic quality
      - Doesn't match footage character
      - \'"CG" appearance\'
    detection_pattern: null

  - id: resolution-chain-errors
    summary: Resolution changing incorrectly through processing chain
    severity: medium
    situation: |
      Processing workflow where resolution changes unexpectedly—
      upscaling then downscaling, or ending at wrong resolution.
    why: |
      Each resolution change potentially loses quality. Upscale-then-
      downscale is pure quality loss. Need to plan resolution through
      entire pipeline.
    solution: |
      RESOLUTION PLANNING:

      1. DEFINE TARGET:
         - What is final delivery resolution?
         - 1080p, 4K, specific platform?

      2. WORK RESOLUTION:
         - Usually same as delivery or higher
         - Higher allows crop/reframe
         - Don't work below delivery

      3. AI GENERATION:
         - Generate at native or slightly above
         - Don't rely on extreme upscaling

      4. UPSCALE ONCE:
         - Upscale as final step
         - Not mid-workflow
         - From highest quality source

      5. AVOID:
         - Upscale → Downscale (quality loss)
         - Multiple upscales (artifacts compound)
         - Working below delivery resolution

      PIPELINE:
      Generate (native) → Process (native) →
      Upscale (if needed) → Deliver
    symptoms:
      - Softer than expected output
      - Quality loss through pipeline
      - Resolution confusion
      - Artifacts from scaling
    detection_pattern: null

  - id: over-processing
    summary: Applying too many effects degrading quality
    severity: low
    situation: |
      Stacking effect after effect trying to "fix" or "improve,"
      ending up with worse quality than starting point.
    why: |
      Each processing step can introduce artifacts. Stacked processing
      compounds these. Sometimes the original is better than
      heavily processed version.
    solution: |
      PROCESSING DISCIPLINE:

      1. MINIMAL INTERVENTION:
         - Do less, not more
         - Each step should have clear purpose
         - If in doubt, don't add

      2. A/B COMPARISON:
         - Compare to original regularly
         - Is this actually better?
         - Get second opinion

      3. QUALITY CHECKPOINTS:
         - Review after each major step
         - Stop if quality degrading
         - Sometimes step back

      4. EXPORT QUALITY:
         - High bitrate between processing steps
         - Avoid lossy compression mid-workflow
         - ProRes/DNxHR for intermediates

      5. FRESH EYES:
         - Take breaks
         - Return with perspective
         - Easy to lose objectivity

      # More effects ≠ better output
    symptoms:
      - Mushy, over-processed look
      - Accumulated artifacts
      - Loss of detail
      - \'"Filter stacking" appearance\'
    detection_pattern: null
