id: ai-workflow-automation-sharp-edges
skill: ai-workflow-automation
version: 1.0.0

edges:
  - id: automation-without-quality-gates
    summary: Automating content generation without quality checks
    severity: critical
    situation: |
      Team sets up AI workflow to automatically generate and publish content without
      quality gates. "Let's just let AI handle it all!" Speed is prioritized over
      quality, and bad content starts flowing to customers at scale.
    why: |
      Automation amplifies everythingâ€”including mistakes. An error in a manual
      process affects one piece of content. An error in an automated workflow
      affects hundreds or thousands. Without quality gates, you're automating
      brand damage. Speed without quality is just fast failure.
    solution: |
      QUALITY GATES BEFORE SCALE:

      MANDATORY GATES:
      1. Technical validation (before generation)
         - Required fields present
         - Formats valid
         - Budget limits checked

      2. Output validation (after generation)
         - Content generated successfully
         - Length requirements met
         - No error states

      3. Brand compliance (automated checks)
         - Brand terms present
         - Prohibited terms absent
         - Tone indicators correct

      4. Platform compliance (automated checks)
         - Character limits met
         - Required elements present
         - Format matches destination

      5. Human review (for high-risk)
         - Strategic content
         - High-spend campaigns
         - Sensitive topics

      PHASED ROLLOUT:
      Phase 1: Generate â†’ 100% human review
      Phase 2: Generate â†’ Quality gates â†’ Human review
      Phase 3: Generate â†’ Quality gates â†’ Sampling review
      Phase 4: Generate â†’ Quality gates â†’ Auto-publish (low risk only)

      MONITORING:
      - Track quality gate pass/fail rates
      - Measure manual edit frequency
      - Monitor customer feedback
      - A/B test automated vs manual

      # Never automate publication before you automate quality
    symptoms:
      - Low-quality content published
      - Customer complaints increase
      - Brand voice inconsistency
      - Off-brand content appearing
      - Team losing trust in automation
    detection_pattern: null

  - id: brand-voice-drift
    summary: Automated content gradually losing brand consistency
    severity: critical
    situation: |
      AI-generated content starts on-brand but slowly drifts over time. Each
      generation is slightly more generic. Months later, content doesn't sound
      like the brand anymore. "Why does everything sound like generic AI now?"
    why: |
      AI models don't intrinsically know your brandâ€”they learn from prompts and
      examples. As prompts get tweaked, as models get updated, as different
      people manage the system, brand voice can drift. At scale, small drift
      becomes big divergence. Generic AI voice replaces unique brand voice.
    solution: |
      BRAND VOICE PRESERVATION SYSTEM:

      1. BRAND VOICE LOCK:
         - Document exact brand voice attributes
         - Create "voice kit": examples of good/bad
         - Lock approved prompt templates (version control)
         - Require approval for prompt changes

      Example voice kit:
      ```
      BRAND VOICE: [Company]
      - Professional but not stuffy
      - Data-driven but not dry
      - Helpful but not hand-holdy

      GOOD EXAMPLES:
      "Our data shows 40% improvement" âœ“
      "Here's what works" âœ“
      "Let's look at the numbers" âœ“

      BAD EXAMPLES:
      "Unlock your potential!" âœ— (too generic)
      "Revolutionary game-changer" âœ— (hype)
      "It's super easy!" âœ— (too casual)
      ```

      2. REGULAR AUDITS:
         - Monthly: Review 20 random outputs
         - Check against brand voice kit
         - Score each on brand alignment (1-5)
         - Target: 90% score 4 or 5

      3. COMPARISON TESTING:
         - Keep "golden examples" from launch
         - Compare new outputs to golden examples
         - Measure similarity/divergence
         - Alert if drift detected

      4. PROMPT REGRESSION TESTING:
         - Before deploying prompt changes
         - Test on standard inputs
         - Compare outputs to previous version
         - Ensure brand voice maintained

      5. HUMAN CALIBRATION:
         - Quarterly: Team reviews same content
         - Align on what's on-brand vs off-brand
         - Update voice kit based on learnings
         - Refine prompts to match consensus

      DRIFT DETECTION METRICS:
      - Track brand-specific term usage
      - Monitor tone consistency
      - Measure edit rate (â†‘ = drift)
      - Compare to human-written baseline

      CORRECTION WHEN DRIFT DETECTED:
      1. Analyze: What changed? (prompts, model, reviewers)
      2. Revert: Roll back to previous version if possible
      3. Refine: Update prompts with more brand examples
      4. Retrain: Educate team on brand voice
      5. Monitor: Closely watch next 100 outputs

      # Brand voice is an assetâ€”protect it actively
    symptoms:
      - Content sounds generic
      - Team says "this doesn't sound like us"
      - Increased edit rate
      - Loss of brand personality
      - Indistinguishable from competitors
    detection_pattern: null

  - id: api-rate-limit-disaster
    summary: Workflow hitting API rate limits and failing at scale
    severity: high
    situation: |
      Automation workflow triggers hundreds of AI API calls simultaneously. Hits
      rate limits. Requests fail. Workflow breaks. Content doesn't publish.
      Emergency manual intervention required. "The automation just stopped working!"
    why: |
      AI APIs have rate limits (requests per minute, tokens per day). Automated
      workflows can easily exceed these limits when scaling. Without rate limiting
      and queuing, workflows fail unpredictably. Failures cascadeâ€”one failed
      request triggers retries, which trigger more failures.
    solution: |
      RATE LIMITING ARCHITECTURE:

      1. KNOW YOUR LIMITS:
         Document API rate limits:
         - Requests per minute (RPM)
         - Tokens per minute (TPM)
         - Requests per day (RPD)
         - Concurrent connections

      Example:
      ```
      Claude API (Tier 2):
      - 4000 RPM
      - 400,000 TPM
      - 10 concurrent connections

      GPT-4 API (Tier 3):
      - 5000 RPM
      - 450,000 TPM
      - Unlimited concurrent
      ```

      2. IMPLEMENT QUEUE SYSTEM:
         Don't call API directlyâ€”queue requests

      ```
      REQUEST â†’ QUEUE â†’ RATE LIMITER â†’ API
                           â†“
                      (respects limits)
      ```

      Queue properties:
      - FIFO (first in, first out)
      - Priority lanes (urgent vs normal)
      - Max queue size (prevent runaway)
      - Queue monitoring (alert if backing up)

      3. RATE LIMITING LOGIC:
         ```python
         class RateLimiter:
           max_rpm = 4000
           max_tpm = 400000
           window = 60  # seconds

           def can_make_request(tokens_needed):
             current_rpm = get_requests_last_minute()
             current_tpm = get_tokens_last_minute()

             if current_rpm >= max_rpm:
               return False, "RPM limit reached"
             if current_tpm + tokens_needed >= max_tpm:
               return False, "TPM limit reached"

             return True, "OK"

           def wait_time():
             # Calculate how long to wait
             return seconds_until_next_available_slot()
         ```

      4. BACKOFF AND RETRY:
         When rate limited:
         - Exponential backoff (wait 1s, 2s, 4s, 8s...)
         - Jitter (add randomness to prevent thundering herd)
         - Max retries (don't retry forever)

      5. LOAD DISTRIBUTION:
         Spread load over time:
         - Batch processing (not all at once)
         - Scheduled workflows (off-peak hours)
         - Multiple API keys (if allowed)
         - Multiple accounts (for isolation)

      Example: Daily blog automation
      - Generate 50 posts per day
      - Spread over 12 hours (9am-9pm)
      - ~4 posts per hour
      - ~1 post every 15 minutes
      - Well under rate limits

      6. CIRCUIT BREAKER:
         When failures spike, stop making requests:
         ```
         IF (error_rate > 10% in last 5 minutes)
         THEN:
           - Pause all requests
           - Alert operations team
           - Investigate issue
           - Manual resume after fix
         ```

      7. MONITORING:
         Track in real-time:
         - Current RPM / TPM usage
         - % of limit consumed
         - Queue depth
         - Error rate
         - Retry rate

      Dashboard alerts:
      - Warning: >70% of rate limit
      - Critical: >90% of rate limit
      - Emergency: Queue depth > 1000

      SCALING STRATEGY:
      - Start small (10 requests/hour)
      - Monitor for issues
      - Gradually increase
      - Never go >80% of documented limits
      - Leave headroom for spikes

      # Design for limits from day oneâ€”they will be hit
    symptoms:
      - Workflows failing intermittently
      - "429 Too Many Requests" errors
      - Requests timing out
      - Inconsistent completion times
      - Manual intervention required
    detection_pattern:
      code: "429|rate.*limit|too.*many.*requests"
      config: "no.*queue|no.*rate.*limit"

  - id: cost-runaway
    summary: Uncontrolled AI generation costs spiraling
    severity: high
    situation: |
      Automation is working greatâ€”generating tons of content. Then the bill comes:
      $10,000 for the month. Expected $500. No cost tracking. No limits. No alerts.
      "How did we spend this much?!"
    why: |
      AI APIs charge per token. Automated workflows can generate millions of tokens
      without anyone noticing. No human is clicking "confirm purchase" for each
      request. Costs accumulate silently. Bugs can cause infinite loops. Without
      monitoring and limits, costs can exceed budgets by 10-100x.
    solution: |
      COST CONTROL SYSTEM:

      1. COST TRACKING (mandatory):
         Log every API call:
         ```json
         {
           "timestamp": "2025-12-25T10:30:00Z",
           "workflow": "blog_automation",
           "model": "gpt-4",
           "input_tokens": 2000,
           "output_tokens": 1500,
           "cost_usd": 0.105,
           "status": "success"
         }
         ```

      2. REAL-TIME COST DASHBOARD:
         Show current spend:
         - Today: $47.23 / $100 budget (47%)
         - This month: $834.12 / $2,000 budget (42%)
         - By workflow: Blog ($31), Social ($12), Email ($4)
         - By model: GPT-4 ($38), GPT-3.5 ($9)
         - Trend: On track / Over budget / Under budget

      3. BUDGET LIMITS (hard stops):
         ```yaml
         budgets:
           daily_limit: 100.00
           monthly_limit: 2000.00

           alerts:
             - at: 50%
               action: slack_notification
             - at: 80%
               action: email_finance_team
             - at: 100%
               action: pause_all_workflows

           per_workflow_limits:
             blog_automation: 50.00/day
             social_media: 30.00/day
             email_campaigns: 20.00/day
         ```

      4. PER-REQUEST LIMITS:
         Prevent single expensive requests:
         ```
         IF (estimated_cost > $5)
         THEN require_manual_approval
         ELSE proceed
         ```

      5. COST OPTIMIZATION:
         Choose cheapest sufficient model:
         ```
         Task: Social media post (300 chars)
         - GPT-4: $0.03/request
         - GPT-3.5: $0.002/request
         - Claude-3-Haiku: $0.001/request
         â†’ Use Haiku (15x cheaper)

         Task: Strategic blog post (2000 words)
         - GPT-4: $0.12/request
         - Claude-3-Opus: $0.15/request
         â†’ Use GPT-4 (quality worth cost)
         ```

      6. FAILURE = WASTED COST:
         Failed requests still cost money:
         - Validate inputs before API call
         - Don't retry if validation fails
         - Log and analyze failure patterns
         - Fix root causes, don't just retry

      7. CACHE AGGRESSIVELY:
         Avoid redundant generation:
         ```
         REQUEST: "Generate social post about Product X"

         CHECK CACHE:
         - Similar request < 7 days ago? Use cached
         - Exact same request? Use cached
         - No cache hit? Generate + cache

         SAVINGS:
         - Cache hit rate: 20%
         - Avoided cost: $150/month
         ```

      8. TESTING COSTS:
         Development and testing also cost:
         - Use smaller models for testing
         - Use shorter outputs in dev
         - Mock API calls in unit tests
         - Production API keys â‰  dev API keys

      9. AUDIT TRAIL:
         Monthly cost review:
         - What workflows cost most?
         - Which are worth it (ROI positive)?
         - Which can be optimized?
         - Are we getting expected value?

      COST-BENEFIT ANALYSIS:
      ```
      Blog Automation:
      - Cost: $500/month (AI generation)
      - Output: 100 blog posts
      - Alternative: $50/post freelancer = $5,000/month
      - Savings: $4,500/month
      - ROI: 900%
      â†’ Worth it âœ“

      Social Media:
      - Cost: $300/month (AI generation)
      - Output: 500 posts
      - Time saved: 40 hours/month
      - Value of time: $50/hour = $2,000
      - Net value: $1,700/month
      â†’ Worth it âœ“
      ```

      EMERGENCY COST RESPONSE:
      If costs spike unexpectedly:
      1. Pause all workflows immediately
      2. Review logs for anomalies
      3. Identify cause (bug, infinite loop, attack?)
      4. Fix issue
      5. Implement additional safeguards
      6. Resume with close monitoring

      # Automation without cost control is financial risk
    symptoms:
      - Unexpected high bills
      - No visibility into spending
      - Can't explain cost increases
      - Budget exceeded without warning
      - Finance team asking questions
    detection_pattern:
      code: "api.*call"
      config: "no.*budget|no.*cost.*track"

  - id: approval-bottleneck
    summary: Single approval point blocking entire workflow
    severity: high
    situation: |
      All automated content funnels to one person for approval. They become
      overwhelmed. Content backs up. Automation is fast but approval is slow.
      The bottleneck negates automation benefits. "We're waiting on Sarah's approval."
    why: |
      Human approval at scale doesn't scale linearly. One person can review 10-20
      items per day comfortably. Automation can generate 100+ items per day. The
      math doesn't work. Single points of approval become single points of failure.
      When the approver is out, everything stops.
    solution: |
      SCALABLE APPROVAL ARCHITECTURE:

      1. TIERED APPROVAL BY RISK:
         ```
         LOW RISK â†’ Auto-approve
         - Social media variations (tested template)
         - Blog updates (minor edits)
         - Standard email sends

         MEDIUM RISK â†’ Single reviewer
         - New blog posts
         - New email campaigns
         - Moderate-spend ads

         HIGH RISK â†’ Multi-reviewer
         - High-spend campaigns
         - Brand messaging changes
         - Legal-sensitive content
         ```

      2. PARALLEL APPROVAL:
         Multiple reviewers can approve simultaneously:
         ```
         SEQUENTIAL (slow):
         Creator â†’ Marketing â†’ Legal â†’ Finance â†’ Publish
         Timeline: 4 days

         PARALLEL (fast):
         Creator â†’ [Marketing + Legal + Finance] â†’ Publish
         Timeline: 1 day
         (Consolidate feedback, creator revises, re-review)
         ```

      3. DELEGATION AND BACKUP:
         Every approver has backup:
         ```yaml
         approvers:
           marketing_lead:
             primary: sarah@company.com
             backup: mike@company.com
             escalation: vp-marketing@company.com

           auto_delegate:
             if_no_response: 24_hours
             delegate_to: backup
         ```

      4. BATCH APPROVAL:
         Review multiple items at once:
         ```
         INEFFICIENT:
         - 10 separate Slack messages
         - 10 separate reviews
         - 10 context switches

         EFFICIENT:
         - Daily digest: "10 items to review"
         - Side-by-side comparison
         - Bulk approve/reject
         - 15 minutes total
         ```

      5. APPROVAL SHORTCUTS:
         Make approval fast:
         - One-click approve/reject (no typing)
         - Keyboard shortcuts (a = approve, r = reject)
         - Mobile-friendly review
         - Inline editing (approve with minor tweaks)

      6. AUTO-APPROVAL WITH SAMPLING:
         Low-risk content auto-publishes, random audit:
         ```
         WORKFLOW:
         - Generate 100 social posts
         - All pass quality gates
         - Auto-publish 90%
         - Human reviews random 10%
         - If >2 issues found â†’ Review all
         - If <2 issues â†’ Continue auto-publishing
         ```

      7. ESCALATION PATHS:
         When approvals are delayed:
         ```
         TIMELINE:
         - Request sent: 9am Monday
         - No response by 9am Tuesday (24h)
           â†’ Send reminder
         - No response by 9am Wednesday (48h)
           â†’ Auto-delegate to backup
         - No response by 9am Thursday (72h)
           â†’ Escalate to manager
         ```

      8. ROLE-BASED APPROVAL:
         Different people approve different types:
         ```
         Content Type        Approver
         ---------------------------------
         Blog posts       â†’ Content lead
         Ads              â†’ Marketing lead
         Product updates  â†’ Product manager
         Legal content    â†’ Legal team
         Technical posts  â†’ Engineering lead
         ```

      9. APPROVAL SLA TRACKING:
         Monitor approval performance:
         ```
         Approval Metrics (Last 30 Days):
         - Avg approval time: 18 hours
         - SLA compliance: 94% (< 24h target)
         - Timeout rate: 3%
         - Bottleneck: Legal reviews (avg 38h)

         ACTION:
         - Add backup legal reviewer
         - Auto-approve low-risk legal content
         ```

      10. GRACEFUL DEGRADATION:
          When approvers unavailable:
          ```
          HOLIDAY/VACATION MODE:
          - Route to backup automatically
          - Lower approval tier (VP â†’ Manager)
          - Delay non-urgent (queue for return)
          - Emergency: Executive override
          ```

      CAPACITY PLANNING:
      ```
      Current state:
      - 1 approver
      - 10 items/day capacity
      - 15 items/day demand
      â†’ 50% over capacity â†’ bottleneck

      Solutions:
      Option A: Add 1 more approver (2 total)
        â†’ 20 items/day capacity
        â†’ 25% buffer âœ“

      Option B: Auto-approve 50% (low risk)
        â†’ 5 items/day need approval
        â†’ 100% buffer âœ“

      Option C: Combination
        â†’ Auto-approve 30% (4.5 items)
        â†’ Add 1 approver (2 total)
        â†’ 10.5 items / 20 capacity
        â†’ 90% buffer âœ“âœ“
      ```

      # Approval should scale with automation, not block it
    symptoms:
      - Content waiting for approval
      - Same person always the blocker
      - Approver overwhelmed
      - Automation sitting idle
      - Team frustrated with delays
    detection_pattern:
      config: "single.*approver|one.*reviewer"
      code: "approval.*required"

  - id: content-duplication-across-channels
    summary: Same content appearing on multiple channels simultaneously
    severity: medium
    situation: |
      Automation publishes identical content to LinkedIn, Twitter, and blog at the
      same time. Followers see the same post everywhere. Looks spammy. Engagement
      drops. "Why are you posting the same thing everywhere?"
    why: |
      Multi-channel distribution is powerful, but identical content across channels
      feels lazy and spammy. Each platform has different audiences, contexts, and
      norms. What works on LinkedIn (professional, long-form) doesn't work on
      Twitter (casual, short-form). Automation without adaptation creates poor
      user experience.
    solution: |
      CHANNEL-SPECIFIC ADAPTATION:

      1. SINGLE SOURCE â†’ MULTIPLE ADAPTATIONS:
         ```
         SOURCE CONTENT:
         "We just published a comprehensive guide to improving
         developer productivity with AI tools. 10 actionable
         strategies, real-world examples, and tool recommendations."

         ADAPTATIONS:

         LinkedIn (Professional, detailed):
         "Just published: How engineering teams are using AI to
         boost productivity by 40%

         Key insights:
         â€¢ AI code completion saves 2h/day per developer
         â€¢ Automated testing reduces bug rates by 35%
         â€¢ AI documentation keeps wikis current

         Full guide (15 min read): [link]

         What AI tools is your team using?
         #DevProductivity #AITools"

         Twitter (Casual, thread):
         "AI is changing how developers work

         We surveyed 500+ devs and found:

         ðŸ”¥ 40% productivity increase
         âš¡ 2 hours saved per day
         ðŸ› 35% fewer bugs

         Thread: 10 ways AI makes you a better developer ðŸ§µ

         1/ Code completion tools..."

         Blog (Full content):
         [Complete 2000-word guide with images, examples, tool comparison table]

         Email (Teaser + CTA):
         Subject: "AI saved our devs 10 hours/week"

         Preview: "Here's how we did it..."

         Body: Summary of top 3 insights + "Read the full guide" CTA
         ```

      2. STAGGER TIMING:
         Don't post everywhere at once:
         ```
         Monday 10am: Blog post published
         Monday 2pm: LinkedIn post (link to blog)
         Tuesday 9am: Twitter thread (summary)
         Wednesday 10am: Email newsletter (to subscribers)
         Thursday 11am: Reddit post (in relevant subreddit)

         REASONING:
         - Blog first (canonical source)
         - Social drives traffic to blog
         - Spaced out over week (not spam)
         - Different audiences see it different days
         ```

      3. FORMAT ADAPTATION:
         Match platform expectations:
         ```
         Content: Product launch announcement

         LinkedIn:
         - Format: Multi-paragraph post
         - Tone: Professional
         - Length: 200-300 words
         - Media: Professional product image
         - CTA: "Learn more" link

         Twitter:
         - Format: Thread (5-7 tweets)
         - Tone: Casual, excited
         - Length: 280 chars per tweet
         - Media: GIF or short video
         - CTA: "Try it now" link

         Instagram:
         - Format: Visual-first (carousel)
         - Tone: Aspirational
         - Length: Caption 50-100 words
         - Media: High-quality lifestyle images
         - CTA: Link in bio

         Email:
         - Format: HTML email
         - Tone: Direct, personal
         - Length: 150 words
         - Media: Header image + screenshots
         - CTA: Prominent button
         ```

      4. AUDIENCE ADAPTATION:
         Same topic, different angles:
         ```
         Topic: New API release

         Developer blog:
         "New REST API endpoints for user management
         Technical specs, code examples, migration guide"

         Marketing blog:
         "Easier customer data integration
         Business benefits, use cases, success stories"

         LinkedIn (targeting CTOs):
         "How the new API reduces integration time by 60%
         Cost savings, team efficiency gains"

         Twitter (targeting developers):
         "New API just dropped ðŸ”¥
         Here's a quick integration example: [code snippet]"
         ```

      5. DEDUPLICATION RULES:
         Prevent accidental duplication:
         ```yaml
         deduplication:
           time_window: 7_days
           similarity_threshold: 80%

           check:
             - IF (new_content similar to recent_content)
               AND (same_channel)
               THEN block_publication
                    alert_operator

           exceptions:
             - scheduled_series (e.g., "Tip of the day")
             - evergreen_repost (after 30 days)
         ```

      6. CROSS-POSTING STRATEGY:
         When to post the same content:
         ```
         ACCEPTABLE CROSS-POSTING:
         âœ“ Blog post â†’ LinkedIn article (native repost)
         âœ“ Long-form â†’ Medium (republish with canonical tag)
         âœ“ Video â†’ YouTube + LinkedIn + Twitter (different audiences)
         âœ“ Evergreen content â†’ Repost after 3+ months

         UNACCEPTABLE:
         âœ— Identical text on LinkedIn and Twitter same day
         âœ— Same image post on Instagram and Facebook simultaneously
         âœ— Duplicate content without adaptation
         âœ— Spam same message across all channels
         ```

      7. TESTING AND ITERATION:
         Measure what works per channel:
         ```
         METRICS BY CHANNEL:
         LinkedIn:
         - Best time: Tuesday 10am
         - Best length: 200-250 words
         - Best format: Text + single image
         - Avg engagement: 4.2%

         Twitter:
         - Best time: Wednesday 3pm
         - Best length: Thread (5-7 tweets)
         - Best format: Text + GIF
         - Avg engagement: 2.8%

         ADAPT WORKFLOW:
         - Schedule based on best times
         - Format based on best performing styles
         - Continuously test and refine
         ```

      # Each platform deserves respect, not copy-paste
    symptoms:
      - Same post on multiple platforms
      - Followers complaining about duplication
      - Lower engagement than expected
      - Feeling of spam
      - Unsubscribes increase
    detection_pattern:
      config: "same.*content.*all.*channels"
      code: "publish.*to.*\\[.*,.*,.*\\]"

  - id: loss-of-human-oversight
    summary: Automation running without human monitoring
    severity: medium
    situation: |
      Workflow is fully automated. No one checks outputs anymore. "It's working
      fine!" Then something changes (model update, API change, market context) and
      the automation starts producing inappropriate content. No one notices for
      days or weeks.
    why: |
      Automation is set-and-forget only if nothing ever changes. In reality,
      everything changes: AI models get updated, APIs change behavior, market
      context shifts, competitors change tactics. Automation without human oversight
      becomes brittle and blind to context changes. By the time issues are noticed,
      significant damage may be done.
    solution: |
      HUMAN OVERSIGHT SYSTEM:

      1. SAMPLING REVIEW:
         Regular human review of automated outputs:
         ```
         DAILY SAMPLE:
         - Random 5-10 pieces of automated content
         - Quick review (2 min per piece)
         - Check: Still on-brand? Quality acceptable?
         - Flag any issues for investigation

         WEEKLY DEEP DIVE:
         - Review 20-30 pieces
         - Detailed analysis
         - Compare to quality baseline
         - Identify any drift or patterns
         ```

      2. DASHBOARD MONITORING:
         Always-visible workflow health:
         ```
         AI WORKFLOW DASHBOARD:

         Today's Activity:
         - Generated: 45 pieces
         - Auto-published: 38
         - Flagged for review: 5
         - Rejected: 2
         - Success rate: 84%

         Quality Trends:
         - Edit rate: 12% (â†‘ from 8% last week) âš ï¸
         - Brand compliance: 96%
         - Customer sentiment: Positive
         - Engagement: Normal

         Alerts:
         - Edit rate increased (investigate)
         - 2 pieces rejected (both mentions competitor)
         ```

      3. ANOMALY DETECTION:
         Automated alerts for unusual patterns:
         ```
         ALERTS:
         - Generated content length suddenly shorter
         - Brand term usage dropped below threshold
         - Engagement rate decreased significantly
         - Error rate spiked
         - Cost per generation increased
         - Quality gate failure rate up

         RESPONSE:
         â†’ Notification to team
         â†’ Automatic pause (optional)
         â†’ Investigation required
         ```

      4. FEEDBACK LOOPS:
         Capture feedback from humans:
         ```
         AFTER PUBLICATION:
         - Customer complaints â†’ Flag in system
         - Internal edits â†’ Log what changed
         - Performance data â†’ Feed back to workflow

         MONTHLY REVIEW:
         - What patterns in feedback?
         - What edits are common?
         - What's working well?
         - What needs improvement?
         ```

      5. CONTEXT AWARENESS:
         Automation adapts to changing context:
         ```
         SCENARIOS REQUIRING HUMAN INTERVENTION:

         - Breaking news in industry
           â†’ Pause scheduled content
           â†’ Review for appropriateness

         - Competitor announcement
           â†’ Hold competitive content
           â†’ Human decides response

         - PR crisis
           â†’ Pause all automated content
           â†’ Manual approval required

         - Product issue/bug
           â†’ Don't auto-publish product praise
           â†’ Human reviews all product content

         - Holiday/cultural event
           â†’ Review for sensitivity
           â†’ Adjust tone if needed
         ```

      6. MODEL VERSION TRACKING:
         Monitor when underlying AI changes:
         ```
         TRACK:
         - AI model version (GPT-4, Claude-3, etc)
         - Last model update date
         - Prompt template version

         WHEN MODEL UPDATES:
         - Increase sampling review (50% for 1 week)
         - Compare outputs to previous version
         - Verify quality maintained
         - Adjust prompts if needed
         - Document any behavioral changes
         ```

      7. HUMAN-IN-THE-LOOP TRIGGERS:
         Automation knows when to ask for help:
         ```
         AUTO-ESCALATE TO HUMAN IF:
         - Confidence score < 80%
         - Topic = new/unusual
         - Sentiment = negative
         - Mentions competitor, pricing, legal terms
         - Quality gate fails
         - Engagement unusually low
         - Customer complaint received
         ```

      8. REGULAR CALIBRATION:
         Team aligns on quality standards:
         ```
         MONTHLY CALIBRATION SESSION:
         1. Review 10 random automated pieces
         2. Each team member scores independently
         3. Discuss discrepancies
         4. Align on what's acceptable
         5. Update guidelines if needed
         6. Refine quality gates to match consensus
         ```

      OVERSIGHT WORKLOAD:
      ```
      Automation: 100 pieces/week
      Oversight: 2 hours/week

      Activities:
      - Daily sampling: 5 pieces Ã— 2 min = 10 min/day = 50 min/week
      - Dashboard review: 5 min/day = 25 min/week
      - Weekly deep dive: 30 min/week
      - Monthly calibration: 1 hour/month = 15 min/week

      Total: ~2 hours/week oversight
      Replaces: 40 hours/week manual creation
      Net savings: 38 hours/week

      ROI: 95% time savings while maintaining quality
      ```

      # Automation enables scale; oversight ensures quality
    symptoms:
      - No one reviewing automated outputs
      - Unnoticed quality degradation
      - Slow response to issues
      - Blindness to context changes
      - Team disconnected from content
    detection_pattern:
      config: "auto_publish.*true"
      code: "no.*review|skip.*approval"

  - id: integration-failure-cascade
    summary: One integration failure breaks entire workflow
    severity: medium
    situation: |
      Workflow depends on 5 integrations: AI API, WordPress API, Slack API, analytics,
      and email service. One API goes down (Slack). Entire workflow stops. Content
      isn't generated. Team not notified. Everything blocks on one failed integration.
    why: |
      Workflows with multiple integrations have multiple points of failure. If each
      integration is a hard dependency, the workflow is fragile. One API downtime,
      one auth token expiration, one rate limitâ€”and everything stops. Without
      graceful degradation, automation becomes unreliable.
    solution: |
      RESILIENT INTEGRATION ARCHITECTURE:

      1. DEPENDENCY CLASSIFICATION:
         ```
         CRITICAL (workflow stops without):
         - AI generation API (core functionality)
         - Database (content storage)

         IMPORTANT (workflow continues with degradation):
         - Publishing API (can queue for later)
         - Analytics (can log locally then sync)

         OPTIONAL (workflow continues normally):
         - Slack notifications (nice to have)
         - Additional monitoring tools
         ```

      2. GRACEFUL DEGRADATION:
         ```python
         def publish_content(content):
           # Critical: Save to database
           try:
             db.save(content)
           except Exception as e:
             log_error(e)
             raise  # Can't continue without this

           # Important: Publish to WordPress
           try:
             wordpress.publish(content)
           except Exception as e:
             log_error(e)
             queue_for_retry(content, "wordpress")
             # Continue workflow

           # Optional: Send Slack notification
           try:
             slack.notify("Published: " + content.title)
           except Exception as e:
             log_error(e)
             # Ignore and continue

           # Optional: Log to analytics
           try:
             analytics.track("content_published")
           except Exception as e:
             log_error(e)
             # Ignore and continue

           return "success_with_optional_failures"
         ```

      3. RETRY LOGIC:
         ```
         Retry strategy:
         - Attempt 1: Immediate
         - Attempt 2: After 5 seconds
         - Attempt 3: After 30 seconds
         - Attempt 4: After 5 minutes
         - Attempt 5: After 1 hour
         - Give up: After 5 attempts

         Exponential backoff + jitter:
         wait_time = (2 ** attempt) + random(0, 5)
         ```

      4. CIRCUIT BREAKER:
         ```python
         class CircuitBreaker:
           threshold = 5  # failures
           timeout = 60   # seconds

           state = "closed"  # closed | open | half_open
           failure_count = 0

           def call_api(api_func):
             if state == "open":
               if time_since_open > timeout:
                 state = "half_open"
               else:
                 raise Exception("Circuit open")

             try:
               result = api_func()
               if state == "half_open":
                 state = "closed"
                 failure_count = 0
               return result

             except Exception as e:
               failure_count += 1
               if failure_count >= threshold:
                 state = "open"
                 open_time = now()
               raise e
         ```

      5. FALLBACK STRATEGIES:
         ```
         PRIMARY: Publish to WordPress
         FALLBACK 1: Queue for manual publish
         FALLBACK 2: Email to content team
         FALLBACK 3: Save to shared drive

         PRIMARY: Send Slack notification
         FALLBACK 1: Send email instead
         FALLBACK 2: Log to dashboard
         FALLBACK 3: Write to file (check later)
         ```

      6. HEALTH CHECKS:
         ```
         BEFORE WORKFLOW RUNS:
         - Check AI API: /health endpoint
         - Check WordPress: Can authenticate?
         - Check Database: Can connect?

         IF CRITICAL SERVICE DOWN:
         - Don't start workflow
         - Alert operations
         - Retry in 5 minutes

         IF OPTIONAL SERVICE DOWN:
         - Log warning
         - Continue with degraded mode
         ```

      7. MONITORING AND ALERTS:
         ```
         TRACK PER INTEGRATION:
         - Success rate
         - Average response time
         - Error types
         - Downtime periods

         ALERTS:
         - Critical service down â†’ Page on-call
         - Important service down â†’ Slack warning
         - Optional service down â†’ Log only
         - Success rate < 95% â†’ Investigate
         ```

      8. INTEGRATION TESTING:
         ```
         TESTS:
         - Happy path (all integrations working)
         - AI API down (can't generate)
         - WordPress API down (queue for retry)
         - Slack API down (workflow continues)
         - Database down (workflow stops)
         - Multiple failures simultaneously
         - Retry logic works correctly
         - Circuit breaker opens/closes
         ```

      9. ASYNC WHERE POSSIBLE:
         ```
         SYNCHRONOUS (blocking):
         Generate content â†’ Wait for AI â†’ Continue
         (Must waitâ€”need content to proceed)

         ASYNCHRONOUS (non-blocking):
         Publish content â†’ Queue to WordPress â†’ Continue
         (Don't waitâ€”can retry later if needed)

         Analytics tracking â†’ Fire and forget
         Slack notification â†’ Fire and forget
         Email sending â†’ Queue for delivery
         ```

      RESILIENCE METRICS:
      ```
      Workflow Resilience Score:

      - Single point of failure: 0% resilience
      - Retry logic: +20%
      - Circuit breakers: +20%
      - Graceful degradation: +30%
      - Fallback strategies: +30%

      Target: >80% resilience
      ```

      # Build for failureâ€”it will happen
    symptoms:
      - Workflows failing completely
      - One API outage breaks everything
      - No graceful degradation
      - Manual intervention always required
      - Team loses trust in automation
    detection_pattern:
      code: "api\\..*\\(.*\\)"
      config: "no.*retry|no.*fallback"

  - id: metric-blindness
    summary: Running workflows without tracking performance
    severity: low
    situation: |
      Automation is generating hundreds of pieces of content per month. No one is
      tracking what performs well. No data on engagement, conversions, or ROI.
      "The automation is running!" But is it working? No idea.
    why: |
      Automation without measurement is hope-based strategy. You're generating
      content but not learning what works. Can't improve what you don't measure.
      Opportunity cost is hugeâ€”you could be 10x-ing the winners and stopping the
      losers, but you're treating everything equally.
    solution: |
      PERFORMANCE TRACKING SYSTEM:

      1. BASELINE METRICS:
         Track for every piece of content:
         ```
         CONTENT METADATA:
         - ID
         - Type (blog, social, email, etc)
         - Topic/category
         - Channel(s) published
         - Publish date/time
         - Generated by (AI model + prompt version)

         PERFORMANCE METRICS:
         - Views/impressions
         - Engagement (likes, shares, comments)
         - Click-through rate
         - Time on page (if applicable)
         - Conversion rate (if CTA)
         - Revenue attributed (if ecommerce)
         ```

      2. COMPARATIVE ANALYSIS:
         ```
         COMPARE:
         - AI-generated vs human-written
         - Different AI models
         - Different prompt templates
         - Different topics
         - Different formats
         - Different publishing times

         LEARN:
         - What performs best?
         - What underperforms?
         - Are there patterns?
         ```

      3. A/B TESTING:
         ```
         TEST: Two versions of automation
         - Version A: Current prompt
         - Version B: New prompt

         SPLIT: 50/50 traffic
         MEASURE: Engagement rate
         DURATION: 2 weeks
         DECIDE: Winner replaces loser

         Example results:
         Version A: 3.2% engagement
         Version B: 4.8% engagement (+50%)
         â†’ Roll out Version B to 100%
         ```

      4. FEEDBACK LOOP:
         ```
         MONTHLY ANALYSIS:
         1. Export performance data
         2. Identify top 10% performers
         3. Identify bottom 10% performers
         4. Analyze differences

         Questions:
         - What do top performers have in common?
         - What do poor performers have in common?
         - Can we replicate success patterns?
         - Can we avoid failure patterns?

         Actions:
         - Update prompts based on learnings
         - Adjust content mix (more of what works)
         - Refine targeting
         - Test new approaches
         ```

      5. DASHBOARD:
         ```
         AI CONTENT PERFORMANCE DASHBOARD

         This Month:
         - Content generated: 342 pieces
         - Total views: 45,231
         - Total engagement: 2,145 (4.7%)
         - Conversions: 89
         - Revenue: $12,450

         Comparison to Last Month:
         - Views: +15%
         - Engagement: +8%
         - Conversions: +22%
         - Revenue: +18%

         Top Performing Content:
         1. "How to X" (blog) - 2,341 views, 8.2% engagement
         2. "Y mistakes" (social) - 892 engagement
         3. "Z guide" (email) - 34% open rate, 12% CTR

         Worst Performing:
         1. "Generic topic" (blog) - 45 views, 0.2% engagement
         2. "Boring update" (social) - 12 engagement
         3. "Salesy email" (email) - 8% open rate, 1% CTR

         Recommendations:
         - More "How to" content (outperforms by 2.5x)
         - Less generic topics (underperform by 80%)
         - Continue testing new email subject lines
         ```

      6. ATTRIBUTION:
         ```
         Track what content drives results:

         BLOG POST:
         - 1,234 views
         - 145 clicked CTA
         - 23 signed up for trial
         - 3 converted to paid
         - $1,470 revenue

         ROI:
         - Cost to generate: $2.50 (AI)
         - Revenue: $1,470
         - ROI: 58,700%

         INSIGHT: This topic is extremely valuable
         ACTION: Generate more like this
         ```

      7. AUTOMATED INSIGHTS:
         ```
         AI ANALYZES AI CONTENT:

         Use Claude to analyze performance data:
         "Here's performance data for 100 blog posts.
         Identify patterns in high performers vs low performers.
         What topics, formats, lengths perform best?"

         Claude might find:
         - Posts with numbers in title: +45% CTR
         - Posts 800-1200 words: +30% engagement
         - Posts published Tuesday 10am: +25% views
         - Posts with "how to": +60% conversions
         ```

      8. CONTINUOUS OPTIMIZATION:
         ```
         OPTIMIZATION CYCLE:

         Week 1: Generate content (current approach)
         Week 2: Measure performance
         Week 3: Analyze patterns
         Week 4: Update prompts/strategy
         Repeat

         Over time:
         - Engagement â†‘
         - Cost per result â†“
         - ROI â†‘
         - Quality â†‘
         ```

      METRIC-DRIVEN DECISIONS:
      ```
      DECISION: Should we continue AI automation?

      DATA:
      - Generated: 500 pieces in 3 months
      - Cost: $1,250 (AI + oversight)
      - Alternative: $50/piece freelance = $25,000
      - Savings: $23,750

      Performance:
      - Avg engagement: 4.2% (human baseline: 4.5%)
      - Conversion rate: 2.1% (human baseline: 2.3%)
      - Quality: 92% acceptable (8% require major edits)

      VERDICT:
      âœ“ Continueâ€”nearly matches human quality at 5% cost
      âœ“ Focus improvement on bridging 0.2% conversion gap
      âœ“ Reduce edit rate from 8% to <5%
      ```

      # Measure everythingâ€”improve what matters
    symptoms:
      - No performance tracking
      - Can't explain ROI
      - Not learning from data
      - No optimization happening
      - Treating all content equally
    detection_pattern:
      config: "no.*analytics|no.*tracking"
      code: "publish.*content"

  - id: over-reliance-on-automation
    summary: Losing human creative capability by automating everything
    severity: low
    situation: |
      Team becomes dependent on automation. No one writes manually anymore. Skills
      atrophy. Then automation breaks, or needs creative breakthrough, or requires
      nuanced judgmentâ€”and no one can do it anymore. "Wait, how do we write this
      without AI?"
    why: |
      Automation is powerful but it's also a crutch. Over-reliance on AI generation
      means losing human skills: creativity, strategic thinking, nuanced judgment.
      When automation handles 100% of routine content, humans lose practice and
      confidence. Then when you need human creativityâ€”for breakthrough ideas, crisis
      response, or strategic pivotsâ€”the capability is gone.
    solution: |
      BALANCED HUMAN-AI APPROACH:

      1. RESERVE HUMAN CREATIVITY:
         ```
         AI HANDLES:
         - Routine content (80%)
         - Variations and adaptations
         - High-volume, low-risk
         - Tested formats

         HUMANS HANDLE:
         - Strategic content (20%)
         - Creative breakthroughs
         - High-stakes messaging
         - New formats/experiments
         ```

      2. SKILL MAINTENANCE:
         ```
         TEAM PRACTICES:
         - "Manual Monday": One day/week, no AI
         - Creative sprints: Brainstorm without AI
         - Writing exercises: Keep skills sharp
         - Prompt writing: Humans craft prompts (skill building)
         ```

      3. HUMAN-LED INNOVATION:
         ```
         INNOVATION PIPELINE:
         1. Human creates breakthrough idea/format
         2. Test with small audience
         3. If successful â†’ Templatize
         4. Let AI handle variations at scale
         5. Human invents next breakthrough

         Example:
         - Human invents "Story-driven case study" format
         - Tests with 3 manual pieces
         - Format performs 3x better
         - Create AI template
         - AI generates 20 variations
         - Human invents next format
         ```

      4. AI AS ASSISTANT, NOT REPLACEMENT:
         ```
         HUMAN-AI COLLABORATION:
         - Human: Strategic brief
         - AI: First draft
         - Human: Creative polish
         - AI: Multi-channel adaptation
         - Human: Final approval

         NOT:
         - AI: Everything
         - Human: Nothing
         ```

      5. CONTINUOUS LEARNING:
         ```
         TEAM DEVELOPMENT:
         - Study top-performing AI content (learn patterns)
         - Analyze what makes prompts effective
         - Practice editing AI outputs (judgment)
         - Understand why AI made certain choices
         - Develop "AI whisperer" skills
         ```

      6. EMERGENCY PREPAREDNESS:
         ```
         SCENARIO: AI automation down

         CAN TEAM STILL:
         â–¡ Write blog posts manually?
         â–¡ Create social content?
         â–¡ Draft emails?
         â–¡ Respond to inquiries?

         PRACTICE:
         - Quarterly "manual mode" exercise
         - Simulate automation failure
         - Create content without AI
         - Verify skills intact
         ```

      7. STRATEGIC DECISIONS STAY HUMAN:
         ```
         HUMANS DECIDE:
         - What topics to cover (strategy)
         - What tone to use (brand)
         - What messages to prioritize (positioning)
         - When to be controversial (risk)
         - How to respond to crises (judgment)

         AI EXECUTES:
         - Generating drafts (speed)
         - Creating variations (scale)
         - Adapting formats (efficiency)
         - Analyzing performance (data)
         ```

      8. MEASURE HUMAN CAPABILITY:
         ```
         QUARTERLY ASSESSMENT:
         - Can team write high-quality content manually?
         - Is creative thinking still sharp?
         - Can team work without AI if needed?
         - Are prompt-writing skills improving?

         RED FLAG:
         - "I don't know how to write this without AI"
         - "Let's ask AI what to do"
         - Creative atrophy
         - Loss of strategic thinking
         ```

      BALANCE FRAMEWORK:
      ```
      Content Portfolio:

      100% AI:
      - Daily social posts
      - Email variations
      - Product descriptions
      - FAQs

      AI + Human Polish:
      - Blog posts
      - Email campaigns
      - Ad copy
      - Landing pages

      100% Human:
      - Brand strategy
      - Creative campaigns
      - Thought leadership
      - Crisis response
      - C-suite communications

      RESULT:
      - Scale where it matters (routine)
      - Creativity where it counts (strategic)
      - Team capability maintained
      - Best of both worlds
      ```

      # Automation amplifies humans, doesn't replace them
    symptoms:
      - Team can't write without AI
      - Loss of creative confidence
      - "Let AI do it" as default
      - Skills atrophying
      - Dependence on automation
    detection_pattern:
      config: "automate.*100%|all.*ai"
      code: "manual.*false"
