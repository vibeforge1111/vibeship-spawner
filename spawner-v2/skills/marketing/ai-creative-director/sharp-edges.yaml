id: ai-creative-director-sharp-edges
skill: ai-creative-director
version: 1.0.0

edges:
  - id: consistency-drift-blindness
    summary: Not noticing gradual style drift across batch generations
    severity: critical
    situation: |
      Generating many assets in sequence and not noticing that the style,
      color palette, or overall feel has gradually drifted from the original.
    why: |
      Each generation has small variations. Over many generations, these
      compound. Asset 1 and asset 50 may look completely different even
      though each consecutive pair looks similar. Brand is destroyed.
    solution: |
      CONSISTENCY ENFORCEMENT:

      1. REFERENCE ANCHORING:
         - Keep approved hero assets visible
         - Compare every 5th generation to hero
         - Use style reference features where available

      2. BATCH REVIEW AS GRID:
         - View all assets simultaneously
         - Spot outliers immediately
         - Never review only sequentially

      3. AUTOMATION:
         - Same seed base where possible
         - Locked prompt prefixes
         - Color correction in post

      4. QA CHECKPOINTS:
         - Review at 10%, 50%, 100%
         - Side-by-side with hero
         - Reject batch if drift detected

      # View as grid, not sequence
    symptoms:
      - Assets don't feel like same brand
      - Stakeholder feedback "these feel different"
      - Color palette wandered
      - Style gradually changed
    detection_pattern: null

  - id: tool-selection-bias
    summary: Always using favorite tool instead of best tool for task
    severity: high
    situation: |
      Defaulting to one familiar AI tool for everything instead of selecting
      the optimal tool for each specific creative challenge.
    why: |
      Each AI tool has different strengths. Midjourney excels at artistic
      style, Flux at photorealism, Veo3 at video. Using wrong tool produces
      suboptimal results and wastes iterations.
    solution: |
      TOOL SELECTION FRAMEWORK:

      ASK FIRST:
      1. What type of content? (image, video, audio)
      2. What style? (photorealistic, artistic, stylized)
      3. What's the key challenge? (text, motion, consistency)
      4. What's the volume? (one-off, batch, scale)

      THEN MATCH:

      PHOTOREALISM → Flux, Imagen
      ARTISTIC → Midjourney
      TEXT IN IMAGE → Ideogram, Flux
      CONTROL/POSES → SD + ControlNet

      VIDEO REALISM → Veo3
      VIDEO STYLE → Runway Gen-3
      QUICK VIDEO → Kling, Pika

      Match tool to task, not habit to task.
    symptoms:
      - Struggling to get results
      - Many regenerations needed
      - Quality doesn't match tool capability
      - '"Works better in X" feedback'
    detection_pattern: null

  - id: brief-to-prompt-loss
    summary: Losing creative intent when translating brief to AI prompts
    severity: high
    situation: |
      Taking a rich creative brief and reducing it to a generic prompt that
      loses the nuance, emotion, and brand requirements.
    why: |
      AI models only know what you tell them. A brief might say "premium,
      sophisticated, aspirational" but if prompt just says "product photo,"
      the result will be generic. Brief intent must transfer to prompt.
    solution: |
      BRIEF-TO-PROMPT TRANSLATION:

      1. EXTRACT KEY DESCRIPTORS:
         From brief: "premium feel, aspirational, young professional audience"
         To prompt elements: "luxury, high-end, modern, sophisticated, clean"

      2. VISUAL VOCABULARY:
         Abstract → Concrete
         "Premium" → "marble surfaces, soft lighting, minimal composition"
         "Energetic" → "dynamic angles, bright colors, motion blur"
         "Trustworthy" → "professional lighting, clean backgrounds, warm tones"

      3. REFERENCE ANCHORING:
         Attach brief mood boards as style references
         "Like [reference image] but with [adjustment]"

      4. VALIDATE:
         Does prompt contain ALL brief requirements?
         Read prompt back—does it evoke the brief?

      # Every brief word should map to prompt elements
    symptoms:
      - Results don't match brief intent
      - Stakeholder says "not what we described"
      - Generic output despite detailed brief
      - Lost emotional tone
    detection_pattern: null

  - id: production-pipeline-chaos
    summary: No clear workflow causing rework and confusion
    severity: high
    situation: |
      Jumping between AI tools without a clear sequence, causing rework
      when upstream changes invalidate downstream work.
    why: |
      AI production has dependencies. If you finalize video edit before
      audio is approved, audio changes require video re-edit. Without
      clear pipeline, you redo work constantly.
    solution: |
      STANDARD PIPELINE ORDER:

      1. BRIEF APPROVAL (before any generation)
      2. HERO ASSETS (one of each type, approved)
      3. BATCH GENERATION (from approved heroes)
      4. ENHANCEMENT (upscale, color, VFX)
      5. ASSEMBLY (edit, composite)
      6. LOCALIZATION (if needed)
      7. FORMAT (platform-specific exports)
      8. FINAL QA

      DEPENDENCIES:
      - Don't finalize video until audio approved
      - Don't localize until hero approved
      - Don't export until final QA

      CHECKPOINTS:
      - No phase starts until previous approved
      - Changes to approved work = change request
      - Document decisions at each checkpoint
    symptoms:
      - Constant rework
      - "But we already did that"
      - Unclear what's approved
      - Bottlenecks and blocking
    detection_pattern: null

  - id: scale-before-quality
    summary: Trying to scale production before hero asset is perfect
    severity: high
    situation: |
      Generating 100 variations before the single hero asset is approved,
      then having to regenerate all 100 when hero feedback comes in.
    why: |
      Batch generation amplifies everything—quality and mistakes. If hero
      has issues, all variations have issues. Fix the one before making many.
    solution: |
      HERO-FIRST WORKFLOW:

      1. CREATE ONE:
         - Generate single hero asset
         - Iterate until perfect
         - Get explicit approval

      2. DOCUMENT HERO:
         - Exact prompt used
         - All settings/parameters
         - Seed if applicable
         - Why this version was approved

      3. TEMPLATE FROM HERO:
         - Create parameterized version
         - What varies (subject, format)
         - What stays locked (style, brand elements)

      4. SCALE WITH CONFIDENCE:
         - Generate batch from template
         - Compare back to hero
         - Flag/regenerate outliers only

      # Perfect the one, then make many
    symptoms:
      - Massive rework after feedback
      - "We need to redo everything"
      - Wasted compute/budget
      - Timeline blown
    detection_pattern: null

  - id: multi-tool-handoff-loss
    summary: Losing quality or context when moving between AI tools
    severity: medium
    situation: |
      Taking output from one AI tool and using as input to another without
      proper format handling, quality preservation, or context transfer.
    why: |
      Each tool has optimal input formats. Sending compressed JPEG to
      upscaler degrades quality. Not providing context to downstream tool
      means it doesn't understand the creative intent.
    solution: |
      HANDOFF PROTOCOL:

      1. FORMAT PRESERVATION:
         - Export at maximum quality from source
         - PNG/TIFF for images (no lossy compression)
         - Highest bitrate for video
         - WAV/FLAC for audio

      2. CONTEXT TRANSFER:
         Document for downstream tool:
         - What this asset is
         - What needs to happen
         - Reference the original brief
         - Any constraints

      3. TOOL-SPECIFIC INPUTS:
         - Check what format downstream tool wants
         - Prepare accordingly
         - Don't assume compatibility

      4. QUALITY CHECKPOINTS:
         - Verify quality at each handoff
         - Compare to original
         - Flag degradation early
    symptoms:
      - Quality drops through pipeline
      - Artifacts accumulate
      - Downstream tool gives poor results
      - "It looked better before"
    detection_pattern: null

  - id: feedback-loop-breakdown
    summary: No clear process for stakeholder feedback causing confusion
    severity: medium
    situation: |
      Multiple stakeholders giving feedback through different channels,
      conflicting notes, unclear what's approved, revision chaos.
    why: |
      AI production moves fast, but feedback still requires human alignment.
      Without clear feedback process, you implement conflicting changes,
      lose track of what's approved, and waste iterations.
    solution: |
      FEEDBACK SYSTEM:

      1. SINGLE SOURCE OF TRUTH:
         - All feedback in one place (Frame.io, Dropbox Replay)
         - Timestamped, attributed comments
         - Clear approve/reject status

      2. CONSOLIDATION:
         - Collect all stakeholder feedback first
         - Resolve conflicts before implementing
         - One decision-maker breaks ties

      3. VERSION CONTROL:
         - Clear versioning (v1, v2, v3)
         - Document what changed each version
         - Never overwrite previous versions

      4. APPROVAL PROTOCOL:
         - Explicit "APPROVED" status
         - Who approved, when
         - Approved = locked (changes = new version)

      # Consolidate feedback before implementing
    symptoms:
      - Conflicting feedback implemented
      - "Who approved this?"
      - Lost versions
      - Revision chaos
    detection_pattern: null

  - id: budget-blindness
    summary: Not tracking costs until budget is blown
    severity: medium
    situation: |
      Running many AI generations across multiple tools without tracking
      costs, then discovering budget is exhausted mid-project.
    why: |
      AI tools charge per generation/minute/token. High-volume production
      costs add up quickly across multiple tools. Without tracking, you
      don't know until you can't pay.
    solution: |
      COST TRACKING:

      1. BUDGET UPFRONT:
         - Estimate per-asset costs
         - Multiply by volume
         - Add 30% buffer for iterations
         - Get budget approval before starting

      2. TRACK IN REAL-TIME:
         ```
         Project Budget: $500
         Images (Midjourney): $50 used
         Video (Runway): $120 used
         Audio (Suno): $20 used
         Avatars (HeyGen): $80 used
         Total: $270 / $500 (54%)
         ```

      3. ALERTS:
         - 50%: Check pacing
         - 75%: Review remaining work
         - 90%: Stop and assess

      4. OPTIMIZE:
         - Batch similar generations
         - Reuse where possible
         - Don't regenerate approved assets
    symptoms:
      - Budget surprise
      - Project halted mid-stream
      - Scope cuts needed
      - "We can't afford to finish"
    detection_pattern: null

  - id: qa-at-end-only
    summary: Only doing quality control at the very end of production
    severity: medium
    situation: |
      Running full production pipeline, then discovering quality issues in
      final QA that require going back to the beginning.
    why: |
      Problems caught late are expensive to fix. If hero image has issues
      and you've already generated 100 variations, edited video, and added
      audio—you might need to redo everything.
    solution: |
      PROGRESSIVE QA:

      CHECKPOINT 1: HERO QA
      - After first asset of each type
      - Full quality check
      - Stakeholder approval
      - Before any batch work

      CHECKPOINT 2: BATCH SAMPLE QA
      - After ~10% of batch
      - Consistency check
      - Style drift check
      - Continue or adjust

      CHECKPOINT 3: MID-PRODUCTION QA
      - After enhancement phase
      - Before assembly
      - Quality preserved?
      - Brand consistent?

      CHECKPOINT 4: FINAL QA
      - Full review
      - But fewer surprises
      - Minor fixes only expected

      # Catch problems early when they're cheap to fix
    symptoms:
      - Major rework needed at end
      - "How did we miss this?"
      - Timeline blown
      - Quality compromises due to time
    detection_pattern: null

  - id: asset-organization-chaos
    summary: Can't find assets, unclear naming, version confusion
    severity: low
    situation: |
      Assets scattered across tools, local folders, cloud storage with no
      clear naming convention or organization system.
    why: |
      As production scales, finding the right asset becomes the bottleneck.
      "Which version?" "Where is it?" "Is this the approved one?" wastes
      time and risks using wrong assets.
    solution: |
      ASSET ORGANIZATION:

      1. NAMING CONVENTION:
         [Project]_[AssetType]_[Variant]_[Version]_[Date]
         Example: Q1Launch_Hero_Blue_v2_240115.png

      2. FOLDER STRUCTURE:
         /Project
           /01_Brief
           /02_References
           /03_Working
           /04_Approved
           /05_Final
           /06_Archive

      3. SINGLE SOURCE:
         - One asset management system
         - All finals in one place
         - Clear approved vs working

      4. METADATA:
         - Tag with project, status, tool used
         - Link to original prompt/settings
         - Track versions

      # Time to find = time wasted
    symptoms:
      - "Where is that file?"
      - Wrong version used
      - Duplicate work
      - Lost assets
    detection_pattern: null

  - id: single-point-knowledge
    summary: Only one person knows how the workflow works
    severity: low
    situation: |
      All the AI tool knowledge, successful prompts, and workflow details
      live in one person's head. When they're unavailable, production stops.
    why: |
      AI production is complex and evolving. Without documentation, the
      organization can't scale, backup, or improve. It's fragile.
    solution: |
      KNOWLEDGE DISTRIBUTION:

      1. DOCUMENT EVERYTHING:
         - Workflow docs with screenshots
         - Prompt libraries with explanations
         - Tool setup guides
         - Common issues and solutions

      2. TEMPLATE LIBRARY:
         - Successful prompts as templates
         - Standard workflows as templates
         - Anyone can use without expert

      3. TRAINING:
         - Record workflow walkthroughs
         - Pair on productions
         - Build team capability

      4. CONTINUITY PLANNING:
         - Who's backup for each tool?
         - Shared access to all accounts
         - No single point of failure

      # If you get hit by a bus, production continues
    symptoms:
      - "Only X knows how to do that"
      - Production blocked by availability
      - Expertise not growing
      - Fragile capability
    detection_pattern: null
