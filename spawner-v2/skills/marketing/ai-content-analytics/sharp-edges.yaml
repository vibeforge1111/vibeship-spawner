id: ai-content-analytics-sharp-edges
skill: ai-content-analytics
version: 1.0.0

sharp_edges:
  - id: measuring-wrong-metrics
    summary: Tracking AI content outputs instead of business outcomes
    severity: critical
    situation: |
      Team celebrates: "We generated 5,000 AI blog posts this quarter!"
      Reality: 47 total conversions, $200k in AI tool costs, negative ROI.
      Measuring vanity (volume) instead of value (revenue).
    why: |
      Outputs are easy to measure but meaningless for business.
      AI makes it trivial to generate massive content volume.
      Without outcome tracking, you're flying blind - burning money on content
      that looks productive but drives zero business value.
    solution: |
      OUTCOME-FOCUSED METRICS:

      PRIMARY (Business Impact):
      - Conversions attributed to AI content (signups, purchases, leads)
      - Revenue attributed to AI content (direct + assisted)
      - ROI: (Revenue - Cost) / Cost
      - Cost per conversion (AI vs Human baseline)

      SECONDARY (Leading Indicators):
      - Engagement rate (time on page, scroll depth)
      - Click-through rate (to CTA or next step)
      - Bounce rate (inverse quality signal)
      - Return visitor rate (loyalty signal)

      TERTIARY (Supporting):
      - Content velocity (speed to publish)
      - Quality score (composite metric)
      - Brand sentiment (NPS impact, survey data)

      IGNORE (Vanity):
      - Total content pieces generated
      - Word count produced
      - Publishing frequency
      - Tool usage statistics

      DASHBOARD RULE:
      If the CEO asks "Is AI content working?", your dashboard should answer with
      revenue and ROI, not content volume.

      EXAMPLE:
      WRONG: "We published 200 AI articles this month! Up 400%!"
      RIGHT: "AI content drove $50k in attributed revenue at 6x ROI vs $25k human
              content at 2x ROI. AI wins on efficiency, human wins on absolute revenue."
    symptoms:
      - Celebrating volume milestones
      - No revenue/conversion tracking
      - Can't answer "is AI content working?"
      - Team measures effort, not impact
    detection_pattern:
      context:
        - "ai content dashboard"
        - "ai content metrics"
        - "ai analytics"
      intent:
        - "how many"
        - "content volume"
        - "publishing stats"
        - "output metrics"
      code: null

  - id: attribution-complexity-paralysis
    summary: Attribution is hard with AI+human hybrid content, so teams track nothing
    severity: critical
    situation: |
      AI generates draft → Human edits → Content published.
      Who gets credit for the conversion? Team can't decide, tracks nothing.
      Result: Can't prove AI value, can't optimize, can't justify investment.
    why: |
      AI content often exists in hybrid workflows (AI draft + human polish).
      Attribution is messy when multiple contributors touch the content.
      Without attribution, you can't measure what's working or defend the AI investment.
    solution: |
      ATTRIBUTION FRAMEWORK FOR HYBRID CONTENT:

      1. TAG EVERYTHING
         - AI-generated: tag with ai_draft_version, model_used, prompt_id
         - Human-edited: tag with editor_id, edit_time_minutes, edit_type (light/heavy)
         - Hybrid: tag with both AI and human metadata

      2. DEFINE CONTRIBUTION
         Three approaches:
         a) Binary: If AI generated initial draft, tag as "AI content"
         b) Weighted: Split credit based on effort (80% AI, 20% human)
         c) Separate tracking: Track AI-only, human-only, and hybrid as separate categories

      3. TRACK FULL JOURNEY
         - UTM parameters: utm_content=ai-blog-prompt-v3
         - Event tracking: ai_content_view, ai_content_engaged, ai_content_converted
         - CRM integration: Link conversions back to content source

      4. USE MULTI-TOUCH ATTRIBUTION
         - First-touch: Credit AI content for starting journey
         - Last-touch: Credit final touchpoint (often not AI)
         - Linear: Equal credit to all touchpoints
         - Position-based: More weight to first and last
         - Data-driven: ML model learns credit distribution

      5. REPORT WITH CONTEXT
         Don't just say "AI content drove X conversions"
         Say "AI content (with human editing) drove X conversions, representing Y% of
         total content conversions at Z% of the cost"

      RECOMMENDATION:
      Start simple: Binary tagging (AI vs Human vs Hybrid)
      Evolve to: Weighted contribution as you get more data

      TOOLING:
      - Google Analytics 4: Custom dimensions for AI metadata
      - Segment: Event tracking with AI properties
      - Attribution tool (Rockerbox, Northbeam): Multi-touch modeling
      - CRM (HubSpot, Salesforce): Closed-loop attribution

      EXAMPLE:
      Customer journey:
      1. Google → AI blog post (first touch, tagged ai_content=true)
      2. Return → Human case study (mid-funnel)
      3. Email → AI nurture sequence (AI-generated, human-approved)
      4. Demo → Sales call (human)
      5. Conversion → Purchase

      Attribution:
      - Last-touch: 0% to AI content (miss)
      - First-touch: 100% to AI blog (overstate)
      - Linear: 40% to AI content (reasonable)
      - Position-based: 35% to AI content (good balance)
    symptoms:
      - No content source tracking
      - Can't attribute conversions to content
      - "We think AI content helps but can't prove it"
      - Hybrid content not categorized
    detection_pattern:
      context:
        - "ai content attribution"
        - "track ai content"
        - "measure ai impact"
      intent:
        - "can't track"
        - "attribution is hard"
        - "hybrid content"
        - "how to measure"
      code: null

  - id: sample-size-impatience
    summary: Calling A/B test winners before reaching statistical significance
    severity: high
    situation: |
      A/B testing two AI variations.
      After 50 visitors: Variant A has 12% conversion, Variant B has 8%.
      Team ships Variant A as winner. After 1000 visitors: both converge to 10%.
      Wasted engineering effort shipping noise.
    why: |
      Small samples have high variance. Early winners often regress to the mean.
      AI makes it easy to generate many variations, tempting to test too many too fast.
      Shipping false positives wastes time and opportunity cost.
    solution: |
      STATISTICAL RIGOR FOR AI A/B TESTS:

      1. CALCULATE REQUIRED SAMPLE SIZE
         Use online calculator (Optimizely, VWO, Evan's Awesome A/B Tools)
         Inputs:
         - Baseline conversion rate (e.g., 10%)
         - Minimum detectable effect (e.g., 20% relative improvement = 12% absolute)
         - Statistical power (80% or 90%)
         - Significance level (p < 0.05)

         Example: 10% baseline, 20% MDE, 80% power, p<0.05
         → Need ~1,600 visitors per variant

      2. WAIT FOR SIGNIFICANCE
         Don't peek until you hit minimum sample size
         Use Bayesian A/B test if you must peek early (adjusts for peeking)
         Check p-value AND confidence interval

      3. VALIDATE WINNING PATTERN
         Don't just ship the winner and forget
         Generate new variations based on winning pattern
         Test if the pattern holds (e.g., "urgency works" → test 3 more urgency variants)

      4. MULTI-ARMED BANDIT (ADVANCED)
         If you have many AI variants to test (10+), use MAB algorithm
         Automatically shifts traffic to winners during test
         Reduces opportunity cost of testing losers
         Tools: Statsig, Split.io, Google Optimize 360

      5. SEGMENT ANALYSIS
         Even with significance, check if result holds across segments
         Desktop vs mobile, organic vs paid, new vs returning
         Variants can win overall but lose in key segments

      PROCESS:
      - Generate 5-10 AI variants
      - Calculate required sample size per variant
      - Run test for duration needed (often 2-4 weeks)
      - Check statistical significance (p < 0.05)
      - Validate practical significance (is lift worth implementing?)
      - Ship winner, start next test with new variants based on winning pattern

      RED FLAGS:
      - "After 2 days, Variant B is winning!" (too early)
      - "We tested 47 variants at once!" (multiple testing problem)
      - "Variant A has 15% conversion vs 14%, let's ship it!" (1% diff not significant)

      TOOLS:
      - Sample size calculator: Optimizely, Evan's Awesome A/B Tools
      - A/B testing platform: VWO, Optimizely, Google Optimize 360, Statsig
      - Statistical significance calculator: AB Testguide, Kissmetrics
    symptoms:
      - Declaring winners after <100 conversions
      - High variance in "winning" variants over time
      - Shipped variants that didn't improve metrics
      - No statistical significance checks
    detection_pattern:
      context:
        - "ab testing"
        - "ai variations"
        - "test results"
      intent:
        - "variant is winning"
        - "ship the winner"
        - "after a few days"
        - "early results"
      code: null

  - id: speed-vs-quality-false-tradeoff
    summary: Assuming AI content must sacrifice quality for speed
    severity: high
    situation: |
      Team uses AI to generate 10x content volume.
      Quality drops from 8/10 (human) to 4/10 (unedited AI).
      Traffic up, conversions down, brand perception damaged.
      "AI is fast but low quality" becomes team belief.
    why: |
      AI can be fast AND high quality with proper process.
      Unedited AI output is a draft, not a finished product.
      The unlock is AI draft speed + human editing efficiency = quality at scale.
    solution: |
      QUALITY AT SPEED FRAMEWORK:

      1. DEFINE QUALITY FLOOR
         Set minimum acceptable quality score (e.g., 7/10)
         Below floor = don't publish
         Quality components:
         - Accuracy (factual correctness)
         - Clarity (readable, scannable)
         - Value (useful, actionable)
         - Brand fit (tone, voice)
         - SEO (optimized, structured)

      2. TIERED PROCESS
         Not all content needs same quality:

         Tier 1 (High-stakes): 9/10 quality, AI draft + heavy human edit
         - Flagship content, thought leadership, conversion pages
         - Process: AI outline → AI draft → Human rewrite → Expert review
         - Time: 8 hours, Cost: $400, Quality: 9/10

         Tier 2 (Standard): 7.5/10 quality, AI draft + moderate edit
         - Blog posts, guides, case studies
         - Process: AI draft → Human edit → QA check
         - Time: 2 hours, Cost: $100, Quality: 7.5/10

         Tier 3 (Volume): 7/10 quality, AI draft + light edit
         - Social posts, email variants, FAQ answers
         - Process: AI generate → Human approve → Publish
         - Time: 15 minutes, Cost: $10, Quality: 7/10

      3. QUALITY CHECKPOINTS
         Don't publish without checks:
         - Automated: Grammar (Grammarly), plagiarism, broken links
         - Human: Accuracy review, brand voice check, value assessment
         - User: A/B test, engagement metrics, feedback loop

      4. ITERATION IMPROVES QUALITY
         First AI draft: 5/10 quality
         After 5 iterations with feedback: 7.5/10 quality
         The speed unlock is faster iteration, not lower quality acceptance

      5. MEASURE QUALITY OVER TIME
         Track quality score by content type
         If quality dropping, add process (more editing, better prompts)
         If quality high and stable, reduce process (less editing, trust AI more)

      REAL EXAMPLE:
      Company A: AI draft → publish (no editing)
      - Volume: 100 posts/month
      - Quality: 4/10
      - Conversions: 50/month
      - Cost: $500/month
      - CPConv: $10

      Company B: AI draft → human edit → publish
      - Volume: 40 posts/month
      - Quality: 7.5/10
      - Conversions: 120/month
      - Cost: $4,000/month
      - CPConv: $33

      Company C: Hybrid (AI for tier 3, human for tier 1)
      - Volume: 60 posts/month (40 tier 3 AI, 15 tier 2 hybrid, 5 tier 1 human)
      - Quality: 7/10 average
      - Conversions: 150/month
      - Cost: $3,000/month
      - CPConv: $20 ← WINNER

      KEY INSIGHT: Speed + quality is possible with tiered approach and editing
    symptoms:
      - "AI content is low quality"
      - Publishing unedited AI output
      - No quality standards defined
      - Quality declining as volume increases
    detection_pattern:
      context:
        - "ai content quality"
        - "speed vs quality"
        - "ai content performance"
      intent:
        - "ai is fast but"
        - "quality suffers"
        - "unedited ai"
      code: null

  - id: ignoring-long-term-brand-impact
    summary: Optimizing for short-term engagement at expense of brand trust
    severity: high
    situation: |
      AI generates clickbait headlines that spike CTR by 40%.
      Team ships it. Bounce rate increases, NPS drops, repeat visits decline.
      6 months later: traffic down, brand perception "spammy", hard to recover.
    why: |
      AI optimizes for what you measure. If you only measure CTR, AI finds clickbait.
      Short-term metric spikes can mask long-term brand damage.
      Trust is slow to build, fast to destroy, hard to measure.
    solution: |
      LONG-TERM BRAND HEALTH METRICS:

      1. TRACK BEYOND ENGAGEMENT
         Short-term (easy to game):
         - Click-through rate
         - Time on page
         - Page views

         Long-term (hard to game):
         - Repeat visitor rate (are they coming back?)
         - Net Promoter Score (would they recommend?)
         - Brand sentiment (survey: "feels authentic" vs "feels robotic")
         - Unsubscribe rate (are they opting out?)
         - Direct traffic growth (brand recall improving?)

      2. QUALITATIVE FEEDBACK LOOPS
         Quantitative metrics are lagging indicators for brand damage.
         Add qualitative signals:
         - User interviews: "How did this content make you feel about our brand?"
         - NPS surveys: Track before/after AI content rollout
         - Support tickets: Are complaints increasing? ("Your content is spam")
         - Social listening: What are people saying about your content?

      3. BALANCE METRICS
         Don't optimize for one metric in isolation.
         Combined score:
         - Engagement: 30% weight (CTR, time on page)
         - Conversion: 40% weight (signup, purchase)
         - Brand: 30% weight (NPS, repeat rate, sentiment)

         Total Score = (Engagement * 0.3) + (Conversion * 0.4) + (Brand * 0.3)

      4. COHORT ANALYSIS
         Track long-term behavior by cohort:
         - Users who first encountered AI content vs human content
         - Retention rate at 30/60/90 days
         - LTV by cohort

         If AI content cohort has lower LTV, AI is optimizing for wrong goal.

      5. GUARDRAILS
         Set limits AI cannot cross:
         - No clickbait language (define explicitly)
         - No misleading headlines
         - No sensationalism
         - Must deliver on promise (headline matches content)

      REAL EXAMPLE:
      E-commerce company:
      - Launches AI product descriptions optimized for CTR
      - CTR increases 35% (celebrate!)
      - 3 months later: Return rate up 20%, NPS down 15 points
      - Investigation: AI descriptions oversold products, set wrong expectations
      - Fix: Add "accuracy" as optimization metric, not just CTR
      - Result: CTR down 10%, but returns down 25%, NPS recovers

      DECISION FRAMEWORK:
      Before shipping AI content variant, ask:
      - Would I be proud to attach my name to this?
      - Does this build or erode trust?
      - Is this optimizing for right outcome (brand) or wrong one (clicks)?

      TOOLS:
      - NPS surveys: Delighted, Promoter.io, built into CRM
      - Sentiment analysis: Brandwatch, Mention, MonkeyLearn
      - User testing: UserTesting.com, Hotjar surveys
    symptoms:
      - CTR up, conversions down
      - Bounce rate increasing
      - NPS declining
      - Complaints about "spammy" content
      - Repeat visitor rate dropping
    detection_pattern:
      context:
        - "ai content optimization"
        - "improve ctr"
        - "engagement metrics"
      intent:
        - "increase clicks"
        - "optimize headlines"
        - "boost engagement"
      code: null

  - id: over-indexing-short-term
    summary: Focusing only on immediate conversion metrics, missing compound value
    severity: medium
    situation: |
      Team kills AI blog strategy because "blogs don't convert immediately."
      Meanwhile, competitor's AI blog content compounds into SEO authority,
      drives 60% of organic traffic 12 months later.
    why: |
      Some content is designed for immediate conversion (landing pages).
      Some content is designed to compound (SEO, thought leadership).
      Measuring both with same short-term metrics misses long-term value.
    solution: |
      CONTENT TIME HORIZONS:

      IMMEDIATE (0-7 days):
      - Landing pages, sales pages, email campaigns
      - Metric: Conversion rate, revenue
      - AI use case: A/B testing variations for immediate optimization

      SHORT-TERM (1-3 months):
      - Blog posts, guides, case studies
      - Metric: Traffic growth, backlinks, engagement
      - AI use case: Volume + quality to build content library

      LONG-TERM (6-24 months):
      - Pillar content, thought leadership, SEO authority
      - Metric: Organic traffic, domain authority, brand search volume
      - AI use case: Comprehensive guides, topic clusters

      MEASUREMENT APPROACH:
      1. Segment content by time horizon
      2. Set different success metrics per segment
      3. Track leading indicators (traffic, rankings) for long-term content
      4. Track lagging indicators (revenue) for immediate content

      EXAMPLE:
      AI Blog Post Analysis (12 months):
      Month 1: 100 visitors, 2 conversions, -ROI (cost > revenue)
      Month 3: 500 visitors, 10 conversions, break-even
      Month 6: 1,200 visitors, 30 conversions, +ROI
      Month 12: 2,500 visitors, 65 conversions, 10x ROI

      If measured at Month 1, would kill the blog.
      If measured at Month 12, clear winner.

      DECISION:
      Immediate content: Measure at 7 days
      Short-term content: Measure at 3 months
      Long-term content: Measure at 12+ months

      Don't judge SEO content by day-7 conversion rate.
    symptoms:
      - Killing content strategies too early
      - Only valuing immediate conversions
      - Not tracking long-term metrics
      - No patience for compounding content
    detection_pattern:
      context:
        - "content roi"
        - "content not converting"
        - "kill the blog"
      intent:
        - "immediate results"
        - "not working"
        - "shut it down"
      code: null

  - id: data-privacy-violations
    summary: Sending user data to AI models without proper consent or handling
    severity: critical
    situation: |
      Marketing team feeds customer emails, names, purchase history into AI to
      "personalize content." No consent, no anonymization.
      GDPR complaint filed. €20M fine. Brand crisis.
    why: |
      AI content personalization requires user data.
      Sending PII to third-party AI models (OpenAI, Anthropic) without consent violates
      GDPR, CCPA, and erodes user trust.
      Legal and ethical disaster.
    solution: |
      DATA PRIVACY FRAMEWORK FOR AI CONTENT:

      1. CLASSIFY DATA
         Public data: OK to use (website content, public posts)
         Anonymized data: OK after anonymization (aggregate trends)
         PII: Requires consent and careful handling (names, emails, purchase data)

      2. ANONYMIZATION
         Before sending to AI:
         - Remove names, emails, IDs
         - Aggregate data (e.g., "users like this" not "John Smith likes this")
         - Use synthetic data for testing

      3. CONSENT
         If using PII for AI personalization:
         - Explicit opt-in (not buried in ToS)
         - Clear explanation: "We use AI to personalize content"
         - Right to opt-out and delete

      4. VENDOR AUDIT
         Check AI vendor data policies:
         - Does OpenAI train on my data? (API: no, ChatGPT: yes)
         - Where is data stored? (US, EU, other?)
         - Data retention policy?
         - SOC 2, ISO 27001 certified?

      5. INTERNAL POLICIES
         - Data access controls (who can send data to AI?)
         - Logging and audit trails
         - Regular privacy audits
         - Incident response plan

      6. LEGAL COMPLIANCE
         - GDPR (EU): Right to access, delete, opt-out
         - CCPA (California): Right to know, delete, opt-out of sale
         - Industry-specific (HIPAA for health, COPPA for children)

      SAFE APPROACH:
      - Use aggregate, anonymized data for AI content insights
      - Personalize using first-party data on your infrastructure (not sent to AI)
      - Get explicit consent for AI-powered personalization
      - Document data flows and vendor agreements

      UNSAFE APPROACH:
      - Sending customer lists to ChatGPT for "personalized emails"
      - Using PII without consent for AI training or optimization
      - Assuming vendor privacy policy covers you (it doesn't)

      EXAMPLE:
      WRONG:
      Upload customer list (names, emails, purchase history) to ChatGPT:
      "Generate personalized email for each customer"
      → GDPR violation, data breach, no consent

      RIGHT:
      Use aggregate data: "Customers who bought X often also buy Y"
      Generate template in AI: "Email for customers interested in [CATEGORY]"
      Personalize on your server: Merge customer name from your CRM (not sent to AI)
      → Privacy-safe, compliant, personalized

      TOOLS:
      - Anonymization: AWS Macie, Microsoft Presidio, Google DLP
      - Consent management: OneTrust, Cookiebot, Osano
      - Vendor assessment: Vanta, Drata, Tugboat Logic
    symptoms:
      - Sending customer data to AI tools
      - No consent forms for AI personalization
      - No data flow documentation
      - No vendor privacy review
    detection_pattern:
      context:
        - "ai personalization"
        - "customer data"
        - "ai content optimization"
      intent:
        - "use customer data"
        - "personalize with ai"
        - "feed data to ai"
      code: null

  - id: model-drift-ignorance
    summary: Not monitoring AI model quality degradation over time
    severity: high
    situation: |
      AI content quality was great 3 months ago (8.5/10).
      Today: 6/10, conversions down 40%.
      Investigation: AI model updated by vendor, team didn't notice.
      Lost 3 months of degraded performance.
    why: |
      AI models change: vendor updates, fine-tuning drift, training data shifts.
      What worked last quarter may fail today.
      Without monitoring, you'll ship degraded content for months before noticing.
    solution: |
      MODEL DRIFT MONITORING:

      1. BASELINE QUALITY METRICS
         Establish baseline when launching AI content:
         - Quality score: 8/10
         - Conversion rate: 12%
         - Engagement: 3 min avg time on page
         - Sentiment: 4.2/5 stars

      2. CONTINUOUS MONITORING
         Track quality metrics daily/weekly:
         - Quality score trending down?
         - Conversion rate declining?
         - User complaints increasing?
         - Engagement dropping?

      3. ALERTS
         Set thresholds for degradation:
         - Quality drops >10%: Warning
         - Quality drops >20%: Critical alert
         - Conversion rate drops >15%: Investigate
         - Multiple metrics declining: Model drift suspected

      4. VERSION CONTROL
         Track what changed:
         - AI model version (GPT-4-0314 → GPT-4-0613)
         - Prompt versions (v1, v2, v3)
         - Date of changes
         - Performance before/after

      5. RESPONSE PLAN
         When drift detected:
         - Investigate: Model update? Prompt issue? Audience shift?
         - A/B test: New model vs old model (if available)
         - Adjust prompts: Compensate for model changes
         - Switch models: If vendor degraded, try competitor

      6. PROACTIVE MEASURES
         - Subscribe to vendor changelogs (OpenAI, Anthropic release notes)
         - Test new model versions before switching (parallel testing)
         - Keep fallback model ready (if GPT-4 degrades, switch to Claude)

      REAL EXAMPLE:
      Company using GPT-4 for email subject lines:
      - Feb 2024: CTR 28%, quality 8.5/10
      - May 2024: CTR 22%, quality 7/10 (down 21% and 18%)
      - Investigation: OpenAI released GPT-4 Turbo update in April
      - Action: Adjusted prompts to be more specific, CTR recovered to 26%
      - Lesson: Monitor quality after every vendor update

      MONITORING SETUP:
      - Dashboard: Daily quality score, conversion rate, engagement
      - Alerts: Slack/email when quality drops >10%
      - Changelog: Track all model/prompt changes
      - Review: Weekly quality review, monthly deep dive

      TOOLS:
      - Monitoring: Grafana, Datadog, custom dashboard
      - Logging: Track model version, prompt version per content piece
      - Alerting: PagerDuty, Slack webhooks, email
    symptoms:
      - Quality declining over time
      - No model version tracking
      - Unaware of vendor updates
      - Performance degradation unnoticed
    detection_pattern:
      context:
        - "ai content quality"
        - "performance declining"
        - "model performance"
      intent:
        - "quality dropped"
        - "not working anymore"
        - "used to work"
      code: null

  - id: cost-per-quality-blind-spots
    summary: Tracking total cost but not cost efficiency relative to quality/outcomes
    severity: high
    situation: |
      Team A: "AI content costs $5 per piece, human costs $400. AI is 80x cheaper!"
      Reality: AI converts at 2%, human at 15%. AI cost-per-conversion is $250, human is $53.
      AI is actually 5x more expensive per conversion despite being "cheaper."
    why: |
      Cheap content that doesn't convert is expensive.
      Expensive content that converts well is cheap.
      Cost-per-output is vanity, cost-per-outcome is ROI.
    solution: |
      COST EFFICIENCY FRAMEWORK:

      1. CALCULATE TOTAL COST
         - AI tool costs (API, subscription)
         - Human time (prompting, editing, QA)
         - Infrastructure (hosting, analytics, distribution)
         Total Cost = AI Tools + Human Time + Infrastructure

      2. MEASURE QUALITY
         Define quality composite score (0-100):
         - Engagement (30%): Time on page, scroll depth, bounce rate
         - Conversion (40%): CTR, signup rate, purchase rate
         - Brand (30%): Sentiment, NPS, backlinks
         Quality = (Engagement * 0.3) + (Conversion * 0.4) + (Brand * 0.3)

      3. COST PER QUALITY POINT
         CPQ = Total Cost / Quality Score
         Lower is better.

      4. COST PER CONVERSION
         CPC = Total Cost / Conversions
         The ultimate efficiency metric.

      5. COMPARE OPTIONS
         Don't compare AI to human on cost alone.
         Compare on cost-per-quality and cost-per-conversion.

      EXAMPLE:
      Option A (Unedited AI):
      - Cost: $5
      - Quality: 50
      - Conversions: 2
      - CPQ: $0.10
      - CPC: $2.50

      Option B (Edited AI):
      - Cost: $50
      - Quality: 75
      - Conversions: 10
      - CPQ: $0.67
      - CPC: $5

      Option C (Human):
      - Cost: $400
      - Quality: 90
      - Conversions: 20
      - CPQ: $4.44
      - CPC: $20

      DECISION MATRIX:
      - If optimizing for volume at low quality: Option A
      - If optimizing for ROI: Option B (best CPC for effort)
      - If optimizing for absolute conversions: Option C

      KEY INSIGHT:
      Option B (edited AI) has 10x higher cost than Option A, but 5x more conversions.
      It's worth spending more if ROI improves.

      AVOID:
      "AI content costs $5, human costs $400, let's use AI!"
      Without measuring conversions, you're optimizing for wrong metric.

      TRACK OVER TIME:
      As prompts improve and editing gets faster:
      - Option B cost drops from $50 to $30 (more efficient editing)
      - Quality stays at 75
      - CPQ improves from $0.67 to $0.40
      - ROI compounds

      TOOLS:
      - Spreadsheet with cost, quality, conversion tracking
      - Dashboard showing CPQ and CPC trends
      - Segment by content type (blog, email, landing page)
    symptoms:
      - Only tracking creation cost
      - Not tracking cost per conversion
      - Cheap content with low ROI
      - Expensive content dismissed without ROI analysis
    detection_pattern:
      context:
        - "ai content cost"
        - "ai vs human cost"
        - "content efficiency"
      intent:
        - "cheaper"
        - "save money"
        - "cost comparison"
      code: null

  - id: comparison-bias
    summary: Comparing AI content to different conditions (time, topic, distribution)
    severity: medium
    situation: |
      Team: "Our AI blog posts get 500 views on average!"
      Reality: Comparing to human blog posts from 2 years ago when SEO landscape was different.
      Or comparing AI posts (promoted on social) to human posts (not promoted).
      Apples to oranges.
    why: |
      Confounding variables invalidate comparisons.
      To prove AI value, you need controlled comparison: same topic, same time, same promotion.
      Otherwise, you're measuring market changes or distribution differences, not AI effectiveness.
    solution: |
      RIGOROUS COMPARISON FRAMEWORK:

      1. MATCH ON VARIABLES
         When comparing AI to human content, control for:
         - Topic: Same topic area (don't compare AI tech posts to human lifestyle posts)
         - Time period: Same month/quarter (avoid seasonality, algorithm changes)
         - Distribution: Same promotion level (both shared on social or neither)
         - Funnel stage: Both awareness, or both conversion (different goals = different metrics)
         - Format: Both blog posts, or both emails (format affects performance)

      2. A/B TEST PROPERLY
         Ideal: Simultaneous A/B test
         - AI version and human version of same content
         - Split traffic 50/50
         - Measure performance over same time period
         - Isolate AI as the only variable

      3. USE CONTROL GROUP
         If A/B test not feasible:
         - Create human baseline (10 human posts)
         - Create AI test group (10 AI posts on similar topics)
         - Publish in same time window
         - Compare performance

      4. SEGMENT ANALYSIS
         Even with matched variables, check segments:
         - Organic vs paid traffic (AI may perform differently by source)
         - Mobile vs desktop
         - New vs returning visitors
         - Geographic regions

      5. REPORT HONESTLY
         Don't cherry-pick:
         - Report full distribution (not just top performers)
         - Show variance (AI: avg 500 views, range 100-2000)
         - Acknowledge limitations (small sample, short time period)

      EXAMPLE OF BAD COMPARISON:
      "AI blog posts get 2x more traffic than human posts!"
      - AI posts: Published in 2024, promoted on social, trending topics
      - Human posts: Published in 2022, no promotion, evergreen topics
      → Can't conclude AI is better, too many confounds

      EXAMPLE OF GOOD COMPARISON:
      A/B test: "Best CRM Software" guide
      - Version A: Human-written, published Oct 15, promoted via email + social
      - Version B: AI-written (edited), published Oct 15, promoted via email + social
      - Split traffic 50/50 for 30 days
      - Result: Version A: 5,200 views, 85 conversions; Version B: 4,800 views, 78 conversions
      → Conclusion: AI performs at 92% of human (views) and 92% of human (conversions),
        at 20% of the cost. Strong ROI for AI.

      TOOLS:
      - A/B testing platform: VWO, Optimizely, Google Optimize 360
      - Analytics segmentation: Google Analytics 4, Mixpanel
      - Statistical analysis: Excel, Google Sheets, R, Python
    symptoms:
      - Comparing AI to old content
      - Different promotion levels for AI vs human
      - Comparing different topics or formats
      - No control for confounding variables
    detection_pattern:
      context:
        - "ai vs human performance"
        - "ai content comparison"
        - "ai content results"
      intent:
        - "ai gets more"
        - "ai performs better"
        - "compared to our old"
      code: null
