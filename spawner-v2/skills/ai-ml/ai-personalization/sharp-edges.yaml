# Sharp Edges - AI Personalization
# Gotchas, pitfalls, and edge cases in recommendation systems

version: 1.0.0
skill_id: ai-personalization

sharp_edges:
  - id: filter-bubble-effect
    summary: Recommendations create echo chambers
    severity: high
    symptoms:
      - "Users only see content reinforcing existing preferences"
      - "No exposure to diverse categories or viewpoints"
      - "Engagement metrics high but user satisfaction declining"
      - "Content variety in recommendations decreasing over time"
    why: |
      Pure relevance-optimized recommendations naturally converge on what users
      already like. This creates filter bubbles where users never discover new
      interests. While engagement metrics stay high short-term, users eventually
      feel the experience is stale or limiting.
    solution: |
      // Diversity-aware recommendation with exploration
      interface DiversityConfig {
        categoryDiversity: number; // 0-1, min different categories
        noveltyBoost: number; // Boost for items user hasn't seen category
        serendipitySlots: number; // Forced exploration slots
      }

      async function diverseRecommendations(
        userId: string,
        baseRecs: ScoredItem[],
        config: DiversityConfig
      ): Promise<ScoredItem[]> {
        const userHistory = await getUserCategoryHistory(userId);
        const underexposed = getUnderexposedCategories(userHistory);

        const result: ScoredItem[] = [];
        const usedCategories = new Set<string>();

        // Fill primary slots with diversity constraint
        for (const rec of baseRecs) {
          const category = rec.metadata.category;

          // Skip if category overrepresented
          if (usedCategories.size > 0) {
            const categoryRatio = Array.from(usedCategories)
              .filter((c) => c === category).length / result.length;

            if (categoryRatio > (1 - config.categoryDiversity)) {
              continue;
            }
          }

          // Boost if category underexposed
          if (underexposed.has(category)) {
            rec.score *= (1 + config.noveltyBoost);
          }

          result.push(rec);
          usedCategories.add(category);

          if (result.length >= baseRecs.length - config.serendipitySlots) {
            break;
          }
        }

        // Add serendipity slots from underexposed categories
        const serendipityItems = await getSerendipityItems(
          userId,
          underexposed,
          config.serendipitySlots
        );

        return [...result, ...serendipityItems];
      }
    detection_patterns:
      - "recommendations.*sort.*score"
      - "topK.*without.*diversity"
      - "similar.*only"

  - id: cold-start-empty-recs
    summary: New users get empty or random recommendations
    severity: high
    symptoms:
      - "New user sees 'No recommendations yet'"
      - "Random popular items that don't match user's signup intent"
      - "High bounce rate for new users"
      - "Users don't come back after first visit"
    why: |
      Collaborative filtering requires interaction history. Without it, the
      algorithm returns nothing or falls back to global popularity, which
      may not match why the user signed up in the first place.
    solution: |
      // Multi-phase cold start handling
      class ColdStartHandler {
        async getRecommendations(
          userId: string,
          options: { limit: number }
        ): Promise<RecommendationResult> {
          const userData = await this.getUserData(userId);

          // Phase 0: Brand new - use signup context
          if (userData.interactionCount === 0) {
            return this.signupContextRecs(userData.signupContext, options);
          }

          // Phase 1: Few interactions - blend popular + preference elicitation
          if (userData.interactionCount < 5) {
            return this.earlyPhaseRecs(userId, userData, options);
          }

          // Phase 2: Some data - hybrid with heavy content-based weight
          if (userData.interactionCount < 20) {
            return this.warmingPhaseRecs(userId, userData, options);
          }

          // Phase 3: Sufficient data - full collaborative
          return this.fullRecommendations(userId, options);
        }

        private async signupContextRecs(
          context: SignupContext,
          options: { limit: number }
        ): Promise<RecommendationResult> {
          // Use signup answers, referral source, landing page
          const categoryHints = this.extractCategoryHints(context);

          const popular = await this.getPopularInCategories(
            categoryHints,
            options.limit
          );

          return {
            items: popular,
            strategy: "signup_context",
            confidence: 0.3,
          };
        }

        private async earlyPhaseRecs(
          userId: string,
          userData: UserData,
          options: { limit: number }
        ): Promise<RecommendationResult> {
          // Blend: 70% content-based from early likes, 30% diverse exploration
          const contentBased = await this.contentBasedFromLikes(
            userData.likedItems,
            Math.floor(options.limit * 0.7)
          );

          const exploration = await this.diverseExploration(
            userData.seenCategories,
            Math.ceil(options.limit * 0.3)
          );

          return {
            items: [...contentBased, ...exploration],
            strategy: "early_hybrid",
            confidence: 0.5,
          };
        }
      }
    detection_patterns:
      - "if.*interactions.*length.*===.*0.*return.*\\[\\]"
      - "no recommendations"
      - "collaborative.*filter.*only"

  - id: popularity-bias
    summary: Popular items dominate, long-tail items never shown
    severity: medium
    symptoms:
      - "Same items appear in everyone's recommendations"
      - "New items never get recommended"
      - "Rich-get-richer effect in engagement data"
      - "Catalog coverage below 10%"
    why: |
      Popular items have more interaction data, making them score higher in
      collaborative filtering. This creates a feedback loop where popular items
      get more exposure, generating more data, becoming even more popular.
    solution: |
      // Inverse propensity scoring to debias popularity
      interface PopularityDebiasing {
        itemPopularity: Map<string, number>; // itemId -> interaction count
        alpha: number; // Debiasing strength (0.5-1.0)
      }

      function debiasScores(
        recommendations: ScoredItem[],
        config: PopularityDebiasing
      ): ScoredItem[] {
        // Calculate popularity scores
        const maxPop = Math.max(...config.itemPopularity.values());

        return recommendations.map((rec) => {
          const popularity = config.itemPopularity.get(rec.itemId) ?? 1;
          const normalizedPop = popularity / maxPop;

          // Inverse propensity weighting
          // Popular items get score reduced, rare items boosted
          const propensity = Math.pow(normalizedPop, config.alpha);
          const debiasedScore = rec.score / propensity;

          return { ...rec, score: debiasedScore };
        });
      }

      // Catalog coverage monitoring
      async function measureCatalogCoverage(
        recommendations: ScoredItem[][],
        totalCatalogSize: number
      ): Promise<CoverageMetrics> {
        const uniqueItems = new Set(
          recommendations.flat().map((r) => r.itemId)
        );

        return {
          coverage: uniqueItems.size / totalCatalogSize,
          uniqueItemsRecommended: uniqueItems.size,
          totalItems: totalCatalogSize,
          healthy: uniqueItems.size / totalCatalogSize > 0.2,
        };
      }
    detection_patterns:
      - "sort.*popularity"
      - "popular.*items.*first"
      - "without.*propensity"

  - id: stale-embeddings
    summary: User preferences change but embeddings don't update
    severity: medium
    symptoms:
      - "User explicitly dislikes items but similar ones keep appearing"
      - "Seasonal preferences not reflected (summer vs winter)"
      - "Life events (new job, moved) not captured"
      - "Recommendations feel dated"
    why: |
      User embeddings built from historical data reflect past preferences.
      If not regularly updated, they miss preference drift. Users change,
      and embeddings trained on old data become increasingly irrelevant.
    solution: |
      // Time-weighted embedding with decay
      interface EmbeddingConfig {
        halfLifeDays: number; // How fast old data decays
        minRecentWeight: number; // Minimum weight for oldest data
        updateFrequency: "realtime" | "hourly" | "daily";
      }

      async function updateUserEmbedding(
        userId: string,
        config: EmbeddingConfig
      ): Promise<number[]> {
        const interactions = await getInteractions(userId, { days: 90 });

        if (interactions.length === 0) {
          return getCurrentEmbedding(userId);
        }

        const now = Date.now();
        const halfLifeMs = config.halfLifeDays * 24 * 60 * 60 * 1000;

        // Weight interactions by recency
        const weightedTexts: string[] = [];

        for (const interaction of interactions) {
          const age = now - interaction.timestamp.getTime();
          const decay = Math.exp(-age / halfLifeMs);
          const weight = Math.max(decay, config.minRecentWeight);

          // More recent = more repetition in embedding input
          const repetitions = Math.ceil(weight * 10);
          for (let i = 0; i < repetitions; i++) {
            weightedTexts.push(interaction.itemDescription);
          }
        }

        // Include negative signals (explicit dislikes)
        const dislikes = await getNegativeSignals(userId, { days: 30 });
        const negativeContext = dislikes.length > 0
          ? `\nUser explicitly dislikes: ${dislikes.join(", ")}`
          : "";

        const input = weightedTexts.join(" ") + negativeContext;

        const response = await openai.embeddings.create({
          model: "text-embedding-3-small",
          input,
        });

        return response.data[0].embedding;
      }

      // Schedule regular updates
      // inngest/update-embeddings.ts
      export const updateUserEmbeddings = inngest.createFunction(
        { id: "update-user-embeddings" },
        { cron: "0 */6 * * *" }, // Every 6 hours
        async ({ step }) => {
          const activeUsers = await step.run("get-active-users", async () => {
            return db.user.findMany({
              where: { lastActive: { gte: subDays(new Date(), 7) } },
              select: { id: true },
            });
          });

          for (const user of activeUsers) {
            await step.run(`update-${user.id}`, async () => {
              await updateUserEmbedding(user.id, {
                halfLifeDays: 14,
                minRecentWeight: 0.1,
                updateFrequency: "daily",
              });
            });
          }
        }
      );
    detection_patterns:
      - "embedding.*created.*once"
      - "user.*embedding.*cache.*forever"
      - "without.*timestamp.*weight"

  - id: implicit-bias-amplification
    summary: Training data biases get amplified in recommendations
    severity: high
    symptoms:
      - "Gender-stereotyped recommendations (toys, careers)"
      - "Recommendations vary by user demographics inappropriately"
      - "Certain user segments consistently get lower quality recommendations"
    why: |
      If historical data reflects societal biases (e.g., women bought X, men
      bought Y), collaborative filtering learns and amplifies these patterns.
      The system becomes discriminatory even without explicit demographic features.
    solution: |
      // Bias detection and mitigation
      interface BiasMetrics {
        demographicParity: number; // Similar recs across groups
        equalizedOdds: number; // Similar accuracy across groups
        itemCoverageByGroup: Map<string, number>;
      }

      async function measureBias(
        testUsers: Array<{ userId: string; group: string }>,
        getRecommendations: (userId: string) => Promise<string[]>
      ): Promise<BiasMetrics> {
        const recsByGroup = new Map<string, string[][]>();

        for (const user of testUsers) {
          const recs = await getRecommendations(user.userId);

          if (!recsByGroup.has(user.group)) {
            recsByGroup.set(user.group, []);
          }
          recsByGroup.get(user.group)!.push(recs);
        }

        // Calculate item distribution per group
        const itemDistributions = new Map<string, Map<string, number>>();

        for (const [group, recLists] of recsByGroup) {
          const dist = new Map<string, number>();
          for (const recs of recLists) {
            for (const item of recs) {
              dist.set(item, (dist.get(item) ?? 0) + 1);
            }
          }
          itemDistributions.set(group, dist);
        }

        // Measure parity
        const groups = Array.from(recsByGroup.keys());
        let parityScore = 0;

        for (let i = 0; i < groups.length - 1; i++) {
          for (let j = i + 1; j < groups.length; j++) {
            const overlap = measureDistributionOverlap(
              itemDistributions.get(groups[i])!,
              itemDistributions.get(groups[j])!
            );
            parityScore += overlap;
          }
        }

        return {
          demographicParity: parityScore / (groups.length * (groups.length - 1) / 2),
          equalizedOdds: 0, // Requires ground truth
          itemCoverageByGroup: new Map(
            groups.map((g) => [g, itemDistributions.get(g)!.size])
          ),
        };
      }

      // Post-hoc fairness constraint
      function fairnessConstrainedRanking(
        recommendations: ScoredItem[],
        sensitiveCategories: string[],
        maxPerCategory: number
      ): ScoredItem[] {
        const categoryCounts = new Map<string, number>();
        const result: ScoredItem[] = [];

        for (const rec of recommendations) {
          const category = rec.metadata.sensitiveCategory;

          if (sensitiveCategories.includes(category)) {
            const count = categoryCounts.get(category) ?? 0;
            if (count >= maxPerCategory) {
              continue; // Skip over-represented category
            }
            categoryCounts.set(category, count + 1);
          }

          result.push(rec);
        }

        return result;
      }
    detection_patterns:
      - "train.*historical.*data"
      - "without.*bias.*check"
      - "demographic.*feature"

  - id: privacy-leakage
    summary: Recommendations reveal sensitive user information
    severity: critical
    symptoms:
      - "Health-related recommendations visible to shared account users"
      - "Gift recommendations spoiling surprises"
      - "Embarrassing categories in 'recommended for you' section"
    why: |
      Recommendations are inherently derived from user behavior. In shared
      accounts or when shown publicly (e.g., 'Users also bought'), they can
      inadvertently reveal sensitive information.
    solution: |
      // Privacy-aware recommendation filtering
      interface PrivacyConfig {
        sensitiveCategories: string[];
        requireExplicitConsent: boolean;
        sharedAccountMode: boolean;
      }

      async function privacyFilteredRecommendations(
        userId: string,
        recommendations: ScoredItem[],
        config: PrivacyConfig
      ): Promise<ScoredItem[]> {
        const user = await getUser(userId);

        // Filter sensitive categories unless consented
        if (!user.settings.showSensitiveRecommendations) {
          recommendations = recommendations.filter(
            (rec) => !config.sensitiveCategories.includes(rec.category)
          );
        }

        // For shared accounts, only show common interest items
        if (config.sharedAccountMode && user.isSharedAccount) {
          const commonInterests = await getCommonInterests(user.accountId);
          recommendations = recommendations.filter((rec) =>
            commonInterests.some((interest) => rec.tags.includes(interest))
          );
        }

        return recommendations;
      }

      // Differential privacy for similar users
      function differentialPrivacySimilarUsers(
        similarities: Array<{ userId: string; score: number }>,
        epsilon: number // Privacy budget
      ): Array<{ userId: string; score: number }> {
        // Add Laplacian noise
        return similarities.map((s) => ({
          userId: s.userId,
          score: s.score + laplacianNoise(1 / epsilon),
        }));
      }

      function laplacianNoise(scale: number): number {
        const u = Math.random() - 0.5;
        return -scale * Math.sign(u) * Math.log(1 - 2 * Math.abs(u));
      }

      // Don't show "Users who bought X also bought Y" if Y is sensitive
      async function filterPublicRecommendations(
        itemId: string,
        alsoBought: string[]
      ): Promise<string[]> {
        const sensitiveItems = await getSensitiveItems();
        return alsoBought.filter((id) => !sensitiveItems.has(id));
      }
    detection_patterns:
      - "similar.*users.*public"
      - "also.*bought.*without.*filter"
      - "recommendation.*shared.*account"

  - id: latency-at-scale
    summary: Recommendations become slow as catalog grows
    severity: medium
    symptoms:
      - "Recommendations take >500ms"
      - "Timeouts during peak traffic"
      - "Degraded experience as catalog grows"
    why: |
      Naive implementation scores every item in catalog for every request.
      With millions of items and high QPS, this becomes computationally
      prohibitive. Vector similarity on full catalog is O(n).
    solution: |
      // Multi-stage retrieval for scale
      interface ScalableRecommendationPipeline {
        // Stage 1: Fast candidate generation (ANN)
        generateCandidates(
          userEmbedding: number[],
          limit: number
        ): Promise<string[]>;

        // Stage 2: Lightweight filtering
        filterCandidates(
          candidates: string[],
          context: UserContext
        ): Promise<string[]>;

        // Stage 3: Heavy personalization scoring
        scoreCandidates(
          filtered: string[],
          userContext: UserContext
        ): Promise<ScoredItem[]>;

        // Stage 4: Business rules and diversity
        finalRerank(
          scored: ScoredItem[]
        ): Promise<ScoredItem[]>;
      }

      class FastRecommendations implements ScalableRecommendationPipeline {
        private vectorIndex: VectorIndex;
        private modelCache: LRUCache<string, ScoringModel>;

        async getRecommendations(
          userId: string,
          limit: number
        ): Promise<ScoredItem[]> {
          const userContext = await this.getUserContext(userId);

          // Stage 1: ANN search for 500 candidates (~10ms)
          const candidates = await this.generateCandidates(
            userContext.embedding,
            500
          );

          // Stage 2: Filter to 100 (~5ms)
          const filtered = await this.filterCandidates(
            candidates,
            userContext
          );

          // Stage 3: Score remaining 100 (~50ms)
          const scored = await this.scoreCandidates(
            filtered,
            userContext
          );

          // Stage 4: Final rerank (~5ms)
          return this.finalRerank(scored).then((r) => r.slice(0, limit));
        }

        async generateCandidates(
          embedding: number[],
          limit: number
        ): Promise<string[]> {
          // Use HNSW index for O(log n) approximate nearest neighbors
          const results = await this.vectorIndex.query({
            vector: embedding,
            topK: limit,
            ef: 100, // Search effort parameter
          });

          return results.map((r) => r.id);
        }

        async filterCandidates(
          candidates: string[],
          context: UserContext
        ): Promise<string[]> {
          // Fast boolean filters (in stock, not seen, category match)
          const seen = new Set(context.recentlyViewed);
          const allowedCategories = new Set(context.preferredCategories);

          const itemMeta = await this.batchGetItemMeta(candidates);

          return candidates.filter((id) => {
            const meta = itemMeta.get(id);
            if (!meta || seen.has(id) || !meta.inStock) return false;
            if (allowedCategories.size > 0 && !allowedCategories.has(meta.category)) {
              return false;
            }
            return true;
          });
        }
      }

      // Precomputed recommendations for popular paths
      export const precomputePopularRecs = inngest.createFunction(
        { id: "precompute-popular-recs" },
        { cron: "0 * * * *" }, // Every hour
        async ({ step }) => {
          const segments = ["new_user", "power_user", "casual"];

          for (const segment of segments) {
            await step.run(`compute-${segment}`, async () => {
              const recs = await computeSegmentRecommendations(segment);
              await redis.set(`recs:segment:${segment}`, JSON.stringify(recs), {
                ex: 3600,
              });
            });
          }
        }
      );
    detection_patterns:
      - "for.*item.*of.*catalog"
      - "score.*all.*items"
      - "without.*candidate.*generation"

  - id: feedback-loop-collapse
    summary: Recommendations converge to narrow set of items
    severity: high
    symptoms:
      - "Same 50 items get 80% of recommendations"
      - "New items never break into top recommendations"
      - "A/B tests show no variation"
    why: |
      Items that get recommended get more exposure, more clicks, more positive
      signals, and thus score higher next time. This positive feedback loop
      causes the system to converge on a small set of "winners."
    solution: |
      // Exploration-exploitation balance
      interface ExplorationConfig {
        explorationRate: number; // 0-1, fraction of slots for exploration
        newItemBoost: number; // Boost factor for items < 7 days old
        qualityFloor: number; // Minimum score for exploration candidates
      }

      async function explorationAwareRecommendations(
        baseRecs: ScoredItem[],
        config: ExplorationConfig,
        limit: number
      ): Promise<ScoredItem[]> {
        const explorationSlots = Math.floor(limit * config.explorationRate);
        const exploitationSlots = limit - explorationSlots;

        // Top items for exploitation
        const exploitation = baseRecs.slice(0, exploitationSlots);

        // Sample from tail for exploration (Thompson Sampling style)
        const explorationCandidates = baseRecs
          .slice(exploitationSlots)
          .filter((r) => r.score >= config.qualityFloor);

        const exploration = sampleWithProbability(
          explorationCandidates,
          explorationSlots,
          (item) => item.score // Probability proportional to score
        );

        // Boost new items
        const now = Date.now();
        const boosted = [...exploitation, ...exploration].map((rec) => {
          const itemAge = now - rec.metadata.createdAt;
          const isNew = itemAge < 7 * 24 * 60 * 60 * 1000;

          return {
            ...rec,
            score: isNew ? rec.score * config.newItemBoost : rec.score,
          };
        });

        return boosted.sort((a, b) => b.score - a.score);
      }

      // Track exploration effectiveness
      interface ExplorationMetrics {
        explorationCTR: number;
        exploitationCTR: number;
        discoveryRate: number; // Users who engaged with explored items
      }

      async function measureExplorationEffectiveness(
        userId: string,
        recommendations: ScoredItem[],
        engagements: Set<string>
      ): Promise<ExplorationMetrics> {
        const explored = recommendations.filter((r) => r.metadata.isExploration);
        const exploited = recommendations.filter((r) => !r.metadata.isExploration);

        const explorationEngaged = explored.filter((r) =>
          engagements.has(r.itemId)
        );
        const exploitationEngaged = exploited.filter((r) =>
          engagements.has(r.itemId)
        );

        return {
          explorationCTR: explorationEngaged.length / explored.length,
          exploitationCTR: exploitationEngaged.length / exploited.length,
          discoveryRate: explorationEngaged.length > 0 ? 1 : 0,
        };
      }
    detection_patterns:
      - "top.*k.*only"
      - "without.*exploration"
      - "sort.*slice.*0"
