# Sharp Edges - Semantic Search
# Gotchas that cause failures, poor retrieval, or cost issues

version: 1.0.0
skill_id: semantic-search

sharp_edges:
  - id: embedding-model-version-drift
    summary: "Changing embedding models breaks existing vectors"
    severity: critical
    situation: "Upgrading from ada-002 to text-embedding-3-small or switching providers"
    why: |
      Different embedding models produce different vector spaces.
      Vectors from ada-002 are incompatible with text-embedding-3-small.
      Mixing models in the same index produces nonsensical results.

      You cannot:
      - Query with new model against old vectors
      - Partially re-embed (some docs with old, some with new)
      - Use cross-model similarity

    detection_pattern: |
      embed.*model.*(?:ada-002|text-embedding-3|voyage)(?!.*migrate|reindex)

    solution: |
      Track embedding model version in metadata:

      ```typescript
      interface VectorRecord {
        id: string;
        vector: number[];
        metadata: {
          text: string;
          embeddingModel: "text-embedding-3-small" | "voyage-3-lite";
          embeddingVersion: string; // e.g., "2024-01-01"
          indexedAt: string;
        };
      }

      // Migration strategy
      async function migrateToNewModel(
        oldModelRecords: VectorRecord[],
        newModel: string
      ) {
        console.log(`Migrating ${oldModelRecords.length} records to ${newModel}`);

        // 1. Create new namespace/collection
        const newNamespace = `${COLLECTION_NAME}_${newModel}`;

        // 2. Re-embed all documents
        for (const batch of chunk(oldModelRecords, 100)) {
          const texts = batch.map((r) => r.metadata.text);
          const newEmbeddings = await embed(texts, { model: newModel });

          await vectorIndex.upsert(
            newNamespace,
            batch.map((r, i) => ({
              id: r.id,
              vector: newEmbeddings[i],
              metadata: {
                ...r.metadata,
                embeddingModel: newModel,
                embeddingVersion: new Date().toISOString().split("T")[0],
              },
            }))
          );
        }

        // 3. Swap namespaces atomically
        // 4. Delete old namespace after validation
      }
      ```

  - id: lost-in-the-middle
    summary: "LLMs ignore context in the middle of long prompts"
    severity: high
    situation: "Providing many retrieved chunks to LLM context"
    why: |
      Research shows LLMs attend strongly to beginning and end of context,
      but struggle with information in the middle (the 'lost in the middle'
      phenomenon). Stuffing 10+ chunks into context means chunks 4-8 may
      be effectively ignored.

      Retrieval order matters: most relevant should be first or last.

    detection_pattern: |
      context.*join|chunks\.map.*content(?!.*reorder|sort)

    solution: |
      Reorder chunks for LLM attention patterns:

      ```typescript
      // Interleave important chunks at start and end
      function reorderForAttention<T>(
        items: T[],
        scoreKey: keyof T
      ): T[] {
        if (items.length <= 3) return items;

        // Sort by relevance
        const sorted = [...items].sort(
          (a, b) => (b[scoreKey] as number) - (a[scoreKey] as number)
        );

        // Interleave: best at start, second-best at end, etc.
        const reordered: T[] = [];
        let left = 0;
        let right = sorted.length - 1;

        sorted.forEach((item, i) => {
          if (i % 2 === 0) {
            reordered[left++] = item;
          } else {
            reordered[right--] = item;
          }
        });

        return reordered;
      }

      // Or: Just use fewer, higher-quality chunks
      const topChunks = results.slice(0, 3); // Better than 10 mediocre chunks
      ```

  - id: semantic-but-not-exact
    summary: "Vector search misses exact keyword matches"
    severity: high
    situation: "User searches for specific ID, code, or term"
    why: |
      Vector embeddings capture semantic meaning, not exact strings.
      Search for "ERROR-404" might return documents about "page not found"
      but miss the exact error code.

      Search for "useState" might return React documentation generally
      but miss the specific hook documentation.

    detection_pattern: |
      vector.*search|embed.*query(?!.*keyword|bm25|hybrid)

    solution: |
      Implement hybrid search:

      ```typescript
      async function hybridSearch(query: string, options?: { vectorWeight?: number }) {
        const { vectorWeight = 0.7 } = options || {};
        const keywordWeight = 1 - vectorWeight;

        // Parallel retrieval
        const [vectorResults, keywordResults] = await Promise.all([
          vectorSearch(query, { topK: 20 }),
          keywordSearch(query, { topK: 20 }), // Elasticsearch, Typesense, etc.
        ]);

        // Reciprocal Rank Fusion
        const scores: Record<string, number> = {};

        vectorResults.forEach((r, i) => {
          const rank = i + 1;
          scores[r.id] = (scores[r.id] || 0) + vectorWeight * (1 / (60 + rank));
        });

        keywordResults.forEach((r, i) => {
          const rank = i + 1;
          scores[r.id] = (scores[r.id] || 0) + keywordWeight * (1 / (60 + rank));
        });

        // Sort by fused score
        const allDocs = [...vectorResults, ...keywordResults];
        const seen = new Set<string>();

        return Object.entries(scores)
          .sort(([, a], [, b]) => b - a)
          .map(([id]) => {
            if (seen.has(id)) return null;
            seen.add(id);
            return allDocs.find((d) => d.id === id);
          })
          .filter(Boolean);
      }
      ```

  - id: embedding-cost-explosion
    summary: "Embedding costs spiral with naive re-indexing"
    severity: high
    situation: "Re-indexing documents on every update"
    why: |
      Embedding costs add up:
      - OpenAI text-embedding-3-small: $0.02/1M tokens
      - 1M documents Ã— 500 tokens = 500M tokens = $10 per full reindex
      - Daily reindex = $300/month just for embeddings

      Naive updates that re-embed unchanged documents waste money.

    detection_pattern: |
      reindex|update.*embed(?!.*hash|change|diff)

    solution: |
      Content-addressable embedding with change detection:

      ```typescript
      import { createHash } from "crypto";

      interface IndexedDocument {
        id: string;
        contentHash: string;
        vector: number[];
        indexedAt: Date;
      }

      // Hash content to detect changes
      function contentHash(text: string): string {
        return createHash("sha256").update(text).digest("hex").slice(0, 16);
      }

      // Only embed changed documents
      async function incrementalIndex(
        documents: Array<{ id: string; content: string }>,
        existingIndex: Map<string, IndexedDocument>
      ) {
        const toEmbed: typeof documents = [];
        const unchanged: string[] = [];

        for (const doc of documents) {
          const hash = contentHash(doc.content);
          const existing = existingIndex.get(doc.id);

          if (existing && existing.contentHash === hash) {
            unchanged.push(doc.id);
          } else {
            toEmbed.push(doc);
          }
        }

        console.log(`Unchanged: ${unchanged.length}, To embed: ${toEmbed.length}`);

        if (toEmbed.length === 0) return { embedded: 0 };

        // Only embed changed docs
        const embeddings = await batchEmbed(toEmbed.map((d) => d.content));

        // Upsert only changed
        await vectorIndex.upsert(
          toEmbed.map((doc, i) => ({
            id: doc.id,
            vector: embeddings[i],
            metadata: {
              contentHash: contentHash(doc.content),
              indexedAt: new Date().toISOString(),
            },
          }))
        );

        return { embedded: toEmbed.length, skipped: unchanged.length };
      }
      ```

  - id: chunking-breaks-context
    summary: "Fixed-size chunks split sentences and lose meaning"
    severity: medium
    situation: "Chunking documents for indexing"
    why: |
      "The patient was diagnosed with" [CHUNK BREAK] "diabetes and prescribed..."

      First chunk embeds to "medical diagnosis" vaguely.
      Second chunk loses the subject entirely.
      Neither chunk is useful for retrieval.

      Tables, code blocks, and lists are especially vulnerable.

    detection_pattern: |
      split.*\\d+|chunk.*size.*=|slice\\(.*\\d+

    solution: |
      Semantic-aware chunking:

      ```typescript
      function semanticChunk(text: string): string[] {
        const chunks: string[] = [];
        let current = "";

        // Split by semantic boundaries
        const sections = text.split(/(?=^#{1,3}\s)/m); // Headers

        for (const section of sections) {
          // Split section into paragraphs
          const paragraphs = section.split(/\n\n+/);

          for (const para of paragraphs) {
            // Keep tables and code blocks intact
            if (para.match(/^\|.*\|$/m) || para.match(/^```/m)) {
              if (current) chunks.push(current.trim());
              chunks.push(para);
              current = "";
              continue;
            }

            // Check if adding paragraph exceeds limit
            if (current.length + para.length > 1000) {
              if (current) chunks.push(current.trim());
              current = para;
            } else {
              current += (current ? "\n\n" : "") + para;
            }
          }
        }

        if (current) chunks.push(current.trim());

        return chunks.filter((c) => c.length > 50); // Filter tiny chunks
      }
      ```

  - id: cold-start-empty-results
    summary: "New index returns no results"
    severity: medium
    situation: "Just set up vector database, queries return empty"
    why: |
      Common cold start issues:
      - Documents not yet indexed (async indexing not complete)
      - Wrong namespace/collection being queried
      - Dimension mismatch between query and index
      - Filter excludes all documents
      - Index not yet built/optimized

    detection_pattern: |
      query.*results.*length.*===.*0|matches.*empty

    solution: |
      Graceful cold start handling:

      ```typescript
      async function searchWithFallback(
        query: string,
        options?: { namespace?: string; filter?: Record<string, unknown> }
      ) {
        const { namespace, filter } = options || {};

        // Check index health first
        const stats = await vectorIndex.describeIndexStats();
        const nsStats = stats.namespaces?.[namespace || ""];

        if (!nsStats || nsStats.vectorCount === 0) {
          return {
            results: [],
            warning: "Index is empty. Documents may still be indexing.",
            indexStats: stats,
          };
        }

        // Try with filter
        let results = await vectorIndex.query({
          vector: await embed(query),
          topK: 10,
          filter,
          namespace,
        });

        // If empty, try without filter
        if (results.matches?.length === 0 && filter) {
          console.warn("No results with filter, trying without...");
          results = await vectorIndex.query({
            vector: await embed(query),
            topK: 10,
            namespace,
          });

          if (results.matches?.length) {
            return {
              results: results.matches,
              warning: "No results matched filter. Showing unfiltered results.",
            };
          }
        }

        return { results: results.matches || [] };
      }
      ```

  - id: dimension-mismatch
    summary: "Query vectors don't match index dimensions"
    severity: critical
    situation: "Using different embedding models for indexing vs querying"
    why: |
      - text-embedding-3-small: 1536 dimensions
      - text-embedding-3-large: 3072 dimensions
      - voyage-3: 1024 dimensions

      Index expects 1536, query provides 1024 = cryptic error or silent failure.
      Some databases truncate silently, returning garbage results.

    detection_pattern: |
      embed.*model.*(?!.*same|match|config)

    solution: |
      Centralize embedding configuration:

      ```typescript
      // lib/embedding-config.ts
      const EMBEDDING_CONFIG = {
        model: "text-embedding-3-small",
        dimensions: 1536,
        provider: "openai",
      } as const;

      // Single embed function used everywhere
      export async function embed(text: string): Promise<number[]> {
        const response = await openai.embeddings.create({
          model: EMBEDDING_CONFIG.model,
          input: text,
          dimensions: EMBEDDING_CONFIG.dimensions,
        });
        return response.data[0].embedding;
      }

      // Validate on index creation
      export function validateIndexConfig(indexDimensions: number) {
        if (indexDimensions !== EMBEDDING_CONFIG.dimensions) {
          throw new Error(
            `Index dimensions (${indexDimensions}) don't match embedding config (${EMBEDDING_CONFIG.dimensions}). ` +
            `Using model: ${EMBEDDING_CONFIG.model}`
          );
        }
      }
      ```

  - id: rate-limit-cascade
    summary: "Embedding API rate limits cause cascade failures"
    severity: medium
    situation: "Batch indexing or high query volume"
    why: |
      OpenAI: 3,000 RPM (requests per minute) on most tiers
      Parallel indexing of 10,000 documents = 100 parallel calls
      = Immediate rate limit = 429 errors = Failed indexing

    detection_pattern: |
      Promise\.all.*embed|map.*embed(?!.*batch|queue|limit)

    solution: |
      Rate-limited batch processing:

      ```typescript
      import pLimit from "p-limit";

      const embeddingLimit = pLimit(10); // Max 10 concurrent requests

      async function batchEmbedWithLimits(
        texts: string[],
        options?: { batchSize?: number; delayMs?: number }
      ): Promise<number[][]> {
        const { batchSize = 100, delayMs = 100 } = options || {};
        const allEmbeddings: number[][] = [];

        // Split into batches
        const batches: string[][] = [];
        for (let i = 0; i < texts.length; i += batchSize) {
          batches.push(texts.slice(i, i + batchSize));
        }

        // Process with concurrency limit
        const results = await Promise.all(
          batches.map((batch, i) =>
            embeddingLimit(async () => {
              // Delay between batches
              if (i > 0) {
                await new Promise((r) => setTimeout(r, delayMs));
              }

              const response = await openai.embeddings.create({
                model: "text-embedding-3-small",
                input: batch,
              });

              return response.data.map((d) => d.embedding);
            })
          )
        );

        return results.flat();
      }
      ```

  - id: stale-cache-wrong-results
    summary: "Cached embeddings return outdated search results"
    severity: medium
    situation: "Caching query embeddings for performance"
    why: |
      If you cache query embeddings and documents update, the cached
      embedding still returns old similarity scores. User gets stale
      results even though index was updated.

      Also: cache key collisions if not careful with query normalization.

    detection_pattern: |
      cache.*embed|embed.*cache(?!.*ttl|expire|invalidate)

    solution: |
      Time-bounded caching with invalidation:

      ```typescript
      import { LRUCache } from "lru-cache";

      const queryEmbeddingCache = new LRUCache<string, number[]>({
        max: 1000,
        ttl: 1000 * 60 * 5, // 5 minute TTL
      });

      // Track index version
      let indexVersion = Date.now();

      async function embedQuery(query: string): Promise<number[]> {
        // Normalize query for cache key
        const normalizedQuery = query.toLowerCase().trim().replace(/\s+/g, " ");
        const cacheKey = `${indexVersion}:${normalizedQuery}`;

        const cached = queryEmbeddingCache.get(cacheKey);
        if (cached) return cached;

        const embedding = await embed(normalizedQuery);
        queryEmbeddingCache.set(cacheKey, embedding);

        return embedding;
      }

      // Call when index updates
      function invalidateQueryCache() {
        indexVersion = Date.now();
        // Old cache keys won't match
      }
      ```

  - id: multimodal-text-only-index
    summary: "Text embeddings miss image/table content"
    severity: medium
    situation: "Indexing documents with images, diagrams, tables"
    why: |
      Text embeddings only embed... text. If a document's key information
      is in a diagram or table that wasn't OCR'd properly, search will
      fail to find it.

      "Show me the system architecture" returns nothing because the
      architecture is only in a diagram, not text.

    detection_pattern: |
      pdf.*embed|document.*index(?!.*ocr|vision|multimodal)

    solution: |
      Multimodal indexing strategy:

      ```typescript
      async function indexDocument(doc: { path: string; type: "pdf" | "image" | "text" }) {
        const chunks: Array<{ text: string; type: string }> = [];

        if (doc.type === "pdf") {
          // Extract text
          const textContent = await extractPdfText(doc.path);
          chunks.push({ text: textContent, type: "text" });

          // Extract and describe images
          const images = await extractPdfImages(doc.path);
          for (const image of images) {
            const description = await describeImage(image); // Vision API
            chunks.push({
              text: `[Image: ${description}]`,
              type: "image_description",
            });
          }

          // Extract and format tables
          const tables = await extractPdfTables(doc.path);
          for (const table of tables) {
            chunks.push({
              text: tableToText(table), // Convert to searchable text
              type: "table",
            });
          }
        }

        // Embed all chunks
        for (const chunk of chunks) {
          await indexChunk(chunk);
        }
      }

      // Describe image for embedding
      async function describeImage(imageBase64: string): Promise<string> {
        const response = await anthropic.messages.create({
          model: "claude-sonnet-4-20250514",
          max_tokens: 500,
          messages: [
            {
              role: "user",
              content: [
                { type: "image", source: { type: "base64", media_type: "image/png", data: imageBase64 } },
                { type: "text", text: "Describe this image in detail for search indexing. Include all text, labels, relationships shown." },
              ],
            },
          ],
        });

        return response.content[0].type === "text" ? response.content[0].text : "";
      }
      ```
