# Crisis Communications Sharp Edges
# Real gotchas that destroy trust during incidents

sharp_edges:
  - id: golden-hour-violation
    title: Golden Hour Violation - Silence In The First 60 Minutes
    severity: critical
    situation: |
      Incident occurs at 2pm. Team scrambles to investigate. "We need to know
      what happened before we say anything." Two hours pass. Customers are
      tweeting. Support is overwhelmed. By the time you communicate, the
      narrative has been written for you - by angry customers.
    why: |
      Research shows responding within 1 hour results in 30% less reputation
      damage than waiting 3+ hours. The void you create gets filled by
      speculation, anger, and your competitors' subtle suggestions. Your
      first communication doesn't need answers - it needs acknowledgment.
    solution: |
      1. First response template (within 1 hour):
         """
         MINIMUM VIABLE COMMUNICATION:

         "We're aware that [specific symptom users see].
         We're investigating and will update you in [30 min].
         We're sorry for the disruption."

         That's it. Three sentences. Send it.
         """

      2. Pre-write templates for common issues:
         - Service outage
         - Performance degradation
         - Data issue discovered
         - Security incident
         - Third-party dependency failure

      3. Designate communication owner:
         - Not the person debugging
         - Someone whose job is to communicate
         - Has authority to post without approval

      4. Set calendar reminder:
         - "First update sent?" at T+30 min
         - Prevents getting lost in debugging
    symptoms:
      - "We're still investigating"
      - "We need to know root cause first"
      - Hours without external communication
      - Support team has no talking points
    detection_pattern: "incident|outage|investigating|no update"

  - id: ceo-hiding
    title: CEO Hiding - Leadership Absence During Crisis
    severity: high
    situation: |
      Major incident occurs. PR team sends generic statement. CEO is silent.
      Days pass. CEO finally posts: "We take this seriously." By then,
      customers have concluded leadership doesn't care. The apology feels
      corporate, not human.
    why: |
      In crisis, people want to hear from the person in charge. A PR statement
      feels like a shield. A CEO statement feels like accountability. When
      leadership hides, it signals that either they don't care, or the
      situation is worse than being disclosed.
    solution: |
      1. CEO visibility thresholds:
         """
         WHEN CEO MUST COMMUNICATE:
         - Any incident lasting >2 hours
         - Any data/security breach
         - Any incident making news
         - Any customer-facing apology needed
         - Any incident affecting >10% of users
         """

      2. CEO communication format:
         """
         First person: "I want to..."
         Take responsibility: "This is on me"
         Be specific: Not "we take security seriously"
         Commit personally: "I'm personally..."

         Example:
         "I want to address what happened today directly.
         At 2pm, we had a failure in our authentication
         system that locked many of you out. This is on us.
         I've been in the incident room since we discovered
         it, and I wanted to give you an update myself..."
         """

      3. Timing matters:
         - First 4 hours: Tweet/short post acceptable
         - First 24 hours: Full statement
         - Within week: Detailed postmortem

      4. CEO doesn't need all answers:
         - "I don't have all the details yet"
         - "I wanted to address this personally"
         - Honesty > polish
    symptoms:
      - CEO absent from communications
      - Only PR statements issued
      - "No comment from leadership"
      - Customers asking "where's the CEO?"
    detection_pattern: "CEO|founder|leadership|executive|statement"

  - id: support-blindsided
    title: Support Blindsided - Team Learns From Customers
    severity: high
    situation: |
      Incident starts. Engineering is debugging. Status page updated.
      But nobody told support. Customer service reps are answering tickets
      with "I don't see any issues on my end." Customers screenshot this
      and post on Twitter. Now you have two crises.
    why: |
      Support is your front line. They're the human voice customers hear.
      When they're uninformed, they give wrong information. When they give
      wrong information, customers feel gaslit. "The support person said
      everything was fine!" becomes the story, not the incident.
    solution: |
      1. Internal notification before external:
         """
         SEQUENCE:
         1. Alert internal Slack/Teams (ALL staff)
         2. Brief support with talking points
         3. THEN update status page
         4. THEN tweet/email

         Support needs 5-10 minute head start.
         """

      2. Support talking points template:
         """
         INCIDENT BRIEF FOR SUPPORT:

         WHAT'S HAPPENING:
         [One sentence description]

         CUSTOMER IMPACT:
         [What customers are experiencing]

         WHAT TO SAY:
         "Yes, we're aware of [issue]. Our team is working on it.
         I don't have an ETA yet but we're updating our status
         page at [URL]. I'm sorry for the inconvenience."

         WHAT NOT TO SAY:
         - "I don't see any issues"
         - Specific technical details
         - Time estimates (unless official)

         ESCALATION:
         Tag @incident-channel for anything unusual
         """

      3. Auto-alert support channels:
         - Status page changes → Support alert
         - PagerDuty triggers → Support alert
         - Customer complaint spike → Support alert

      4. Post-incident debrief support:
         - What questions did they get?
         - What information was missing?
         - Update talking points library
    symptoms:
      - Support says "no known issues"
      - Customers correcting support staff
      - Support asking "is something going on?"
      - Inconsistent information across channels
    detection_pattern: "support|customer service|help desk|talking points"

  - id: weasel-words
    title: Weasel Words - Corporate Language That Enrages
    severity: high
    situation: |
      Apology drafted. Legal reviews. PR polishes. Final version:
      "We apologize for any inconvenience this may have caused to some
      users." Customers read this. Rage intensifies. "ANY inconvenience?
      MAY have caused? SOME users? I lost a day of work!"
    why: |
      Weasel words are designed to minimize liability. Customers hear them
      as minimizing impact. "Any inconvenience" dismisses real harm.
      "Some users" makes individuals feel unimportant. "May have" denies
      what clearly happened. These words turn apologies into insults.
    solution: |
      1. Weasel word detector:
         """
         REMOVE THESE:
         ✗ "any inconvenience" → "the disruption to your work"
         ✗ "some users" → "[X] users" or "many of you"
         ✗ "may have experienced" → "experienced"
         ✗ "regret that this occurred" → "we're sorry we did this"
         ✗ "we take X seriously" → [show, don't tell]
         ✗ "at this time" → [remove entirely]
         ✗ "going forward" → [remove or be specific]
         """

      2. The "read it angry" test:
         """
         Before publishing, read your statement as if you're
         an angry customer who lost money/time/data.

         Every phrase that sounds dismissive? Remove it.
         Every hedge that sounds defensive? Remove it.

         If you wince reading it, so will they.
         """

      3. Legal review guidelines:
         """
         TO LEGAL:

         "I understand the liability concerns, but weasel words
         increase lawsuit risk by making us look evasive. Direct
         apologies with specific remediation plans are actually
         better legal strategy. Courts favor companies that
         showed genuine contrition."

         Offer: "I'll be specific about what we did and are doing.
         I won't speculate about causes we haven't confirmed."
         """

      4. Replace with specifics:
         """
         ✗ "We apologize for any inconvenience"
         ✓ "We're sorry we broke your workflow today. We know
            many of you had deadlines and we made them harder."

         ✗ "We take security seriously"
         ✓ "We've hired [firm] to audit our security. Here's
            what we're changing: [specific list]"
         """
    symptoms:
      - '"Any inconvenience" in apology'
      - Passive voice throughout
      - No specific numbers
      - Legal-reviewed to death
    detection_pattern: "inconvenience|regret|some users|may have|seriously"

  - id: premature-closure
    title: Premature Closure - Declaring Victory Too Soon
    severity: medium
    situation: |
      Incident seems resolved. Status page set to "Resolved." Tweet sent:
      "All systems operational." Ten minutes later, issue recurs. Now you
      update again. Customers are confused. "Didn't they just say it was
      fixed?" Trust erodes with each false resolution.
    why: |
      Pressure to end incidents leads to premature declarations. "Monitoring"
      should last longer than it does. When you declare resolved and aren't,
      customers question all future resolutions. They stop trusting your
      status page. The cry-wolf effect is real.
    solution: |
      1. Resolution criteria:
         """
         DON'T DECLARE RESOLVED UNTIL:

         - [ ] All error rates back to baseline (not just dropping)
         - [ ] At least 15 minutes of stability
         - [ ] Support ticket volume normalizing
         - [ ] No customer reports in last 10 minutes
         - [ ] Team agrees root cause addressed (not just symptoms)
         """

      2. Use "Monitoring" aggressively:
         """
         STATUS PROGRESSION:

         INVESTIGATING → IDENTIFIED → MONITORING → RESOLVED

         MONITORING means:
         "We've deployed a fix and are watching it.
         If stable for [30 min], we'll mark resolved."

         Don't skip MONITORING to look fast.
         """

      3. Resolution message template:
         """
         RESOLVED - [Time]

         This incident is resolved. [Service] is fully operational.

         We monitored for [X] minutes with no recurrence.

         If you're still experiencing issues, please let us know
         at [contact] - it may be a different problem.

         Full postmortem coming within [timeframe].
         """

      4. After false resolution:
         """
         If you declared too early, own it:

         "UPDATE: We declared this resolved too soon.
         We're seeing [issue] again. Back to investigating.
         Sorry for the confusion - we'll be more careful
         with resolution in the future."
         """
    symptoms:
      - Multiple "Resolved" then "Investigating" cycles
      - Resolved after 5 minutes of stability
      - Customer reports after resolution
      - Team not aligned on resolution criteria
    detection_pattern: "resolved|fixed|operational|closed"

  - id: channel-chaos
    title: Channel Chaos - Different Information Everywhere
    severity: high
    situation: |
      Incident ongoing. Status page says "Investigating." Twitter says
      "We've identified the issue." In-app banner says "Some features
      unavailable." Support is saying "Should be fixed in 30 minutes."
      Email hasn't gone out. Customers are comparing notes. They trust
      none of it.
    why: |
      Inconsistent information across channels creates confusion and
      erodes trust. Customers assume the worst version is true, or that
      you're hiding something. Coordination feels impossible in crisis,
      but it's essential. One voice, many channels.
    solution: |
      1. Single source of truth:
         """
         STATUS PAGE IS THE SOURCE.

         All other channels should:
         - Link to status page
         - Use same language
         - Update after status page updates

         If status page says "Investigating,"
         Twitter cannot say "Identified."
         """

      2. Update cascade:
         """
         SEQUENCE FOR EVERY UPDATE:

         1. Status page (source)
         2. Internal Slack (support prep)
         3. Twitter (link to status)
         4. In-app banner (if applicable)
         5. Email (for extended incidents only)

         Each update triggers next in sequence.
         Automate if possible.
         """

      3. Message synchronization:
         """
         TEMPLATE FOR ALL CHANNELS:

         [STATUS]: [One sentence description]

         Details: [status page URL]

         Each channel gets the SAME status word.
         Details always point to status page.
         """

      4. Channel ownership during incident:
         """
         Assign owners:
         - Status page: [Name]
         - Twitter: [Name]
         - Support: [Name]
         - Email: [Name]

         One person can own multiple, but ownership
         must be explicit. No "someone should update Twitter."
         """
    symptoms:
      - Different status words across channels
      - Customers quoting conflicting info
      - "Twitter says X but email says Y"
      - Support giving different timeline than public
    detection_pattern: "status page|twitter|email|channels|update"

  - id: postmortem-procrastination
    title: Postmortem Procrastination - Never Publishing The Analysis
    severity: medium
    situation: |
      Major incident resolved. Promise made: "We'll publish a full
      postmortem within a week." Week passes. Then two. Then a month.
      Internal postmortem done but "not ready for public." Customers
      who were promised transparency feel deceived again.
    why: |
      Postmortems are hard. They require admitting mistakes publicly.
      Legal worries about liability. PR wants to "move on." But customers
      remember the promise. Every day without it, they wonder what you're
      hiding. The cover-up (even if just procrastination) becomes the story.
    solution: |
      1. Commit to specific timeline:
         """
         STANDARD COMMITMENTS:

         Minor incidents: Internal postmortem within 48 hours
         Major incidents: Public postmortem within 5 business days
         Critical incidents: Preliminary within 48 hours, full within 2 weeks

         Put the date in your resolution message.
         Make it public commitment.
         """

      2. Postmortem template:
         """
         PUBLIC POSTMORTEM STRUCTURE:

         1. SUMMARY
            What happened, when, how long, who affected

         2. TIMELINE
            Key events with timestamps

         3. ROOT CAUSE
            Technical but accessible explanation

         4. IMPACT
            Specific numbers if possible

         5. WHAT WE'VE DONE
            Immediate fixes

         6. WHAT WE'RE DOING
            Longer-term prevention

         7. THANK YOU
            Acknowledge customer patience
         """

      3. Legal-friendly language:
         """
         TO LEGAL:

         "We're not admitting liability by being transparent.
         We're stating facts about what happened and what we're
         doing. Companies that publish postmortems face fewer
         lawsuits because they demonstrate good faith."

         Avoid: Speculation, blame individuals, unconfirmed causes
         Include: Timeline, facts, actions taken
         """

      4. If you can't meet deadline:
         """
         UPDATE ON POSTMORTEM:

         "We committed to publishing our postmortem by [date].
         We need more time to complete our investigation properly.
         New target: [date]. We haven't forgotten, and we'll
         deliver a thorough analysis."

         Extending is okay. Silence is not.
         """
    symptoms:
      - Postmortem promised but not delivered
      - Weeks since incident with no follow-up
      - "Internal postmortem done but not public"
      - Customers asking "where's the postmortem?"
    detection_pattern: "postmortem|follow-up|analysis|report|promised"

  - id: apology-without-action
    title: Apology Without Action - Sorry But Nothing Changes
    severity: high
    situation: |
      Third outage this month. Each time: "We're sorry, we're working
      on improvements." But no specifics. No visible changes. Same issues
      keep happening. Apologies start to feel like insults. "They're not
      sorry, they're just saying it."
    why: |
      Apologies without action are worse than no apology. They teach
      customers that your words mean nothing. Each empty apology trains
      them to expect nothing. Eventually, you can't apologize your way
      out because you've spent all your credibility.
    solution: |
      1. Every apology needs specifics:
         """
         ✗ "We're working on improvements"
         ✓ "We're adding redundancy to our payment system.
            This requires migrating to a multi-region setup,
            which we'll complete by [date]."

         ✗ "We take reliability seriously"
         ✓ "We're hiring two more SREs and implementing
            automated failover. Here's our public roadmap: [URL]"
         """

      2. Commit to metrics:
         """
         PUBLIC COMMITMENTS:

         "We're committing to 99.9% uptime this quarter.
         Here's our public status page history: [URL]

         If we miss this target, we'll [specific consequence]:
         - Credit customers X%
         - Publish detailed explanation
         - Adjust pricing accordingly"
         """

      3. Follow up on previous commitments:
         """
         MONTHLY UPDATE:

         "Last month we committed to [X]. Here's our progress:
         - [Action 1]: Complete
         - [Action 2]: In progress, 70%
         - [Action 3]: Delayed because [honest reason]

         We're not perfect, but we're keeping our promises."
         """

      4. When patterns emerge:
         """
         After 3rd similar incident:

         "We've had three [type] incidents in [timeframe].
         That's not acceptable. Here's what's different this time:

         1. We've promoted reliability to CEO priority
         2. We're investing $[X] in infrastructure
         3. We're bringing in [external firm] to audit
         4. We're publishing monthly reliability reports

         We understand if you're skeptical. Watch our actions."
         """
    symptoms:
      - Same issue recurring
      - Vague improvement promises
      - No follow-up on commitments
      - Customers saying "here we go again"
    detection_pattern: "improvements|working on|taking action|sorry again"

  - id: compensation-confusion
    title: Compensation Confusion - Making Credits Worse Than Incident
    severity: medium
    situation: |
      Major outage. Company decides to give credits. But: credits require
      submitting a ticket, expire in 30 days, don't apply to annual plans,
      require promo code... By the time customers navigate the process,
      they're angrier than before the compensation was offered.
    why: |
      Bad compensation is worse than no compensation. It says "we want
      credit for making it right without actually making it right."
      Every hurdle in claiming compensation is another reminder of the
      original failure. Make it automatic or don't bother.
    solution: |
      1. Automatic > Requested:
         """
         GOOD: "We're automatically crediting all affected
         accounts [X amount]. No action needed."

         BAD: "Affected users can submit a ticket to
         request consideration for a credit."

         If you're going to give credits, give them.
         Don't make customers beg.
         """

      2. Simple terms:
         """
         GOOD TERMS:
         - Automatic application
         - No expiration (or 12+ months)
         - Applies to all plan types
         - Clear amount communicated

         BAD TERMS:
         - Must request
         - Expires in 30 days
         - Only for monthly plans
         - "Up to" amounts
         """

      3. Compensation guidelines:
         """
         RULE OF THUMB:

         Hours of outage × 2 = Days of credit

         4 hour outage = ~1 week credit
         Full day outage = ~2 weeks credit
         Data loss = Month+ or refund

         Err on generous side. Goodwill > dollars.
         """

      4. Communication:
         """
         CREDIT ANNOUNCEMENT:

         "We've automatically applied a [amount/time] credit to
         all accounts affected by yesterday's outage. You'll
         see this reflected in your next billing cycle.

         No action needed on your part.

         We know this doesn't make up for lost time, but we
         hope it demonstrates we take this seriously."
         """
    symptoms:
      - Complex credit process
      - Credits with restrictions
      - Customers complaining about compensation process
      - "The credit was more annoying than the outage"
    detection_pattern: "credit|compensation|refund|make it right"

