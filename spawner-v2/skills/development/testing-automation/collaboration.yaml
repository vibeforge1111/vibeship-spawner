id: testing-automation-collaboration
skill: testing-automation
version: 1.0.0

# ============================================================================
# RECEIVES FROM (Who delegates TO this skill)
# ============================================================================
receives_from:
  - skill: backend
    context: "Backend code needs test coverage"
    receives:
      - "API endpoints to test"
      - "Business logic functions"
      - "Database operations"
      - "Service layer code"
    provides: "Unit and integration tests with proper mocking"

  - skill: frontend
    context: "Frontend components need testing"
    receives:
      - "React/Vue components"
      - "State management logic"
      - "User interaction flows"
      - "Form validation logic"
    provides: "Component tests and E2E tests with proper selectors"

  - skill: api-design-architect
    context: "API contracts need verification"
    receives:
      - "API specifications"
      - "Contract definitions"
      - "Request/response schemas"
    provides: "Contract tests and API integration tests"

  - skill: ci-cd-pipeline
    context: "Pipeline needs test automation setup"
    receives:
      - "CI/CD workflow requirements"
      - "Deployment targets"
      - "Environment configurations"
    provides: "Test commands, coverage thresholds, test parallelization config"

  - skill: database-schema-design
    context: "Database operations need testing"
    receives:
      - "Schema definitions"
      - "Migration scripts"
      - "Query patterns"
    provides: "Database integration tests with proper isolation"

# ============================================================================
# DELEGATION TRIGGERS
# ============================================================================
delegation_triggers:
  - trigger: "backend|api|service layer"
    delegate_to: backend
    pattern: sequential
    context: "Need backend code to test"
    handoff_data:
      - "Test requirements"
      - "Coverage expectations"
      - "Testing framework preference"
    receive: "Implementation code to test"

  - trigger: "frontend|component|ui"
    delegate_to: frontend
    pattern: sequential
    context: "Need frontend code to test"
    handoff_data:
      - "Test requirements"
      - "User flow specifications"
      - "Testing framework preference"
    receive: "Components to test"

  - trigger: "ci/cd|pipeline|automation"
    delegate_to: ci-cd-pipeline
    pattern: parallel
    context: "Tests need CI/CD integration"
    handoff_data:
      - "Test commands"
      - "Coverage thresholds"
      - "Parallelization needs"
    receive: "Pipeline configuration for tests"

  - trigger: "performance|load testing|stress test"
    delegate_to: performance-optimization
    pattern: parallel
    context: "Tests need performance validation"
    handoff_data:
      - "Performance baselines"
      - "Load scenarios"
      - "Acceptance criteria"
    receive: "Performance test configuration"

  - trigger: "security|penetration|vulnerability"
    delegate_to: cybersecurity
    pattern: parallel
    context: "Tests need security validation"
    handoff_data:
      - "Security requirements"
      - "Attack scenarios"
      - "Compliance needs"
    receive: "Security test patterns"

# ============================================================================
# FEEDBACK LOOPS
# ============================================================================
feedback_loops:
  receives_feedback_from:
    - skill: ci-cd-pipeline
      signal: "Tests failing in CI"
      action: "Debug CI-specific failures (timing, environment, parallelization)"

    - skill: observability-sre
      signal: "Production bugs not caught by tests"
      action: "Add missing test coverage for failure scenarios"

    - skill: performance-optimization
      signal: "Test suite too slow"
      action: "Optimize test execution, add parallelization, review mocking strategy"

    - skill: backend
      signal: "Tests break on refactoring"
      action: "Refactor tests to test behavior instead of implementation"

    - skill: frontend
      signal: "E2E tests flaky"
      action: "Improve selectors, add proper waits, fix timing issues"

  sends_feedback_to:
    - skill: backend
      signal: "Code is hard to test"
      action: "Improve dependency injection, reduce coupling"

    - skill: frontend
      signal: "Components lack testability"
      action: "Add data-testid attributes, improve component structure"

    - skill: ci-cd-pipeline
      signal: "Tests need better CI integration"
      action: "Configure caching, parallelization, coverage reporting"

    - skill: database-schema-design
      signal: "Tests need better data isolation"
      action: "Add test schema, configure transaction rollback"

# ============================================================================
# CROSS-DOMAIN INSIGHTS
# ============================================================================
cross_domain_insights:
  - domain: Site Reliability Engineering
    insight: |
      SREs understand that tests are production's first line of defense:
      - Flaky tests erode trust - team starts ignoring failures
      - Missing error path tests lead to production surprises
      - Test coverage != test quality
      - The tests that matter most test failure scenarios
      Good tests prevent pages.
    applies_when: "Evaluating test coverage and quality"

  - domain: DevOps Engineering
    insight: |
      DevOps engineers see tests as pipeline infrastructure:
      - Slow tests are a deployment bottleneck
      - Parallel test execution requires isolation
      - Test environments must match production
      - Flaky tests are a CI/CD reliability problem
      Fast, reliable tests enable continuous deployment.
    applies_when: "Integrating tests with CI/CD"

  - domain: Software Architecture
    insight: |
      Architects know that testability is a design property:
      - Hard-to-test code is usually poorly designed
      - Dependency injection enables unit testing
      - The testing pyramid reflects architecture layers
      - Integration boundaries define test boundaries
      Good architecture makes testing natural.
    applies_when: "Designing testable systems"

  - domain: Product Management
    insight: |
      Product managers care about confidence to ship:
      - Tests enable faster feature delivery (counterintuitive)
      - Regression bugs erode user trust
      - Test coverage decisions are product risk decisions
      - E2E tests protect critical user journeys
      Tests are investment in shipping velocity.
    applies_when: "Prioritizing test coverage"

  - domain: Security Engineering
    insight: |
      Security engineers see tests as vulnerability prevention:
      - Input validation tests prevent injection
      - Authentication tests prevent bypass
      - Authorization tests prevent privilege escalation
      - Error handling tests prevent information disclosure
      Security tests are cheaper than security incidents.
    applies_when: "Implementing security test patterns"

# ============================================================================
# COMMON COMBINATIONS
# ============================================================================
common_combinations:
  - name: Full-Stack Test Setup
    skills:
      - testing-automation
      - backend
      - frontend
    workflow: |
      1. Design testing strategy (testing-automation)
      2. Set up backend unit and integration tests
      3. Set up frontend component tests
      4. Create E2E tests for critical flows
      5. Configure coverage thresholds

  - name: TDD Feature Development
    skills:
      - testing-automation
      - backend
      - api-design-architect
    workflow: |
      1. Write API contract tests first (testing-automation)
      2. Implement API to pass tests (backend)
      3. Add unit tests for business logic
      4. Add integration tests for full flows
      5. Refactor with test safety net

  - name: Test-Integrated Pipeline
    skills:
      - testing-automation
      - ci-cd-pipeline
      - observability-sre
    workflow: |
      1. Create test suite (testing-automation)
      2. Configure test jobs in pipeline (ci-cd-pipeline)
      3. Add test metrics monitoring (observability-sre)
      4. Set up flaky test detection
      5. Configure coverage gates

  - name: Database Integration Testing
    skills:
      - testing-automation
      - database-schema-design
      - backend
    workflow: |
      1. Set up test database container
      2. Create migration test strategy
      3. Implement repository tests with real database
      4. Configure transaction rollback for isolation
      5. Add seed data factories

  - name: E2E Test Automation
    skills:
      - testing-automation
      - frontend
      - ci-cd-pipeline
    workflow: |
      1. Identify critical user journeys
      2. Create page objects/component models
      3. Write E2E tests with Playwright/Cypress
      4. Configure CI for E2E execution
      5. Set up visual regression testing

# ============================================================================
# ECOSYSTEM
# ============================================================================
ecosystem:
  primary_tools:
    - "Jest - JavaScript testing framework"
    - "Vitest - Vite-native testing framework"
    - "Pytest - Python testing framework"
    - "Playwright - Cross-browser E2E testing"
    - "Cypress - JavaScript E2E testing"
    - "Testing Library - User-centric testing utilities"
    - "MSW - Mock Service Worker for API mocking"

  alternatives:
    - name: Jest
      use_when: "JavaScript/TypeScript, React projects, existing Jest setup"
      avoid_when: "Vite projects (use Vitest), need faster execution"

    - name: Vitest
      use_when: "Vite projects, need fast HMR test reload, ESM-first"
      avoid_when: "Non-Vite projects, team unfamiliar with Vitest"

    - name: Pytest
      use_when: "Python projects, need fixtures, parametrized tests"
      avoid_when: "Non-Python projects"

    - name: Playwright
      use_when: "Cross-browser testing, need auto-wait, multiple languages"
      avoid_when: "Team has Cypress expertise, simple SPAs"

    - name: Cypress
      use_when: "JavaScript-only team, great developer experience, time-travel debugging"
      avoid_when: "Need cross-browser (WebKit), non-Node environments"

    - name: Testing Library
      use_when: "React/Vue/Angular component testing, user-centric approach"
      avoid_when: "Need to test implementation details (rare legitimate cases)"

    - name: MSW
      use_when: "API mocking, need same mocks in tests and browser, service workers"
      avoid_when: "Simple mocking needs, non-browser environments"

  testing_types:
    - name: Unit Tests
      use_when: "Business logic, pure functions, utilities, isolated components"
      tools: "Jest, Vitest, Pytest, Testing Library"
      target: "60-70% of test suite"

    - name: Integration Tests
      use_when: "API endpoints, database operations, service interactions"
      tools: "Supertest, Test Containers, Pytest fixtures"
      target: "20-30% of test suite"

    - name: E2E Tests
      use_when: "Critical user journeys, smoke tests, full-stack validation"
      tools: "Playwright, Cypress, Selenium"
      target: "5-10% of test suite"

    - name: Contract Tests
      use_when: "Microservices, API consumers, schema validation"
      tools: "Pact, Spectral, OpenAPI validators"
      target: "Per-API-boundary"

    - name: Visual Regression
      use_when: "UI changes, design systems, cross-browser appearance"
      tools: "Percy, Chromatic, Playwright visual comparisons"
      target: "Component libraries, critical pages"

    - name: Load Testing
      use_when: "Performance validation, capacity planning, SLA verification"
      tools: "k6, Locust, Artillery"
      target: "Before production launches"

  mocking_tools:
    - name: jest.mock/vi.mock
      use_when: "Module mocking, dependency replacement"

    - name: jest.spyOn/vi.spyOn
      use_when: "Watching function calls, partial mocking"

    - name: nock
      use_when: "HTTP request mocking in Node.js"

    - name: MSW (Mock Service Worker)
      use_when: "API mocking that works in browser and Node"

    - name: Test Containers
      use_when: "Real database/service in Docker for integration tests"

    - name: faker.js
      use_when: "Generating realistic test data"

