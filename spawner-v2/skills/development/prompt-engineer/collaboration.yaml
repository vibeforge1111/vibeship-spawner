# Prompt Engineer Collaboration Model
# How this skill works with other specialists

prerequisites:
  skills: []
  knowledge:
    - "LLM fundamentals (tokens, context windows)"
    - "Understanding of model capabilities"
    - "Basic programming for testing"
    - "Writing and communication skills"

complementary_skills:
  - skill: ai-agents-architect
    relationship: "Agent prompts"
    brings: "Agent design, tool definitions"

  - skill: rag-engineer
    relationship: "Context-aware prompts"
    brings: "Retrieval pipeline, context format"

  - skill: backend
    relationship: "Prompt integration"
    brings: "API implementation, prompt management"

  - skill: product-manager
    relationship: "Requirements"
    brings: "User needs, feature specs"

  - skill: test-architect
    relationship: "Prompt testing"
    brings: "Testing frameworks, evaluation metrics"

  - skill: data-engineering
    relationship: "Prompt data"
    brings: "Training data, evaluation datasets"

delegation:
  - trigger: "agent design"
    delegate_to: ai-agents-architect
    pattern: sequential
    context: "Prompt templates, tool descriptions"
    receive: "Agent architecture, tool specs"

  - trigger: "RAG context"
    delegate_to: rag-engineer
    pattern: parallel
    context: "Context format requirements"
    receive: "Retrieval pipeline, context structure"

  - trigger: "prompt API"
    delegate_to: backend
    pattern: parallel
    context: "Prompt templates, variables"
    receive: "API endpoints, prompt management"

  - trigger: "product requirements"
    delegate_to: product-manager
    pattern: sequential
    context: "Capability questions"
    receive: "User stories, acceptance criteria"

  - trigger: "prompt evaluation"
    delegate_to: test-architect
    pattern: parallel
    context: "Evaluation needs"
    receive: "Test framework, metrics"

  - trigger: "evaluation data"
    delegate_to: data-engineering
    pattern: parallel
    context: "Data requirements"
    receive: "Test datasets, golden examples"

collaboration_patterns:
  sequential:
    - "product-manager defines needs, then I design prompts"
    - "I write prompts, then ai-agents-architect integrates"
    - "I design templates, then backend implements API"

  parallel:
    - "I write prompts while test-architect builds evaluation"
    - "I design context format while rag-engineer builds retrieval"
    - "I iterate prompts while data-engineering prepares test data"

  review:
    - "Review ai-agents-architect's tool descriptions for clarity"
    - "Review rag-engineer's context format for prompt usability"
    - "Review test-architect's evaluation metrics for relevance"

cross_domain_insights:
  - domain: technical-writing
    insight: "Clarity trumps cleverness in instructions"
    applies_when: "Writing any prompt"

  - domain: psychology
    insight: "Priming and framing affect output"
    applies_when: "Structuring prompts"

  - domain: software-testing
    insight: "Edge cases break systems - test them first"
    applies_when: "Evaluating prompts"

  - domain: linguistics
    insight: "Word choice carries implicit meaning"
    applies_when: "Choosing prompt language"

  - domain: education
    insight: "Examples teach better than rules"
    applies_when: "Using few-shot prompts"

ecosystem:
  primary_tools:
    - "OpenAI Playground - Quick testing"
    - "Anthropic Workbench - Claude testing"
    - "Promptfoo - Prompt evaluation"
    - "LangSmith - Prompt tracing"
    - "Weights & Biases - Experiment tracking"

  alternatives:
    - name: PromptLayer
      use_when: "Need prompt versioning"
      avoid_when: "Simple use case"

    - name: Humanloop
      use_when: "Need prompt optimization loop"
      avoid_when: "Manual iteration preferred"

    - name: Agenta
      use_when: "Open source, self-hosted"
      avoid_when: "Want managed solution"

    - name: Braintrust
      use_when: "Need A/B testing for prompts"
      avoid_when: "Simple evaluation needs"

  deprecated:
    - "Changing prompts without measurement"
    - "Single-model testing only"
    - "Ignoring edge cases"
    - "Prompts without version control"
