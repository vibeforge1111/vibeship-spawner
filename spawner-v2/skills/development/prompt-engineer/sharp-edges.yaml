# Prompt Engineer Sharp Edges
# Gotchas and pitfalls in prompt engineering

sharp_edges:
  - id: vague-instructions
    summary: "Using imprecise language in prompts"
    severity: high
    situation: "Instructions like 'be helpful' or 'do a good job'"
    why: |
      LLMs interpret vague instructions differently than humans expect.
      "Be concise" means different things to different models and contexts.
      Without specific criteria, output varies unpredictably.
    solution: |
      Be explicit:
      - "Respond in 2-3 sentences"
      - "List exactly 5 items"
      - "Use only information from the provided context"
      - Test how model interprets your instructions
    symptoms:
      - "Output varies between runs"
      - "Model doesn't do what you meant"
      - "Works sometimes, fails sometimes"
    detection_pattern:
      language: generic
      pattern: "be helpful|do your best|as needed|when appropriate|if necessary"

  - id: no-output-format
    summary: "Expecting specific format without specifying it"
    severity: high
    situation: "Wanting JSON but not requiring it in prompt"
    why: |
      Models default to natural language. Without explicit format requirements,
      you get inconsistent structures that break parsing. Even with format
      instructions, examples are more reliable than descriptions.
    solution: |
      Specify format explicitly:
      - Include output schema or example
      - Use JSON mode when available
      - Show exact format in examples
      - Validate output programmatically
    symptoms:
      - "Parse errors on model output"
      - "Inconsistent JSON structure"
      - "Model adds explanatory text"
    detection_pattern:
      language: generic
      pattern: "json\\.loads\\(|parse.*response(?!.*format|.*schema)"

  - id: no-negative-instructions
    summary: "Only saying what to do, not what to avoid"
    severity: medium
    situation: "Prompts without explicit don'ts"
    why: |
      Models make predictable mistakes you could prevent. Without negative
      instructions, they hallucinate, add unwanted content, or format
      incorrectly. Don'ts are as important as dos.
    solution: |
      Include explicit don'ts:
      - "Do NOT make up information"
      - "Do NOT include explanatory text"
      - "NEVER mention that you're an AI"
      - Test for common failure modes
    symptoms:
      - "Model adds unwanted content"
      - "Predictable but undesired behaviors"
      - "Same mistakes repeatedly"
    detection_pattern:
      language: generic
      pattern: "system.*prompt(?!.*not|.*never|.*don't|.*avoid)"

  - id: prompt-guessing
    summary: "Changing prompts without measuring impact"
    severity: medium
    situation: "Tweaking prompts based on intuition alone"
    why: |
      Prompt changes have non-obvious effects. A 'better' prompt by intuition
      might actually hurt performance. Without measurement, you're guessing
      and may be making things worse.
    solution: |
      Systematic evaluation:
      - Create test set with expected outputs
      - Measure before and after changes
      - Track metrics over time
      - A/B test significant changes
    symptoms:
      - "Performance regresses unexpectedly"
      - "No idea if changes help"
      - "Endless prompt tweaking"
    detection_pattern:
      language: generic
      pattern: "prompt.*=|system_prompt.*=(?!.*evaluate|.*test|.*benchmark)"

  - id: context-overload
    summary: "Including irrelevant context 'just in case'"
    severity: medium
    situation: "Stuffing all available information into prompt"
    why: |
      More context can dilute important information, confuse the model,
      and increase costs. Models have attention limits - irrelevant content
      competes with relevant content.
    solution: |
      Curate context:
      - Include only relevant information
      - Use retrieval for large docs
      - Order context by importance
      - Set relevance thresholds
    symptoms:
      - "Model ignores key information"
      - "High token costs"
      - "Answers drift with more context"
    detection_pattern:
      language: generic
      pattern: "context.*=.*all|include.*everything|full.*document"

  - id: few-shot-bias
    summary: "Biased or unrepresentative examples"
    severity: medium
    situation: "Examples that don't represent expected inputs"
    why: |
      Models learn patterns from examples. If examples are biased toward
      certain cases, the model performs poorly on others. Examples should
      cover the expected distribution.
    solution: |
      Diverse examples:
      - Include edge cases
      - Cover expected input distribution
      - Balance example types
      - Include negative examples when helpful
    symptoms:
      - "Works on examples, fails on real inputs"
      - "Biased toward certain outputs"
      - "Narrow success patterns"
    detection_pattern:
      language: generic
      pattern: "examples.*=.*\\[(?!.*edge|.*different|.*varied)"

  - id: temperature-ignorance
    summary: "Using default temperature for all tasks"
    severity: medium
    situation: "Not adjusting temperature for task type"
    why: |
      Temperature affects output determinism. High temperature for factual
      tasks causes errors. Low temperature for creative tasks causes
      repetitive, boring outputs.
    solution: |
      Task-appropriate temperature:
      - 0-0.3 for factual/structured tasks
      - 0.5-0.7 for balanced tasks
      - 0.8-1.0 for creative tasks
      - Test different values
    symptoms:
      - "Factual errors from high temperature"
      - "Boring outputs from low temperature"
      - "Inconsistent creative quality"
    detection_pattern:
      language: generic
      pattern: "temperature.*=.*1\\.0|temperature.*=.*0(?!.*creative|.*factual)"

  - id: prompt-injection-naive
    summary: "Not considering prompt injection in user input"
    severity: high
    situation: "Concatenating user input directly into prompt"
    why: |
      User input can contain instructions that override your prompt. This
      is prompt injection - users can make the model ignore your instructions
      and follow theirs instead.
    solution: |
      Defend against injection:
      - Clearly delimit user input
      - Use XML tags or quotes
      - Instruction hierarchy (system > user)
      - Validate/sanitize user input
    symptoms:
      - "Model follows user instructions over yours"
      - "Unexpected behaviors with certain inputs"
      - "Security issues in production"
    detection_pattern:
      language: generic
      pattern: "prompt.*\\+.*user_input|f\".*\\{user|concat.*input"
