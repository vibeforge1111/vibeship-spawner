# AI Product Skill
# Building products with LLMs - not demos, real products

id: ai-product
name: AI Product Development
category: development
version: 1.0.0
last_updated: 2025-12-19

description: |
  Every product will be AI-powered. The question is whether you'll build it
  right or ship a demo that falls apart in production.

  This skill covers LLM integration patterns, RAG architecture, prompt
  engineering that scales, AI UX that users trust, and cost optimization
  that doesn't bankrupt you.

triggers:
  keywords:
    - llm
    - gpt
    - claude
    - openai
    - anthropic
    - ai integration
    - rag
    - embeddings
    - vector search
    - prompt engineering
    - fine tuning
    - ai ux
    - ai product
  file_patterns:
    - "**/ai/**"
    - "**/llm/**"
    - "**/prompt*.ts"
    - "**/chat*.ts"
    - "**/embedding*.ts"
  code_patterns:
    - "openai"
    - "anthropic"
    - "langchain"
    - "embedding"
    - "ChatCompletion"

principles:
  - name: LLMs are probabilistic, not deterministic
    description: |
      The same input can give different outputs. Design for variance.
      Add validation layers. Never trust output blindly. Build for the
      edge cases that will definitely happen.
    examples:
      good: "Validate LLM output against schema, fallback to human review"
      bad: "Parse LLM response and use directly in database"

  - name: Prompt engineering is product engineering
    description: |
      Prompts are code. Version them. Test them. A/B test them. Document them.
      One word change can flip behavior. Treat them with the same rigor as code.
    examples:
      good: "Prompts in version control, regression tests, A/B testing"
      bad: "Prompts inline in code, changed ad-hoc, no testing"

  - name: RAG over fine-tuning for most use cases
    description: |
      Fine-tuning is expensive, slow, and hard to update. RAG lets you add
      knowledge without retraining. Start with RAG. Fine-tune only when RAG
      hits clear limits.
    examples:
      good: "Company docs in vector store, retrieved at query time"
      bad: "Fine-tuned model on company data, stale after 3 months"

  - name: Design for latency
    description: |
      LLM calls take 1-30 seconds. Users hate waiting. Stream responses.
      Show progress. Pre-compute when possible. Cache aggressively.
    examples:
      good: "Streaming response with typing indicator, cached embeddings"
      bad: "Spinner for 15 seconds, then wall of text appears"

  - name: Cost is a feature
    description: |
      LLM API costs add up fast. At scale, inefficient prompts bankrupt you.
      Measure cost per query. Use smaller models where possible. Cache
      everything cacheable.
    examples:
      good: "GPT-4 for complex tasks, GPT-3.5 for simple ones, cached embeddings"
      bad: "GPT-4 for everything, no caching, verbose prompts"

anti_patterns:
  - name: Demo-ware
    description: AI features that work in demos but fail in production
    example: |
      Works with perfect input, falls apart with typos, edge cases,
      adversarial input, or high volume
    why_bad: Demos deceive. Production reveals truth. Users lose trust fast.
    fix: Test with real messy data. Add validation. Handle failures gracefully.

  - name: Context window stuffing
    description: Cramming everything into the context window
    example: |
      Entire codebase in context, all docs in prompt, no retrieval
    why_bad: Expensive, slow, hits limits. Dilutes relevant context with noise.
    fix: Smart retrieval (RAG). Only include relevant context. Summarize.

  - name: Unstructured output parsing
    description: Parsing free-form text instead of structured output
    example: |
      Asking for JSON in the prompt, parsing response with regex
    why_bad: Breaks randomly. Inconsistent formats. Injection risks.
    fix: Use function calling / tool use. Validate with Zod. Retry on failure.

  - name: No fallback strategy
    description: App breaks when LLM fails or returns garbage
    example: No error handling, no human fallback, no graceful degradation
    why_bad: APIs fail. Rate limits hit. Garbage in = garbage out.
    fix: Circuit breakers. Fallback to rules. Human-in-the-loop for critical paths.

  - name: Ignoring safety
    description: No guardrails for harmful or incorrect output
    example: |
      LLM outputs go directly to users, no content filtering, no fact checking
    why_bad: Hallucinations, inappropriate content, liability. Brand damage.
    fix: Content filters. Confidence thresholds. Human review for high-stakes.

frameworks:
  - name: RAG Architecture
    when_to_use: Adding domain knowledge to LLM
    structure:
      - "Ingestion: Chunk documents intelligently (semantic, not fixed-size)"
      - "Embedding: OpenAI or Cohere embeddings into vector DB"
      - "Retrieval: Query vector store, get top-k relevant chunks"
      - "Augmentation: Add retrieved context to prompt"
      - "Generation: LLM generates response with context"
    notes: |
      Vector DBs: Pinecone (managed), Weaviate (self-host), pgvector (postgres).
      Chunk size matters. 500-1000 tokens usually good. Overlap chunks.

  - name: Prompt Engineering Layers
    when_to_use: Structuring production prompts
    structure:
      - "System prompt: Role, constraints, format requirements"
      - "Few-shot examples: 2-5 input/output pairs"
      - "Retrieved context: RAG results"
      - "User input: Sanitized user query"
      - "Output format: Explicit structure with examples"

  - name: LLM Evaluation Framework
    when_to_use: Measuring AI feature quality
    structure:
      - "Build evaluation dataset (100+ examples with expected outputs)"
      - "Define metrics: accuracy, relevance, safety, latency, cost"
      - "Run evals on every prompt/model change"
      - "Track regressions in CI"
      - "A/B test in production with user feedback"

  - name: Cost Optimization Strategy
    when_to_use: Scaling AI features affordably
    structure:
      - "Model tiering: Use smallest model that works for each task"
      - "Caching: Cache embeddings, cache common query results"
      - "Prompt efficiency: Shorter prompts, fewer tokens"
      - "Batch processing: Aggregate requests where possible"
      - "Usage limits: Rate limit per user, usage tiers"

handoffs:
  receives_from:
    - skill: product-strategy
      receives: AI feature requirements
    - skill: backend
      receives: Data to integrate

  hands_to:
    - skill: frontend
      provides: AI component integration patterns
    - skill: devops
      provides: LLM monitoring and scaling requirements
    - skill: security
      provides: Content safety requirements

resources:
  essential:
    - title: "Anthropic Prompt Engineering Guide"
      url: "https://docs.anthropic.com/claude/docs/prompt-engineering"
      type: guide
      why: "Authoritative guide from Claude's creators"
    - title: "OpenAI Cookbook"
      url: "https://cookbook.openai.com"
      type: resource
      why: "Practical patterns for production AI"

  recommended:
    - title: "LangChain"
      url: "https://langchain.com"
      type: tool
      why: "LLM orchestration framework (use judiciously)"
    - title: "Vercel AI SDK"
      url: "https://sdk.vercel.ai"
      type: tool
      why: "Streaming AI responses in React/Next.js"
