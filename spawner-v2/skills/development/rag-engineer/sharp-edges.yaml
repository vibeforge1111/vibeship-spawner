# RAG Engineer Sharp Edges
# Gotchas and pitfalls in RAG system design

sharp_edges:
  - id: naive-chunking
    summary: "Fixed-size chunking breaks sentences and context"
    severity: high
    situation: "Using fixed token/character limits for chunking"
    why: |
      Fixed-size chunks split mid-sentence, mid-paragraph, or mid-idea.
      The resulting embeddings represent incomplete thoughts, leading to
      poor retrieval quality. Users search for concepts but get fragments.
    solution: |
      Use semantic chunking that respects document structure:
      - Split on sentence/paragraph boundaries
      - Use embedding similarity to detect topic shifts
      - Include overlap for context continuity
      - Preserve headers and document structure as metadata
    symptoms:
      - "Retrieved chunks feel incomplete or cut off"
      - "Answer quality varies wildly"
      - "High recall but low precision"
    detection_pattern:
      language: generic
      pattern: "chunk_size.*=.*\\d{3,4}|split.*\\[:.*\\]|textwrap"

  - id: no-metadata-filtering
    summary: "Pure semantic search without metadata pre-filtering"
    severity: medium
    situation: "Only using vector similarity, ignoring metadata"
    why: |
      Semantic search finds semantically similar content, but not necessarily
      relevant content. Without metadata filtering, you return old docs when
      user wants recent, wrong categories, or inapplicable content.
    solution: |
      Implement hybrid filtering:
      - Pre-filter by metadata (date, source, category) before vector search
      - Post-filter results by relevance criteria
      - Include metadata in the retrieval API
      - Allow users to specify filters
    symptoms:
      - "Returns outdated information"
      - "Mixes content from wrong sources"
      - "Users can't scope their searches"
    detection_pattern:
      language: generic
      pattern: "similarity_search\\(query\\)|search\\(embedding\\)|query.*without.*filter"

  - id: embedding-mismatch
    summary: "Using same embedding model for different content types"
    severity: medium
    situation: "One embedding model for code, docs, and structured data"
    why: |
      Embedding models are trained on specific content types. Using a text
      embedding model for code, or a general model for domain-specific
      content, produces poor similarity matches.
    solution: |
      Evaluate embeddings per content type:
      - Use code-specific embeddings for code (e.g., CodeBERT)
      - Consider domain-specific or fine-tuned embeddings
      - Benchmark retrieval quality before choosing
      - Separate indices for different content types if needed
    symptoms:
      - "Code search returns irrelevant results"
      - "Domain terms not matched properly"
      - "Similar concepts not clustered"
    detection_pattern:
      language: generic
      pattern: "all-MiniLM.*code|text-embedding.*code|single.*embedding.*model"

  - id: no-reranking
    summary: "Using first-stage retrieval results directly"
    severity: medium
    situation: "Taking top-K from vector search without reranking"
    why: |
      First-stage retrieval (vector search) optimizes for recall, not precision.
      The top results by embedding similarity may not be the most relevant
      for the specific query. Cross-encoder reranking dramatically improves
      precision for the final results.
    solution: |
      Add reranking step:
      - Retrieve larger candidate set (e.g., top 20-50)
      - Rerank with cross-encoder (query-document pairs)
      - Return reranked top-K (e.g., top 5)
      - Cache reranker for performance
    symptoms:
      - "Clearly relevant docs not in top results"
      - "Results order seems arbitrary"
      - "Adding more results helps quality"
    detection_pattern:
      language: generic
      pattern: "top_k.*=.*[1-9](?!.*rerank)|similarity_search.*limit.*\\d+$"

  - id: context-stuffing
    summary: "Cramming maximum context into LLM prompt"
    severity: medium
    situation: "Using all retrieved context regardless of relevance"
    why: |
      More context isn't always better. Irrelevant context confuses the LLM,
      increases latency and cost, and can cause the model to ignore the
      most relevant information. Models have attention limits.
    solution: |
      Use relevance thresholds:
      - Set minimum similarity score cutoff
      - Limit context to truly relevant chunks
      - Summarize or compress if needed
      - Order context by relevance
    symptoms:
      - "Answers drift with more context"
      - "LLM ignores key information"
      - "High token costs"
    detection_pattern:
      language: generic
      pattern: "context.*=.*\"\\s*\"\\.join|retrieved.*join|all.*chunks"

  - id: no-retrieval-evaluation
    summary: "Not measuring retrieval quality separately from generation"
    severity: high
    situation: "Only evaluating end-to-end RAG quality"
    why: |
      If answers are wrong, you can't tell if retrieval failed or generation
      failed. This makes debugging impossible and leads to wrong fixes
      (tuning prompts when retrieval is the problem).
    solution: |
      Separate retrieval evaluation:
      - Create retrieval test set with relevant docs labeled
      - Measure MRR, NDCG, Recall@K for retrieval
      - Evaluate generation only on correct retrievals
      - Track metrics over time
    symptoms:
      - "Can't diagnose poor RAG performance"
      - "Prompt changes don't help"
      - "Random quality variations"
    detection_pattern:
      language: generic
      pattern: "evaluate.*rag|test.*answer|assert.*response"

  - id: stale-embeddings
    summary: "Not updating embeddings when source documents change"
    severity: medium
    situation: "Embeddings generated once, never refreshed"
    why: |
      Documents change but embeddings don't. Users retrieve outdated content
      or, worse, content that no longer exists. This erodes trust in the
      system.
    solution: |
      Implement embedding refresh:
      - Track document versions/hashes
      - Re-embed on document change
      - Handle deleted documents
      - Consider TTL for embeddings
    symptoms:
      - "Returns outdated information"
      - "References deleted content"
      - "Inconsistent with source"
    detection_pattern:
      language: generic
      pattern: "embed_once|created_at(?!.*updated)|static.*embeddings"

  - id: ignoring-query-type
    summary: "Same retrieval strategy for all query types"
    severity: medium
    situation: "Using pure semantic search for keyword-heavy queries"
    why: |
      Some queries are keyword-oriented (looking for specific terms) while
      others are semantic (looking for concepts). Pure semantic search fails
      on exact matches; pure keyword search fails on paraphrases.
    solution: |
      Implement hybrid search:
      - BM25/TF-IDF for keyword matching
      - Vector similarity for semantic matching
      - Reciprocal Rank Fusion to combine
      - Tune weights based on query patterns
    symptoms:
      - "Exact term searches miss results"
      - "Concept searches too literal"
      - "Users frustrated with both"
    detection_pattern:
      language: generic
      pattern: "similarity_search\\(|vector_search\\((?!.*bm25|.*keyword)"
