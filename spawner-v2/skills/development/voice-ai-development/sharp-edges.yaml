# Voice AI Development Sharp Edges

sharp_edges:
  - id: openai-realtime-vad-tuning
    summary: OpenAI Realtime VAD cuts off speech
    severity: high
    situation: Agent responds before user finishes speaking
    why: |
      Default silence_duration_ms too short.
      VAD threshold too sensitive.
      Users pause while thinking.
    solution: |
      # Configure turn detection carefully
      await ws.send(json.dumps({
          "type": "session.update",
          "session": {
              "turn_detection": {
                  "type": "server_vad",
                  "threshold": 0.5,  # Higher = less sensitive
                  "prefix_padding_ms": 300,  # Audio before speech
                  "silence_duration_ms": 800  # Wait longer before responding
              }
          }
      }))

      # For thinking users, increase silence_duration_ms to 1000-1500

      # Alternative: Manual turn detection
      await ws.send(json.dumps({
          "type": "session.update",
          "session": {
              "turn_detection": None  # Disable automatic
          }
      }))

      # Then manually commit when user done
      await ws.send(json.dumps({
          "type": "input_audio_buffer.commit"
      }))
    symptoms:
      - Agent interrupts user
      - Responses before sentence complete
      - "Let me finish" user frustration
    detection_pattern: 'turn_detection|silence_duration'

  - id: audio-format-mismatch
    summary: Wrong audio format causes noise/silence
    severity: high
    situation: Static, noise, or silent audio
    why: |
      Sample rate mismatch.
      Wrong encoding (PCM vs MP3).
      Mono/stereo confusion.
    solution: |
      # OpenAI Realtime: PCM16, 24kHz, mono
      audio_config = {
          "input_audio_format": "pcm16",
          "output_audio_format": "pcm16"
          # Sample rate: 24000 Hz
          # Channels: 1 (mono)
          # Bit depth: 16
      }

      # Deepgram: Linear16, 16kHz
      deepgram_config = {
          "encoding": "linear16",
          "sample_rate": 16000
      }

      # ElevenLabs: Multiple options
      elevenlabs_formats = [
          "mp3_44100_128",  # MP3, 44.1kHz, 128kbps
          "pcm_16000",      # PCM, 16kHz (lower latency)
          "pcm_24000",      # PCM, 24kHz (better quality)
      ]

      # Convert between formats
      import audioop

      def resample(audio_bytes, from_rate, to_rate):
          return audioop.ratecv(
              audio_bytes, 2, 1,  # 16-bit, mono
              from_rate, to_rate, None
          )[0]

      # Always verify format matches
      def validate_audio(audio_bytes, expected_rate):
          # Check length matches expected duration
          expected_samples = expected_rate * duration_seconds
          actual_samples = len(audio_bytes) // 2  # 16-bit = 2 bytes
          if abs(actual_samples - expected_samples) > 100:
              raise ValueError("Sample rate mismatch")
    symptoms:
      - Static or white noise
      - Audio too fast or slow
      - Silent output
      - Robotic distortion
    detection_pattern: 'sample_rate|pcm|encoding'

  - id: websocket-connection-drops
    summary: WebSocket disconnects during call
    severity: high
    situation: Call cuts out, audio stops
    why: |
      Network hiccup.
      Idle timeout.
      Server scaling.
    solution: |
      import asyncio
      import websockets
      from websockets.exceptions import ConnectionClosed

      class RobustConnection:
          def __init__(self, url, headers):
              self.url = url
              self.headers = headers
              self.ws = None
              self.reconnect_delay = 1

          async def connect(self):
              while True:
                  try:
                      self.ws = await websockets.connect(
                          self.url,
                          extra_headers=self.headers,
                          ping_interval=20,  # Keep alive
                          ping_timeout=10
                      )
                      self.reconnect_delay = 1  # Reset on success
                      return
                  except Exception as e:
                      print(f"Connection failed: {e}")
                      await asyncio.sleep(self.reconnect_delay)
                      self.reconnect_delay = min(30, self.reconnect_delay * 2)

          async def send(self, message):
              try:
                  await self.ws.send(message)
              except ConnectionClosed:
                  await self.connect()
                  await self.ws.send(message)

          async def receive(self):
              try:
                  return await self.ws.recv()
              except ConnectionClosed:
                  await self.connect()
                  # Notify caller to resync state
                  raise ReconnectException()

      # For LiveKit - built-in reconnection
      room = rtc.Room()
      room.on("reconnecting", lambda: print("Reconnecting..."))
      room.on("reconnected", lambda: print("Reconnected!"))
    symptoms:
      - Audio cuts out
      - "Connection lost" errors
      - Partial responses
    detection_pattern: 'websocket|ConnectionClosed'

  - id: tts-latency-buffer
    summary: TTS waiting for too much text
    severity: medium
    situation: Long pause before agent speaks
    why: |
      Buffering too many tokens.
      Waiting for sentence end.
      Not streaming properly.
    solution: |
      # WRONG - wait for full response
      full_response = await llm.generate(prompt)
      audio = await tts.synthesize(full_response)  # SLOW!

      # CORRECT - stream with small buffer
      text_buffer = ""
      CHUNK_SIZE = 50  # Characters

      async for token in llm.stream(prompt):
          text_buffer += token

          # Send to TTS when buffer is full OR sentence ends
          if len(text_buffer) >= CHUNK_SIZE or token in ".!?,":
              audio_stream = tts.synthesize_stream(text_buffer)
              async for audio in audio_stream:
                  yield audio
              text_buffer = ""

      # Flush remaining text
      if text_buffer:
          async for audio in tts.synthesize_stream(text_buffer):
              yield audio

      # ElevenLabs specific - use websocket for lowest latency
      async with eleven.text_to_speech.websocket(voice_id) as ws:
          async for token in llm.stream(prompt):
              audio = await ws.send(token)
              yield audio
          final = await ws.flush()
          yield final

      # Latency budget:
      # - STT: 100-300ms
      # - LLM first token: 200-500ms
      # - TTS first audio: 100-300ms
      # Total: 400-1100ms (aim for <800ms)
    symptoms:
      - Long pause before response
      - Unnatural speech rhythm
      - Users think agent is slow
    detection_pattern: 'synthesize|tts.*stream'

  - id: interruption-handling
    summary: Agent doesn't stop when interrupted
    severity: medium
    situation: Agent talks over user
    why: |
      No barge-in detection.
      Audio queue not cleared.
      TTS continues generating.
    solution: |
      class InterruptibleAgent:
          def __init__(self):
              self.is_speaking = False
              self.audio_queue = asyncio.Queue()
              self.cancel_speech = asyncio.Event()

          async def speak(self, text: str):
              self.is_speaking = True
              self.cancel_speech.clear()

              try:
                  async for audio in self.tts.stream(text):
                      if self.cancel_speech.is_set():
                          break  # Stop generating
                      await self.audio_queue.put(audio)
              finally:
                  self.is_speaking = False

          async def handle_user_audio(self, audio: bytes):
              # Detect if user is speaking
              if self.vad.is_speech(audio):
                  if self.is_speaking:
                      # User interrupted!
                      await self.stop_speaking()

          async def stop_speaking(self):
              self.cancel_speech.set()

              # Clear audio queue
              while not self.audio_queue.empty():
                  try:
                      self.audio_queue.get_nowait()
                  except asyncio.QueueEmpty:
                      break

              # OpenAI Realtime: Cancel response
              await self.ws.send(json.dumps({
                  "type": "response.cancel"
              }))

              # Clear input buffer too
              await self.ws.send(json.dumps({
                  "type": "input_audio_buffer.clear"
              }))
    symptoms:
      - Agent talks over user
      - Can't interrupt agent
      - Conversation feels robotic
    detection_pattern: 'interrupt|barge|cancel'

  - id: elevenlabs-voice-cloning-quality
    summary: Cloned voice sounds robotic
    severity: low
    situation: Custom voice doesn't sound natural
    why: |
      Poor quality source audio.
      Not enough samples.
      Wrong model selection.
    solution: |
      # Voice cloning best practices:

      # 1. Source audio requirements
      requirements = {
          "duration": "1-5 minutes",  # More is better
          "quality": "Studio or quiet room",
          "content": "Natural speech, varied sentences",
          "format": "WAV or MP3, 44.1kHz preferred"
      }

      # 2. Use professional voice cloning
      voice = eleven.voices.add(
          name="Custom Voice",
          files=["sample1.wav", "sample2.wav"],
          labels={"accent": "american", "age": "middle_aged"}
      )

      # 3. Use appropriate model
      models = {
          "eleven_multilingual_v2": "Best quality, multilingual",
          "eleven_turbo_v2_5": "Faster, English-optimized",
          "eleven_english_v1": "Legacy, avoid"
      }

      # 4. Tune stability and similarity
      audio = eleven.text_to_speech.convert(
          voice_id=voice.voice_id,
          text="Hello world",
          voice_settings={
              "stability": 0.5,  # Higher = more consistent
              "similarity_boost": 0.75,  # Higher = closer to original
              "style": 0.0,  # 0 for cloned voices
              "use_speaker_boost": True
          }
      )

      # 5. For best results, use Instant Voice Clone + fine-tuning
    symptoms:
      - Robotic or flat voice
      - Inconsistent pronunciation
      - Strange artifacts
    detection_pattern: 'voice.*clone|add.*voice'
