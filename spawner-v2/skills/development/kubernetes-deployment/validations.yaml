# Kubernetes Deployment Validations
# Automated checks to catch K8s manifest mistakes before they reach production

validations:
  - id: k8s-no-resource-limits
    name: Container Without Resource Limits
    severity: error
    type: regex
    pattern:
      - 'containers:\s*\n\s*-\s*name:[^\n]*(?:\n(?!.*limits:)[^\n]*)*(?=\n\s*-\s*name:|\n[a-zA-Z]|\Z)'
      - 'kind:\s*Deployment(?:(?!limits:).)*$'
    message: "Container without resource limits. Can consume entire node resources and cause OOMKilled cascade."
    fix_action: "Add resources.limits.memory and resources.limits.cpu to all containers"
    applies_to:
      - "*.yaml"
      - "*.yml"
      - "*deployment*.yaml"

  - id: k8s-no-resource-requests
    name: Container Without Resource Requests
    severity: warning
    type: regex
    pattern:
      - 'containers:\s*\n\s*-\s*name:[^\n]*(?:\n(?!.*requests:)[^\n]*)*(?=\n\s*-\s*name:|\n[a-zA-Z]|\Z)'
    message: "Container without resource requests. Scheduler can't make optimal placement decisions."
    fix_action: "Add resources.requests.memory and resources.requests.cpu"
    applies_to:
      - "*.yaml"
      - "*.yml"

  - id: k8s-latest-tag
    name: Image Using Latest Tag
    severity: error
    type: regex
    pattern:
      - 'image:\s*[^:\s]+:latest'
      - 'image:\s*[^:\s]+\s*$'
      - 'image:\s*["''][^:]+["'']'
    message: "Image using :latest tag or no tag. Deployments become non-deterministic and rollbacks don't work."
    fix_action: "Pin to specific version tag (e.g., v1.2.3) or use image digest"
    applies_to:
      - "*.yaml"
      - "*.yml"

  - id: k8s-no-liveness-probe
    name: Container Without Liveness Probe
    severity: warning
    type: regex
    pattern:
      - 'kind:\s*Deployment[\s\S]*containers:[\s\S]*?(?!livenessProbe)(?=\n\s{4,}[a-zA-Z]|\nkind:|\Z)'
    message: "Container without liveness probe. Kubernetes can't detect if app is stuck and needs restart."
    fix_action: "Add livenessProbe with appropriate httpGet, tcpSocket, or exec check"
    applies_to:
      - "*deployment*.yaml"
      - "*.yaml"

  - id: k8s-no-readiness-probe
    name: Container Without Readiness Probe
    severity: warning
    type: regex
    pattern:
      - 'kind:\s*Deployment[\s\S]*containers:[\s\S]*?(?!readinessProbe)(?=\n\s{4,}[a-zA-Z]|\nkind:|\Z)'
    message: "Container without readiness probe. Traffic routed to pods that aren't ready to serve."
    fix_action: "Add readinessProbe to control when pod receives traffic"
    applies_to:
      - "*deployment*.yaml"
      - "*.yaml"

  - id: k8s-running-as-root
    name: Container Running as Root
    severity: error
    type: regex
    pattern:
      - 'runAsNonRoot:\s*false'
      - 'runAsUser:\s*0'
      - 'privileged:\s*true'
    message: "Container running as root or privileged. Container escape vulnerabilities become full node compromise."
    fix_action: "Add securityContext with runAsNonRoot: true and runAsUser: non-zero UID"
    applies_to:
      - "*.yaml"
      - "*.yml"

  - id: k8s-no-security-context
    name: Container Without Security Context
    severity: warning
    type: regex
    pattern:
      - 'containers:\s*\n\s*-\s*name:[^\n]*(?:\n(?!.*securityContext:)[^\n]*)*(?=\n\s*-\s*name:|\n[a-zA-Z]|\Z)'
    message: "Container without securityContext. Running with default (often root) permissions."
    fix_action: "Add securityContext with runAsNonRoot, readOnlyRootFilesystem, allowPrivilegeEscalation: false"
    applies_to:
      - "*.yaml"
      - "*.yml"

  - id: k8s-naked-pod
    name: Naked Pod Without Controller
    severity: error
    type: regex
    pattern:
      - 'kind:\s*Pod\s*\n(?!.*ownerReferences)'
    message: "Naked Pod detected. Won't be rescheduled on node failure, no rolling updates or rollback."
    fix_action: "Use Deployment for stateless, StatefulSet for stateful, DaemonSet for node-level workloads"
    applies_to:
      - "*.yaml"
      - "*.yml"
      - "pod*.yaml"

  - id: k8s-hardcoded-replicas-with-hpa
    name: Hardcoded Replicas With HPA
    severity: warning
    type: regex
    pattern:
      - 'replicas:\s*\d+[\s\S]*HorizontalPodAutoscaler'
      - 'HorizontalPodAutoscaler[\s\S]*replicas:\s*\d+'
    message: "Deployment has hardcoded replicas with HPA. Helm upgrade resets replicas to hardcoded value, fighting HPA."
    fix_action: "Omit replicas from Deployment when using HPA, or use helm lookup function"
    applies_to:
      - "*.yaml"
      - "*.yml"

  - id: k8s-secret-in-configmap
    name: Secret Data in ConfigMap
    severity: error
    type: regex
    pattern:
      - 'kind:\s*ConfigMap[\s\S]*(?:password|secret|api[_-]?key|token|credential):'
      - 'ConfigMap[\s\S]*DATABASE_URL:'
      - 'ConfigMap[\s\S]*AWS_SECRET'
    message: "Sensitive data in ConfigMap. ConfigMaps aren't encrypted at rest and have weaker RBAC."
    fix_action: "Use Secret for sensitive data. Enable encryption at rest for Secrets."
    applies_to:
      - "*.yaml"
      - "*.yml"
      - "*configmap*.yaml"

  - id: k8s-no-pod-disruption-budget
    name: Production Deployment Without PDB
    severity: info
    type: regex
    pattern:
      - 'kind:\s*Deployment[\s\S]*replicas:\s*[3-9]|replicas:\s*\d{2,}(?![\s\S]*PodDisruptionBudget)'
    message: "Production deployment without PodDisruptionBudget. Node drains could take down all replicas."
    fix_action: "Add PodDisruptionBudget with minAvailable or maxUnavailable"
    applies_to:
      - "*.yaml"
      - "*.yml"

  - id: k8s-selector-mismatch
    name: Potential Selector Mismatch
    severity: warning
    type: regex
    pattern:
      - 'selector:\s*\n\s*matchLabels:[\s\S]*?app:\s*(\S+)[\s\S]*?template:[\s\S]*?labels:[\s\S]*?app:\s*(?!\1)'
    message: "Potential label selector mismatch. Service selector must exactly match pod labels."
    fix_action: "Ensure selector.matchLabels matches spec.template.metadata.labels exactly"
    applies_to:
      - "*.yaml"
      - "*.yml"

  - id: k8s-no-ingress-class
    name: Ingress Without IngressClass
    severity: warning
    type: regex
    pattern:
      - 'kind:\s*Ingress[\s\S]*?spec:(?![\s\S]*?ingressClassName)'
    message: "Ingress without ingressClassName. In multi-controller clusters, wrong controller may handle it."
    fix_action: "Add spec.ingressClassName to specify which controller handles this Ingress"
    applies_to:
      - "*.yaml"
      - "*.yml"
      - "*ingress*.yaml"

  - id: k8s-hostpath-volume
    name: HostPath Volume Mount
    severity: warning
    type: regex
    pattern:
      - 'hostPath:\s*\n\s*path:'
    message: "HostPath volume mount detected. Breaks pod portability and can be security risk."
    fix_action: "Use PersistentVolumeClaim or cloud storage instead of hostPath"
    applies_to:
      - "*.yaml"
      - "*.yml"

  - id: k8s-privileged-port
    name: Privileged Port Binding
    severity: warning
    type: regex
    pattern:
      - 'containerPort:\s*([1-9]|[1-9][0-9]|[1-9][0-9][0-9]|10[0-1][0-9]|102[0-3])\s*$'
      - 'hostPort:\s*([1-9]|[1-9][0-9]|[1-9][0-9][0-9]|10[0-1][0-9]|102[0-3])'
    message: "Container binding to privileged port (<1024). Requires root or special capabilities."
    fix_action: "Use port > 1024 in container, let Service/Ingress handle external port mapping"
    applies_to:
      - "*.yaml"
      - "*.yml"

  - id: k8s-aggressive-probe
    name: Aggressive Liveness Probe Settings
    severity: warning
    type: regex
    pattern:
      - 'livenessProbe:[\s\S]*?failureThreshold:\s*1'
      - 'livenessProbe:[\s\S]*?timeoutSeconds:\s*1'
      - 'livenessProbe:[\s\S]*?periodSeconds:\s*[1-3]\s'
    message: "Aggressive liveness probe settings. Under load, healthy pods may be killed unnecessarily."
    fix_action: "Use failureThreshold >= 3, timeoutSeconds >= 3, periodSeconds >= 5 for liveness probes"
    applies_to:
      - "*.yaml"
      - "*.yml"

  - id: k8s-empty-dir-prod
    name: EmptyDir for Persistent Data
    severity: warning
    type: regex
    pattern:
      - 'emptyDir:\s*\{\}[\s\S]*?mountPath:\s*/data'
      - 'emptyDir:[\s\S]*?(database|storage|uploads)'
    message: "EmptyDir used for data that should persist. Data lost when pod is rescheduled."
    fix_action: "Use PersistentVolumeClaim for data that must survive pod restarts"
    applies_to:
      - "*.yaml"
      - "*.yml"

