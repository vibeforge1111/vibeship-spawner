# Realtime Engineer Sharp Edges
# Battle-tested gotchas from building real-time systems at scale

sharp_edges:
  - id: websocket-thundering-herd
    summary: All clients reconnect simultaneously after server restart
    severity: critical
    situation: |
      You deploy a new version or your server restarts. Suddenly all your
      WebSocket connections drop. Every client reconnects at exactly the same
      moment. The server gets hammered and crashes again.
    why: |
      Without jitter, all clients use the same reconnection timing. 10,000 clients
      all hitting your server in the same second will crash it. The restart becomes
      a cascading failure loop.
    solution: |
      class ReconnectingWebSocket {
        private attempt = 0;

        getReconnectDelay(): number {
          // Base: exponential backoff (1s, 2s, 4s, 8s...)
          const base = Math.min(30000, 1000 * Math.pow(2, this.attempt));

          // Jitter: add 0-30% randomness to spread out reconnections
          const jitter = base * 0.3 * Math.random();

          return base + jitter;
        }

        async reconnect() {
          const delay = this.getReconnectDelay();
          console.log(`Reconnecting in ${delay}ms`);
          await sleep(delay);

          try {
            await this.connect();
            this.attempt = 0;  // Reset on success
          } catch {
            this.attempt++;
            this.reconnect();  // Try again with longer delay
          }
        }
      }
    symptoms:
      - Server crashes immediately after restart
      - CPU spikes to 100% on reconnection
      - Connection refused errors flood logs
      - Restart takes multiple attempts to succeed
    detection_pattern: 'reconnect.*setTimeout.*(?!.*random|jitter|Math\.random)'
    version_range: ">=1.0.0"

  - id: half-open-connection-ghost
    summary: Connection appears open but messages are not flowing
    severity: critical
    situation: |
      Your WebSocket shows as connected. Server shows the client as online.
      But messages are not being delivered. Users complain they are not receiving
      updates, but your logs show the connection is healthy.
    why: |
      TCP connections can become half-open - one side thinks it is connected,
      the other does not. Network issues, NAT timeouts, mobile network switches
      can all cause this. Without application-level heartbeat, you will not know
      until TCP timeout (can be 15+ minutes).
    solution: |
      // Client-side heartbeat
      class HeartbeatWebSocket {
        private pingInterval: NodeJS.Timer;
        private pongTimeout: NodeJS.Timer;
        private missedPongs = 0;

        startHeartbeat() {
          // Ping every 30 seconds
          this.pingInterval = setInterval(() => {
            if (this.missedPongs >= 2) {
              console.log('Connection dead - forcing reconnect');
              this.ws.close();
              this.reconnect();
              return;
            }

            this.ws.send(JSON.stringify({ type: 'ping', ts: Date.now() }));
            this.missedPongs++;

            // Expect pong within 10 seconds
            this.pongTimeout = setTimeout(() => {
              console.log('Pong timeout - connection may be dead');
            }, 10000);
          }, 30000);
        }

        handleMessage(msg: any) {
          if (msg.type === 'pong') {
            clearTimeout(this.pongTimeout);
            this.missedPongs = 0;
          }
        }
      }

      // Server-side: also implement ping and track last pong time
    symptoms:
      - Users report missing messages despite "connected" status
      - Some clients show online but never receive updates
      - Messages queue up on server, never delivered
      - Problem more common on mobile or flaky networks
    detection_pattern: 'WebSocket.*(?!.*ping|heartbeat|pong)'
    version_range: ">=1.0.0"

  - id: presence-flickering
    summary: Users appear to join and leave rapidly during network blips
    severity: high
    situation: |
      Your presence indicator shows users constantly going online/offline.
      The "John is typing" indicator flickers. Notifications spam users with
      join/leave messages. Users complain about the distracting experience.
    why: |
      Mobile networks drop connections during handoffs. WiFi to cellular
      transitions cause momentary disconnects. Without a grace period, every
      blip triggers a leave event followed immediately by a join event.
    solution: |
      class PresenceWithGracePeriod {
        private users = new Map<string, { status: string; leaveTimer?: NodeJS.Timer }>();
        private GRACE_PERIOD = 5000;  // 5 seconds

        handleDisconnect(userId: string) {
          const user = this.users.get(userId);
          if (!user) return;

          // Start grace period - do not broadcast leave yet
          user.leaveTimer = setTimeout(() => {
            // Only now broadcast leave
            this.users.delete(userId);
            this.broadcast({ type: 'user_left', userId });
          }, this.GRACE_PERIOD);
        }

        handleReconnect(userId: string) {
          const user = this.users.get(userId);
          if (user?.leaveTimer) {
            // Cancel the leave - user reconnected within grace period
            clearTimeout(user.leaveTimer);
            user.leaveTimer = undefined;
            // No events broadcast - user never "left"
          }
        }
      }

      // Configurable grace periods:
      // - Typing indicator: 2-3 seconds
      // - Presence: 5-10 seconds
      // - Document collaboration: 10-30 seconds
    symptoms:
      - Users flicker online/offline rapidly
      - Join/leave notifications spam the channel
      - Typing indicator blinks constantly
      - UX feels broken and unstable
    detection_pattern: 'disconnect.*broadcast.*leave(?!.*timeout|grace|delay)'
    version_range: ">=1.0.0"

  - id: message-ordering-race
    summary: Messages arrive out of order causing state corruption
    severity: high
    situation: |
      User A sends message 1, then message 2. User B receives message 2 first,
      then message 1. Or two users edit the same field and one edit disappears.
      State becomes inconsistent across clients.
    why: |
      Network packets can take different routes with different latencies. TCP
      guarantees order for a single connection, but not across reconnections
      or between clients. Without explicit ordering, last-write-wins causes
      lost updates.
    solution: |
      // Option 1: Sequence numbers for single client ordering
      class SequencedChannel {
        private seq = 0;
        private received = new Set<number>();
        private pending: Map<number, Message> = new Map();

        send(data: any) {
          this.ws.send(JSON.stringify({ seq: this.seq++, data }));
        }

        receive(msg: { seq: number; data: any }) {
          if (this.received.has(msg.seq)) return;  // Duplicate

          this.pending.set(msg.seq, msg);
          this.deliverInOrder();
        }

        private deliverInOrder() {
          let next = Math.min(...this.pending.keys());
          while (this.pending.has(next)) {
            const msg = this.pending.get(next)!;
            this.pending.delete(next);
            this.received.add(next);
            this.onMessage(msg.data);
            next++;
          }
        }
      }

      // Option 2: For collaborative editing, use CRDT or OT
      // CRDTs (like Yjs) handle concurrent edits automatically
      import * as Y from 'yjs';
      const doc = new Y.Doc();
      const text = doc.getText('content');

      // All operations commute - order does not matter
      text.insert(0, 'hello');
    symptoms:
      - Messages appear in wrong order
      - Edits get lost or overwritten
      - State differs between clients
      - Undo/redo behaves unexpectedly
    detection_pattern: 'send.*message(?!.*seq|order|clock|crdt)'
    version_range: ">=1.0.0"

  - id: backpressure-oom
    summary: Slow client causes server to run out of memory
    severity: high
    situation: |
      One client on a slow connection cannot keep up with messages. Your server
      queues messages for that client. Memory usage grows. Eventually the server
      crashes or kills the connection unexpectedly.
    why: |
      Without backpressure handling, the server buffers indefinitely. A client
      on 2G mobile or with a paused tab can fall arbitrarily behind. The queue
      grows until OOM. Worse, one bad client can take down the whole server.
    solution: |
      class BackpressureWebSocket {
        private queue: Message[] = [];
        private maxQueueSize = 100;
        private sending = false;

        async send(msg: Message) {
          if (this.queue.length >= this.maxQueueSize) {
            // Strategy 1: Drop oldest (for live data like positions)
            this.queue.shift();
            console.warn('Queue full, dropping oldest message');

            // Strategy 2: Drop incoming (for important events)
            // throw new Error('Client too slow, dropping message');

            // Strategy 3: Disconnect slow client
            // this.ws.close(4000, 'Too slow');
          }

          this.queue.push(msg);
          this.flush();
        }

        private async flush() {
          if (this.sending) return;
          this.sending = true;

          while (this.queue.length > 0) {
            const msg = this.queue.shift()!;
            try {
              await this.ws.send(JSON.stringify(msg));
            } catch (e) {
              // Connection closed, stop sending
              this.queue = [];
              break;
            }
          }

          this.sending = false;
        }
      }

      // Also monitor queue sizes in metrics
    symptoms:
      - Server memory grows unbounded
      - OOM crashes after running for hours
      - Slow clients cause instability for everyone
      - Memory usage correlates with client count
    detection_pattern: 'queue\.push(?!.*length|size|max|limit)'
    version_range: ">=1.0.0"

  - id: sticky-session-bypass
    summary: Load balancer sends WebSocket upgrade to wrong server
    severity: high
    situation: |
      You scale to multiple WebSocket servers behind a load balancer. Initial
      HTTP request goes to server A, but the WebSocket upgrade goes to server B.
      Connection fails or user state is on wrong server.
    why: |
      Standard round-robin load balancing does not know about WebSocket.
      The upgrade request is just another HTTP request. If it goes to a different
      server than the one that has the user state, things break.
    solution: |
      // Option 1: Cookie-based sticky sessions (NGINX)
      upstream websocket {
          ip_hash;  // Or use sticky cookie
          server ws1.example.com;
          server ws2.example.com;
      }

      server {
          location /ws {
              proxy_pass http://websocket;
              proxy_http_version 1.1;
              proxy_set_header Upgrade $http_upgrade;
              proxy_set_header Connection "upgrade";
          }
      }

      // Option 2: Centralized state (Redis)
      // Store all state in Redis, any server can handle any client
      class StatelessWebSocketServer {
        private redis: Redis;

        async handleConnection(ws: WebSocket, userId: string) {
          // Subscribe to user channel
          const sub = this.redis.subscribe(`user:${userId}`);
          sub.on('message', (msg) => ws.send(msg));

          // Publish user events to Redis
          ws.on('message', (msg) => {
            this.redis.publish(`room:${roomId}`, msg);
          });
        }
      }

      // Option 3: Consistent hashing
      // Hash user ID to determine which server handles them
    symptoms:
      - WebSocket connections fail after HTTP works
      - State disappears after reconnection
      - Works with one server, fails with multiple
      - Random connection failures under load
    detection_pattern: null
    version_range: ">=1.0.0"

  - id: sse-retry-flood
    summary: SSE auto-reconnect floods server on error
    severity: medium
    situation: |
      Your SSE endpoint returns an error. The browser automatically reconnects.
      You see thousands of requests per second. Server is overwhelmed. Error rate
      goes to 100%.
    why: |
      EventSource has built-in auto-reconnect that is very aggressive by default.
      If your endpoint returns an error (5xx), the browser retries immediately
      and continuously. No backoff. This can DDoS your own server.
    solution: |
      // Server: Set retry interval in the SSE stream
      app.get('/events', (req, res) => {
        res.setHeader('Content-Type', 'text/event-stream');
        res.setHeader('Cache-Control', 'no-cache');

        // Tell browser to wait 10 seconds before reconnecting
        res.write('retry: 10000\n\n');

        // On error, close gracefully with a message
        if (errorCondition) {
          res.write('event: error\n');
          res.write('data: {"message": "Service temporarily unavailable"}\n\n');
          res.end();  // Close cleanly, browser will respect retry
          return;
        }

        // Normal event streaming...
      });

      // Client: Add error handling with manual backoff
      let retryCount = 0;

      function connect() {
        const es = new EventSource('/events');

        es.onerror = () => {
          es.close();
          const delay = Math.min(30000, 1000 * Math.pow(2, retryCount));
          retryCount++;
          setTimeout(connect, delay);
        };

        es.onopen = () => {
          retryCount = 0;  // Reset on successful connection
        };
      }
    symptoms:
      - Massive spike in SSE endpoint requests
      - 5xx errors cascade into more requests
      - Server CPU maxed handling reconnections
      - Looks like a DDoS attack in logs
    detection_pattern: 'EventSource(?!.*error.*setTimeout|backoff)'
    version_range: ">=1.0.0"

  - id: websocket-proxy-timeout
    summary: Corporate proxies close idle WebSocket connections
    severity: medium
    situation: |
      Your WebSocket works great in development. In production, some corporate
      users report random disconnections every few minutes. No pattern. Works
      fine on mobile data.
    why: |
      Many corporate proxies and firewalls have idle connection timeouts.
      If no data flows for 30-60 seconds, they close the connection without
      notice. Your heartbeat interval is too long or not implemented.
    solution: |
      // Heartbeat every 25 seconds (under most proxy timeouts)
      const HEARTBEAT_INTERVAL = 25000;

      class ProxyFriendlyWebSocket {
        startHeartbeat() {
          setInterval(() => {
            if (this.ws.readyState === WebSocket.OPEN) {
              // Send minimal ping - just 1 byte is enough
              this.ws.send('p');
            }
          }, HEARTBEAT_INTERVAL);
        }
      }

      // Server NGINX config for longer timeouts
      location /ws {
          proxy_pass http://websocket;
          proxy_http_version 1.1;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection "upgrade";

          # Longer timeouts for WebSocket
          proxy_read_timeout 3600s;
          proxy_send_timeout 3600s;
      }

      // For very aggressive proxies, consider falling back to long-polling
    symptoms:
      - Disconnections at regular intervals (30s, 60s, 2min)
      - Only affects some users (corporate networks)
      - Works on mobile data, fails on office WiFi
      - No error messages, just clean disconnect
    detection_pattern: 'WebSocket.*(?!.*heartbeat.*25|ping.*25|interval.*25)'
    version_range: ">=1.0.0"

  - id: broadcast-amplification
    summary: One message triggers O(n) database queries or API calls
    severity: medium
    situation: |
      A user sends a message to a room. You broadcast it to 100 users. For each
      user, you check permissions, load preferences, format the message. CPU
      spikes. Latency goes through the roof.
    why: |
      Naive broadcast does N operations per message. With 100 users and 10
      messages/second, that is 1000 operations/second. Database queries, API
      calls, or heavy computation in the broadcast path kills performance.
    solution: |
      // Anti-pattern: O(n) work per broadcast
      async function broadcastBad(room: Room, message: Message) {
        for (const user of room.members) {
          const prefs = await db.getUserPrefs(user.id);  // N queries!
          if (await canReceive(user, message)) {  // N permission checks!
            const formatted = await format(message, prefs);  // N formats!
            user.send(formatted);
          }
        }
      }

      // Better: Cache, batch, precompute
      class EfficientBroadcaster {
        private prefsCache = new Map<string, UserPrefs>();
        private permissionCache = new Map<string, Set<string>>();

        async broadcast(room: Room, message: Message) {
          // Precompute once
          const formatted = this.format(message);

          // Batch send - no per-user work
          const recipients = room.members.filter(u =>
            this.permissionCache.get(room.id)?.has(u.id)
          );

          // Single batch operation
          await Promise.all(recipients.map(u => u.send(formatted)));
        }

        // Refresh caches periodically, not per-message
        async refreshCaches(roomId: string) {
          const perms = await db.getRoomPermissions(roomId);
          this.permissionCache.set(roomId, new Set(perms));
        }
      }
    symptoms:
      - Message latency grows with user count
      - CPU spikes on message broadcast
      - Database connections exhausted
      - Works with 10 users, dies with 1000
    detection_pattern: 'for.*member.*await.*db\.'
    version_range: ">=1.0.0"

  - id: reconnection-state-loss
    summary: Client loses state on reconnection and shows stale data
    severity: medium
    situation: |
      User disconnects and reconnects. They see old messages, miss updates
      that happened while offline, or the UI shows inconsistent state. Refresh
      fixes it but that is bad UX.
    why: |
      The server tracks state, but after disconnect the client has no way to
      know what it missed. Reconnection establishes a new stream but does not
      replay missed events.
    solution: |
      // Track last received event on client
      class StatefulClient {
        private lastEventId: string | null = null;

        connect() {
          // Send last known event ID
          const url = this.lastEventId
            ? `/ws?since=${this.lastEventId}`
            : '/ws';

          this.ws = new WebSocket(url);

          this.ws.onmessage = (event) => {
            const msg = JSON.parse(event.data);
            this.lastEventId = msg.id;  // Track for reconnection
            this.handleMessage(msg);
          };
        }
      }

      // Server: Replay missed events
      class StatefulServer {
        private eventLog = new EventLog();  // Bounded buffer

        handleConnection(ws: WebSocket, sinceId?: string) {
          if (sinceId) {
            // Replay missed events
            const missed = this.eventLog.getAfter(sinceId);
            for (const event of missed) {
              ws.send(JSON.stringify(event));
            }
          }

          // Then stream new events
          this.subscribe(ws);
        }
      }

      // Use Last-Event-ID header with SSE (automatic)
      const es = new EventSource('/events');
      // Browser automatically sends Last-Event-ID on reconnect
    symptoms:
      - Stale data after reconnection
      - Missing messages during offline period
      - Inconsistent state between clients
      - Need to refresh to fix
    detection_pattern: 'reconnect(?!.*lastEventId|since|resume)'
    version_range: ">=1.0.0"

  - id: room-join-race-condition
    summary: User joins room but misses messages sent during join
    severity: medium
    situation: |
      User clicks "Join Room". While the join is processing, other users send
      messages. User successfully joins but those messages are missing. They
      only see messages sent after their join completed.
    why: |
      The join operation is not atomic. Messages sent between "start join"
      and "join complete" fall into a gap. The user is not in the room yet,
      so they are not a broadcast target.
    solution: |
      // Anti-pattern: Non-atomic join
      async function joinRoomBad(userId: string, roomId: string) {
        await db.addMember(userId, roomId);  // Gap here!
        const sub = pubsub.subscribe(roomId);  // Messages before this are lost
        return sub;
      }

      // Better: Subscribe first, then backfill
      async function joinRoomGood(userId: string, roomId: string) {
        // 1. Subscribe FIRST (might get duplicates, that is OK)
        const sub = pubsub.subscribe(roomId);

        // 2. Get recent messages (overlap with subscription is fine)
        const recent = await db.getRecentMessages(roomId, { limit: 50 });

        // 3. Record membership
        await db.addMember(userId, roomId);

        // 4. Client deduplicates by message ID
        return { subscription: sub, backfill: recent };
      }

      // Client side
      function handleJoin(result) {
        const seen = new Set<string>();

        // First, render backfill
        for (const msg of result.backfill) {
          seen.add(msg.id);
          this.renderMessage(msg);
        }

        // Then handle subscription, skipping duplicates
        result.subscription.on('message', (msg) => {
          if (seen.has(msg.id)) return;  // Already have it
          seen.add(msg.id);
          this.renderMessage(msg);
        });
      }
    symptoms:
      - Missing messages right after joining
      - First few messages in a room are missing
      - Messages appear if you rejoin
      - Race condition - sometimes works, sometimes does not
    detection_pattern: 'join.*subscribe(?!.*backfill|recent|history)'
    version_range: ">=1.0.0"
