# Analytics Sharp Edges
# Critical mistakes that turn data into noise and insights into confusion

sharp_edges:
  - id: vanity-metric-trap
    summary: Tracking metrics that look good but don't drive decisions
    severity: critical
    situation: |
      Dashboard shows: 1M pageviews! 500K registered users! 100K downloads!
      Everyone celebrates. Reality: Active users 5K (0.5%), D7 retention 3%,
      Revenue declining, Core action done by 2%.
    why: |
      Vanity metrics make you feel good but don't tell you if you're winning.
      They hide real problems. They don't change behavior. Resources wasted,
      false confidence built.
    solution: |
      # Vanity vs actionable:
      Vanity: Total registered users
      Actionable: Weekly active users

      Vanity: Total pageviews
      Actionable: Pages per session, time on page

      Vanity: App downloads
      Actionable: Day 7 retention

      Vanity: Total revenue
      Actionable: Revenue per user, LTV/CAC

      # Ask "So what?":
      "Pageviews are up 50%"
      "So what? Did that increase conversions?"
      "Did that increase revenue?"

      # Metrics test:
      "If this metric changed, what would we do differently?"
      No answer = vanity metric.
    symptoms:
      - Celebrating vanity metrics
      - Can't connect metrics to decisions
      - Real problems hidden
      - Metrics not actionable
    detection_pattern: null

  - id: data-without-decision
    summary: Collecting data with no plan to use it
    severity: high
    situation: |
      "Let's track everything! We might need it."
      Event catalog: button_clicked (every button), page_viewed (every page),
      mouse_moved (why???), scroll_position (constantly).
      6 months later: 500GB of data, no insights, nobody uses it.
    why: |
      Data has costs: storage, processing, compliance. More data ≠ more insight.
      Unused data is technical debt. Decision paralysis from too much data.
    solution: |
      # Define questions first:
      "What are we trying to learn?"
      "What decision will this inform?"
      "Who will act on this data?"

      # Start with decisions:
      Decision: Should we invest in onboarding?
      Metric needed: Onboarding completion rate
      Events needed: step_started, step_completed

      # Use the 90-day rule:
      Metric not viewed in 90 days → deprecate
      Event not used in 90 days → stop tracking

      # Document metric owners:
      Every metric has an owner
      Owner reviews quarterly
      No owner = probably don't need it
    symptoms:
      - Tracking everything "just in case"
      - Data never analyzed
      - Storage costs growing
      - No connection between events and decisions
    detection_pattern: null

  - id: broken-funnel-definition
    summary: Funnel stages that don't match user journey
    severity: high
    situation: |
      Defined funnel: Visit → Sign Up → Subscribe → Purchase
      Measured conversion: 0.5%. Team: "Let's optimize signup!"
      Reality: Users visit, leave, come back, research, view pricing, leave,
      come back, sign up, use free tier, eventually purchase.
    why: |
      User journeys are messy. Linear funnels are simplifications.
      Wrong funnel = wrong optimization. You optimize the wrong step.
    solution: |
      # Map actual user journeys:
      - Session recordings
      - Path analysis
      - User interviews

      # Define stages by intent:
      Awareness: Visited any page
      Interest: Viewed pricing
      Evaluation: Started trial
      Purchase: Converted

      # Allow for messiness:
      "Completed signup in first 3 sessions"
      Not: "signup immediately after first visit"

      # Multiple funnels:
      - New user funnel
      - Return user funnel
      - Mobile vs desktop funnel

      # Funnel design:
      Start from outcome, work backwards
      Purchase ← What happened before?
    symptoms:
      - Funnel doesn't match reality
      - Optimization doesn't improve outcomes
      - Users don't follow expected path
      - Conversion rates misleading
    detection_pattern: null

  - id: improper-attribution
    summary: Giving credit to wrong touchpoints
    severity: high
    situation: |
      User journey: Sees Facebook ad → Googles brand → Clicks Google ad → Signs up.
      Attribution: "Google Ads gets 100% credit!"
      Decision: "Kill Facebook, double Google!"
      Result: New users drop 50%. Google was capturing demand, not creating it.
    why: |
      Users have many touchpoints. Last click doesn't tell the story.
      First click doesn't either. Wrong attribution = wasted budget,
      wrong optimizations.
    solution: |
      # Use multiple models:
      Last Click: Last touchpoint gets credit
      First Click: First touchpoint gets credit
      Linear: Equal credit to all
      Time Decay: Recent gets more credit
      Compare insights across models

      # Incrementality testing:
      Hold out groups from channel
      Measure true lift
      Not attribution, causation

      # Understand the journey:
      Early stage: Awareness (Facebook, content)
      Mid stage: Consideration (retargeting, email)
      Late stage: Conversion (search, direct)

      # Incrementality > attribution:
      "If we stop spending on X, what happens?"
      Run the test. Don't guess from models.
    symptoms:
      - Single attribution model used
      - Killing channels that drive awareness
      - Over-investing in last-touch
      - Surprised when changes backfire
    detection_pattern: null

  - id: sampling-confusion
    summary: Drawing conclusions from sampled data without understanding implications
    severity: high
    situation: |
      Report shows: "Mobile converts at 15%, Desktop at 8%"
      Decision: "Focus everything on mobile!"
      Reality: Data 10% sampled. Mobile: 50 users (5 converted). Desktop: 5000 users.
      Mobile sample too small. Actual mobile rate: unknown.
    why: |
      Sampled data loses precision. Small segments become noise.
      Statistical significance matters. Wrong conclusions from noisy data.
    solution: |
      # Know your sample rate:
      GA4: Often 10-20% sampled at scale
      Check sampling indicator

      # Understand confidence intervals:
      "15% conversion" means nothing
      "15% ± 8%" means a lot

      # Use unsampled for critical decisions:
      Export to BigQuery (unsampled)
      Pay for higher quotas
      Use warehouse directly

      # Segment carefully:
      Is segment large enough?
      At 10% sampling, need 10x users

      # Sampling math:
      1 out of 10 could easily be 0 or 2
      Range: 0% to 20%
      Not useful for decisions
    symptoms:
      - Small segment conclusions
      - Ignoring sample rate
      - No confidence intervals
      - Decisions on noisy data
    detection_pattern: null

  - id: survivor-bias
    summary: Only analyzing users who stayed, ignoring those who left
    severity: high
    situation: |
      Survey results: "95% satisfied!" "45 min average session!" "NPS +60!"
      Surveyed: Active users (survivors). Ignored: 80% who churned.
      Churned users had the real answers.
    why: |
      Survivors aren't representative. Churned users have the answers.
      Happy users are already won. You miss why people leave.
    solution: |
      # Track churned users:
      - Exit surveys
      - Churn analysis
      - Win-back campaigns (with learning)

      # Include all users in analysis:
      Not: "Active users spend X"
      But: "Of users who signed up, X% became active"

      # Cohort analysis:
      Track cohorts over time
      See who drops and when

      # Churn analysis questions:
      Day 1: 1000 users sign up
      Day 7: 300 still active (30% retention)
      Day 30: 100 still active (10% retention)
      Where did 700 go between Day 1-7? Why?
    symptoms:
      - Only surveying active users
      - High satisfaction but high churn
      - No exit research
      - Missing the churned majority
    detection_pattern: null

  - id: metric-conflict
    summary: Teams optimize conflicting metrics
    severity: high
    situation: |
      Growth team: Maximize signups. Product team: Maximize engagement.
      Finance team: Maximize revenue. Result: Tons of low-quality signups,
      users confused by onboarding, annoyed by upsells. Everyone hits their
      metric. Business struggles.
    why: |
      Metrics drive behavior. Conflicting metrics drive conflicting behavior.
      Local optimization ≠ global optimization. Teams work against each other.
    solution: |
      # North Star Metric:
      One metric everyone aligns to
      All team metrics ladder up

      North Star: Weekly active paid users
      Growth: Quality signups that convert
      Product: Engagement that leads to payment
      Finance: Revenue from retained users

      # Guardrail metrics:
      "Maximize X without hurting Y"
      Growth: Signups, guardrail: Day 7 retention
      Marketing: Leads, guardrail: Lead quality

      # Metric hierarchy:
      Company: Revenue + growth
      ↓
      North Star: Weekly paying users
      ↓
      Team metrics (laddering up)
    symptoms:
      - Teams pulling in different directions
      - Each team wins, business loses
      - No shared north star
      - Conflicting incentives
    detection_pattern: null

  - id: average-illusion
    summary: Relying on averages that hide distribution
    severity: high
    situation: |
      "Average session length: 5 minutes"
      Reality: 70% users: 30 seconds, 20% users: 5 minutes, 10% users: 45 minutes.
      Average meaningless. No user is "average."
    why: |
      Averages hide bimodal distributions and outliers. No user is "average."
      Wrong optimizations from misleading summaries.
    solution: |
      # Use distributions:
      Show histograms, not averages
      Understand the shape

      # Use percentiles:
      p50: Median
      p90: Most users
      p99: Worst case
      "p50 load time: 1s, p95: 5s" better than "average: 2s"

      # Segment users:
      Don't average across segments
      Power users: 45 min sessions
      Casual: 2 min sessions
      Churned: 30 sec sessions

      # Ask "who does this represent?":
      "Average revenue is $50"
      "Who actually pays $50?"
      Nobody? Then useless metric.
    symptoms:
      - Decisions based on averages
      - Distributions not examined
      - Segments hidden in aggregates
      - '"Average user" doesn''t exist'
    detection_pattern: null

  - id: recency-bias
    summary: Overweighting recent data, ignoring historical patterns
    severity: medium
    situation: |
      Monday: "Traffic down 20% week-over-week!" Emergency meeting.
      Tuesday: Traffic recovers. Was normal weekend dip. Initiatives wasted.
    why: |
      Recent data is noisy. Short-term changes often random. React to noise,
      miss trends. Seasonal blindness.
    solution: |
      # Compare same periods:
      Week-over-week: Same day last week
      Year-over-year: Same period last year
      Not: Yesterday vs two days ago

      # Use rolling averages:
      7-day rolling average
      28-day rolling average
      Smooths out noise

      # Document seasonality:
      "December is always +100%"
      "Sundays are always -30%"
      Expected, not news

      # Comparison framework:
      Today vs yesterday: Noise
      This week vs last week: Getting better
      This month vs last month: Trend emerging
      This quarter vs same quarter last year: Confirmed
    symptoms:
      - Reacting to daily fluctuations
      - No seasonality awareness
      - Emergency meetings from noise
      - Historical patterns ignored
    detection_pattern: null

  - id: implementation-drift
    summary: Tracking code breaks or changes without notice
    severity: high
    situation: |
      Q1: Tracking works great. Q2: Dev refactors checkout, breaks tracking.
      Q3: "Checkout conversion dropped 50%!" Emergency meeting.
      Q4: Someone notices tracking broke. Q3 data useless. Time wasted.
    why: |
      Tracking code is code. Code breaks. Nobody monitors analytics code.
      Decisions made on broken data.
    solution: |
      # Tracking as code:
      Track in code review
      Test tracking
      Include in QA

      # Monitoring:
      Alert if event volume drops
      Alert if key events missing
      Daily data quality checks

      # Data contracts:
      Define expected events
      Validate against contract
      Fail loudly on violations

      # Tracking tests:
      E2E tests include analytics
      "When user clicks X, event Y fires"

      # Regular audits:
      Quarterly tracking review
      Walk through key flows
      Verify events fire
    symptoms:
      - Tracking breaks silently
      - No monitoring on event volume
      - Data suddenly changes
      - Decisions on broken data
    detection_pattern: null

  - id: dashboard-graveyard
    summary: Building dashboards nobody uses
    severity: medium
    situation: |
      Year 1: Build 50 dashboards. Year 2: Build 30 more.
      Reality: 5 used regularly, 20 checked occasionally, 55 never opened.
      Cost: Maintenance, query costs, confusion.
    why: |
      Dashboards are products. Products need users. Most dashboards fail.
      Time wasted, data distrust, missed insights.
    solution: |
      # Define user and question:
      "Marketing team wants to know: Which channels drive signups?"
      Build for that.

      # Start simple:
      3-5 metrics per dashboard
      One key question answered
      Add more only when needed

      # Review and retire:
      Quarterly: Check dashboard usage
      Not viewed in 90 days? Archive.

      # Dashboards as products:
      Who's the user?
      What's the use case?
      How often will they use it?
      What action will they take?
    symptoms:
      - Dashboards never viewed
      - Multiple conflicting dashboards
      - "Which dashboard is right?"
      - Dashboard debt growing
    detection_pattern: null

  - id: privacy-oversight
    summary: Tracking personal data without proper consent or protection
    severity: critical
    situation: |
      "Let's track everything about the user!" Email in events, IP stored,
      location tracked, behavior profiled, no consent asked.
      Two years later: GDPR audit, $50M fines, data breach, trust destroyed.
    why: |
      Analytics must respect privacy. Laws require consent. Data minimization required.
      Breaches of analytics = breaches of trust. Legal liability.
    solution: |
      # Anonymize by default:
      // Bad
      track('purchase', { email: user.email })
      // Good
      track('purchase', { userId: hash(user.id) })

      # Consent before tracking:
      if (hasAnalyticsConsent()) {
        initializeTracking()
      }

      # Data minimization:
      Only collect what's needed
      Delete when no longer needed
      Purpose limitation

      # Compliance checklist:
      □ Consent before tracking
      □ No PII in events
      □ Retention limits set
      □ Deletion capability exists
      □ Privacy policy accurate
    symptoms:
      - PII in event properties
      - Tracking before consent
      - No retention policy
      - No deletion capability
    detection_pattern: 'email.*:|phone.*:|name.*:'
