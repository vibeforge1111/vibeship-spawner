id: reinforcement-learning-collaboration
skill: reinforcement-learning
version: 1.0.0

receives_from:
  - skill: transformer-architecture
    context: "Policy network architecture"
    receives:
      - "Transformer-based policy design"
      - "Attention mechanisms for state encoding"
    provides: "RL training objectives and updates"

  - skill: llm-fine-tuning
    context: "SFT phase before RLHF"
    receives:
      - "SFT-trained base model"
      - "Instruction-following capability"
    provides: "RLHF alignment training"

  - skill: distributed-training
    context: "Scaling RL training"
    receives:
      - "Multi-GPU setup"
      - "Experience parallelization"
    provides: "PPO/GRPO training workload"

delegation_triggers:
  - trigger: "transformer|attention|model.*architecture"
    delegate_to: transformer-architecture
    context: "Policy network design"

  - trigger: "sft|supervised.*fine.?tun|instruction.*tun"
    delegate_to: llm-fine-tuning
    context: "SFT before RLHF"

  - trigger: "multi.*gpu|distributed|scale.*training"
    delegate_to: distributed-training
    context: "Scaling RL training"

  - trigger: "quantiz|deploy|inference.*optim"
    delegate_to: model-optimization
    context: "Optimizing trained policy"

feedback_loops:
  receives_feedback_from:
    - skill: llm-fine-tuning
      signal: "SFT model quality"
      action: "Use as reference model for KL penalty"

    - skill: transformer-architecture
      signal: "Model capacity constraints"
      action: "Adjust policy network complexity"

  sends_feedback_to:
    - skill: llm-fine-tuning
      signal: "Alignment issues discovered"
      action: "Improve SFT data quality"

    - skill: model-optimization
      signal: "Final policy checkpoint"
      action: "Optimize for deployment"

common_combinations:
  - name: Full RLHF Pipeline
    skills:
      - llm-fine-tuning
      - reinforcement-learning
      - distributed-training
    workflow: |
      1. SFT on instruction data (llm-fine-tuning)
      2. Train reward model on preferences
      3. PPO training with KL penalty (distributed)
      4. Evaluate on held-out prompts
      5. Iterate on reward model if needed

  - name: Classic RL Training
    skills:
      - reinforcement-learning
      - neural-architecture-search
    workflow: |
      1. Define environment and reward
      2. Choose algorithm (PPO, SAC, DQN)
      3. Optionally search for architecture
      4. Train with proper hyperparameters
      5. Evaluate and deploy

  - name: Constitutional AI
    skills:
      - reinforcement-learning
      - llm-fine-tuning
    workflow: |
      1. Define constitution (principles)
      2. Generate self-critiques
      3. RLAIF instead of RLHF
      4. Iterate on principles

ecosystem:
  rlhf_frameworks:
    - "OpenRLHF - Scalable RLHF"
    - "TRL - HuggingFace RL library"
    - "DeepSpeed-Chat - Microsoft RLHF"

  classic_rl:
    - "Stable Baselines3 - Reliable implementations"
    - "CleanRL - Single-file, readable"
    - "RLlib - Ray-based, scalable"

  environments:
    - "Gymnasium - Standard API"
    - "PettingZoo - Multi-agent"
    - "MuJoCo - Physics simulation"

references:
  rlhf:
    - "InstructGPT: Training language models to follow instructions"
    - "Constitutional AI: Harmlessness from AI Feedback"
    - "GRPO: Group Relative Policy Optimization"

  algorithms:
    - "PPO: Proximal Policy Optimization"
    - "SAC: Soft Actor-Critic"
    - "Rainbow DQN"
