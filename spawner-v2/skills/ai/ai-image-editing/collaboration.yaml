# Collaboration - AI Image Editing
# How this skill works with other skills

version: 1.0.0
skill_id: ai-image-editing

prerequisites:
  required: []

  recommended:
    - skill: backend
      reason: "API integration patterns"
      what_to_know:
        - "REST API design"
        - "Async patterns"
        - "Error handling"

    - skill: file-handling
      reason: "Image upload and storage"
      what_to_know:
        - "Multipart uploads"
        - "Binary data handling"
        - "Cloud storage integration"

    - skill: llm-architect
      reason: "Prompt engineering for images"
      what_to_know:
        - "Prompt structure"
        - "Negative prompts"
        - "Style descriptors"

delegation_triggers:
  - trigger: "user needs video from images"
    delegate_to: text-to-video
    context: "Image-to-video animation"

  - trigger: "user needs 3D from images"
    delegate_to: 3d-generation
    context: "Image to 3D model"

  - trigger: "user needs batch processing"
    delegate_to: workflow-automation
    context: "Image processing pipelines"

  - trigger: "user needs local inference"
    delegate_to: on-device-ai
    context: "Run models locally"

  - trigger: "user needs content moderation"
    delegate_to: ai-safety-alignment
    context: "NSFW detection and filtering"

receives_context_from:
  - skill: backend
    receives:
      - "API patterns"
      - "Authentication"
      - "Error handling"

  - skill: file-handling
    receives:
      - "Upload handling"
      - "Storage patterns"
      - "CDN integration"

  - skill: llm-architect
    receives:
      - "Prompt patterns"
      - "Model selection"
      - "Cost optimization"

provides_context_to:
  - skill: text-to-video
    provides:
      - "Image generation for video input"
      - "Style consistency"
      - "Resolution requirements"

  - skill: frontend
    provides:
      - "Image display patterns"
      - "Loading states"
      - "Error states"

  - skill: e-commerce
    provides:
      - "Product image editing"
      - "Background removal"
      - "Batch processing"

escalation_paths:
  - situation: "Complex ComfyUI workflows"
    escalate_to: workflow-automation
    context: "Node-based image pipelines"

  - situation: "Performance optimization"
    escalate_to: gpu-optimization
    context: "VRAM management, batching"

  - situation: "Copyright/IP concerns"
    escalate_to: legal-specialist
    context: "Training data, licensing"

  - situation: "Deployment at scale"
    escalate_to: devops
    context: "GPU infrastructure, CDN"

workflow_integration:
  typical_sequence:
    1:
      step: "Set up API integration"
      skills: [ai-image-editing, backend]
      output: "API client configured"

    2:
      step: "Implement content moderation"
      skills: [ai-image-editing, ai-safety-alignment]
      output: "Safe generation pipeline"

    3:
      step: "Build editing UI"
      skills: [ai-image-editing, frontend]
      output: "Canvas with mask drawing"

    4:
      step: "Add file upload handling"
      skills: [ai-image-editing, file-handling]
      output: "Upload and storage working"

    5:
      step: "Implement rate limiting"
      skills: [ai-image-editing, backend]
      output: "Usage controls in place"

    6:
      step: "Add cost tracking"
      skills: [ai-image-editing, backend]
      output: "Budget management"

  decision_points:
    - question: "Which API provider to use?"
      guidance: |
        Replicate:
        - Best model variety (Flux, SDXL, etc.)
        - Pay-per-use pricing
        - Easy model switching
        - Good for prototyping and production

        Stability AI:
        - Enterprise-grade reliability
        - Search-and-replace (no mask needed)
        - AWS Bedrock integration
        - Good for enterprise

        Fal.ai:
        - Fast inference (queue-less)
        - Competitive pricing
        - Real-time applications
        - Good for speed-critical apps

        Local/Self-hosted:
        - Privacy requirements
        - High volume (cost savings)
        - Custom models
        - Requires GPU infrastructure

    - question: "Flux or SDXL for generation?"
      guidance: |
        Use Flux for:
        - Best quality output
        - Better text rendering
        - More prompt adherence
        - Higher cost tolerance

        Use SDXL for:
        - Good quality at lower cost
        - More ControlNet options
        - Faster generation
        - More community resources

        Use both:
        - Flux for final outputs
        - SDXL for iterations/testing

collaboration_patterns:
  with_nextjs:
    when: "Building image editing in Next.js"
    approach: |
      Server-side API integration:

      ```typescript
      // app/api/generate/route.ts
      import Replicate from 'replicate';

      const replicate = new Replicate({
        auth: process.env.REPLICATE_API_TOKEN,
      });

      export async function POST(req: Request) {
        const { prompt, image, mask } = await req.json();

        // Rate limiting (use your preferred library)
        const { success } = await rateLimit.check(req);
        if (!success) {
          return Response.json(
            { error: 'Rate limit exceeded' },
            { status: 429 }
          );
        }

        // Content moderation
        const isSafe = await moderatePrompt(prompt);
        if (!isSafe) {
          return Response.json(
            { error: 'Prompt blocked by content policy' },
            { status: 400 }
          );
        }

        try {
          const output = await replicate.run(
            'black-forest-labs/flux-fill-pro',
            {
              input: {
                image,
                mask,
                prompt,
              },
            }
          );

          return Response.json({ url: output });
        } catch (error) {
          console.error('Generation failed:', error);
          return Response.json(
            { error: 'Generation failed' },
            { status: 500 }
          );
        }
      }
      ```

      ```typescript
      // components/ImageEditor.tsx
      'use client';

      import { useState, useRef } from 'react';

      export function ImageEditor() {
        const [image, setImage] = useState<string | null>(null);
        const [mask, setMask] = useState<string | null>(null);
        const [generating, setGenerating] = useState(false);
        const canvasRef = useRef<HTMLCanvasElement>(null);

        const handleGenerate = async (prompt: string) => {
          if (!image || !mask) return;

          setGenerating(true);
          try {
            const res = await fetch('/api/generate', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({ prompt, image, mask }),
            });

            const data = await res.json();
            if (data.url) {
              setImage(data.url);
              setMask(null);
            }
          } finally {
            setGenerating(false);
          }
        };

        return (
          <div>
            <canvas ref={canvasRef} />
            {/* Mask drawing UI */}
            {generating && <div>Generating...</div>}
          </div>
        );
      }
      ```

  with_cloudflare:
    when: "Deploying on Cloudflare Workers"
    approach: |
      Edge-compatible integration:

      ```typescript
      // src/index.ts (Cloudflare Worker)
      import Replicate from 'replicate';

      export interface Env {
        REPLICATE_API_TOKEN: string;
        RATE_LIMIT: KVNamespace;
      }

      export default {
        async fetch(request: Request, env: Env): Promise<Response> {
          if (request.method !== 'POST') {
            return new Response('Method not allowed', { status: 405 });
          }

          const { prompt, image, mask } = await request.json();

          // Rate limit with KV
          const ip = request.headers.get('CF-Connecting-IP') ?? 'unknown';
          const count = parseInt(await env.RATE_LIMIT.get(ip) ?? '0');

          if (count > 10) {
            return Response.json(
              { error: 'Rate limit exceeded' },
              { status: 429 }
            );
          }

          await env.RATE_LIMIT.put(ip, String(count + 1), {
            expirationTtl: 60,
          });

          // Generate
          const replicate = new Replicate({
            auth: env.REPLICATE_API_TOKEN,
          });

          try {
            const prediction = await replicate.predictions.create({
              model: 'black-forest-labs/flux-fill-pro',
              input: { prompt, image, mask },
            });

            // Return prediction ID for polling
            return Response.json({
              id: prediction.id,
              status: prediction.status,
            });
          } catch (error) {
            return Response.json(
              { error: 'Generation failed' },
              { status: 500 }
            );
          }
        },
      };
      ```

  with_s3:
    when: "Storing generated images in S3"
    approach: |
      Upload results to S3:

      ```typescript
      import { S3Client, PutObjectCommand } from '@aws-sdk/client-s3';
      import { v4 as uuid } from 'uuid';

      const s3 = new S3Client({ region: process.env.AWS_REGION });

      async function saveGeneratedImage(
        imageUrl: string,
        userId: string
      ): Promise<string> {
        // Download from generation API
        const response = await fetch(imageUrl);
        const buffer = await response.arrayBuffer();

        const key = `generated/${userId}/${uuid()}.png`;

        await s3.send(new PutObjectCommand({
          Bucket: process.env.S3_BUCKET,
          Key: key,
          Body: Buffer.from(buffer),
          ContentType: 'image/png',
          Metadata: {
            userId,
            generatedAt: new Date().toISOString(),
          },
        }));

        return `https://${process.env.S3_BUCKET}.s3.amazonaws.com/${key}`;
      }

      // Usage in generation flow
      async function generateAndStore(
        prompt: string,
        image: string,
        mask: string,
        userId: string
      ) {
        // Generate
        const output = await replicate.run(
          'black-forest-labs/flux-fill-pro',
          { input: { prompt, image, mask } }
        );

        // Store permanently
        const permanentUrl = await saveGeneratedImage(output, userId);

        // Track in database
        await db.generations.create({
          userId,
          prompt,
          url: permanentUrl,
        });

        return permanentUrl;
      }
      ```

  with_canvas:
    when: "Building mask drawing UI"
    approach: |
      Canvas-based mask editor:

      ```typescript
      // components/MaskEditor.tsx
      'use client';

      import { useRef, useState, useEffect } from 'react';

      interface MaskEditorProps {
        imageUrl: string;
        onMaskChange: (maskDataUrl: string) => void;
        brushSize?: number;
      }

      export function MaskEditor({
        imageUrl,
        onMaskChange,
        brushSize = 20,
      }: MaskEditorProps) {
        const canvasRef = useRef<HTMLCanvasElement>(null);
        const [isDrawing, setIsDrawing] = useState(false);
        const [ctx, setCtx] = useState<CanvasRenderingContext2D | null>(null);

        useEffect(() => {
          const canvas = canvasRef.current;
          if (!canvas) return;

          const context = canvas.getContext('2d');
          if (!context) return;

          // Load image
          const img = new Image();
          img.crossOrigin = 'anonymous';
          img.onload = () => {
            canvas.width = img.width;
            canvas.height = img.height;
            context.drawImage(img, 0, 0);
            setCtx(context);
          };
          img.src = imageUrl;
        }, [imageUrl]);

        const draw = (e: React.MouseEvent) => {
          if (!isDrawing || !ctx) return;

          const rect = canvasRef.current!.getBoundingClientRect();
          const x = e.clientX - rect.left;
          const y = e.clientY - rect.top;

          // Draw white circle (mask area)
          ctx.beginPath();
          ctx.arc(x, y, brushSize, 0, Math.PI * 2);
          ctx.fillStyle = 'white';
          ctx.fill();
        };

        const exportMask = () => {
          const canvas = canvasRef.current;
          if (!canvas) return;

          // Create mask canvas (black background)
          const maskCanvas = document.createElement('canvas');
          maskCanvas.width = canvas.width;
          maskCanvas.height = canvas.height;

          const maskCtx = maskCanvas.getContext('2d')!;
          maskCtx.fillStyle = 'black';
          maskCtx.fillRect(0, 0, canvas.width, canvas.height);

          // Copy white areas
          maskCtx.globalCompositeOperation = 'lighter';
          maskCtx.drawImage(canvas, 0, 0);

          onMaskChange(maskCanvas.toDataURL('image/png'));
        };

        return (
          <div>
            <canvas
              ref={canvasRef}
              onMouseDown={() => setIsDrawing(true)}
              onMouseUp={() => { setIsDrawing(false); exportMask(); }}
              onMouseMove={draw}
              style={{ cursor: 'crosshair' }}
            />
            <button onClick={() => ctx?.clearRect(0, 0, canvasRef.current!.width, canvasRef.current!.height)}>
              Clear Mask
            </button>
          </div>
        );
      }
      ```

  with_queues:
    when: "Processing images asynchronously"
    approach: |
      Queue-based processing with BullMQ:

      ```typescript
      // lib/queues/image-generation.ts
      import { Queue, Worker } from 'bullmq';
      import Replicate from 'replicate';

      const replicate = new Replicate();

      const generationQueue = new Queue('image-generation', {
        connection: { host: 'localhost', port: 6379 },
      });

      // Add job to queue
      export async function queueGeneration(
        userId: string,
        prompt: string,
        image: string,
        mask?: string
      ) {
        const job = await generationQueue.add('generate', {
          userId,
          prompt,
          image,
          mask,
        }, {
          attempts: 3,
          backoff: { type: 'exponential', delay: 1000 },
        });

        return job.id;
      }

      // Worker processes jobs
      const worker = new Worker('image-generation', async (job) => {
        const { userId, prompt, image, mask } = job.data;

        // Update progress
        await job.updateProgress(10);

        // Generate
        const output = await replicate.run(
          'black-forest-labs/flux-fill-pro',
          { input: { prompt, image, mask } }
        );

        await job.updateProgress(80);

        // Store result
        const url = await saveToStorage(output, userId);

        await job.updateProgress(100);

        // Notify user (WebSocket, email, etc.)
        await notifyUser(userId, {
          type: 'generation_complete',
          url,
        });

        return { url };
      }, {
        connection: { host: 'localhost', port: 6379 },
        concurrency: 5,  // Process 5 jobs at once
      });

      // Check job status
      export async function getJobStatus(jobId: string) {
        const job = await generationQueue.getJob(jobId);
        if (!job) return null;

        return {
          id: job.id,
          status: await job.getState(),
          progress: job.progress,
          result: job.returnvalue,
        };
      }
      ```

platform_integration:
  replicate:
    setup: |
      # Environment variables
      REPLICATE_API_TOKEN=r8_...

      # Install
      npm install replicate

      # Models
      # flux-fill-pro: Best inpainting
      # flux-dev: General generation
      # sdxl: Cost-effective

  stability:
    setup: |
      # Environment variables
      STABILITY_API_KEY=sk-...

      # Direct API (REST)
      # No SDK needed, use fetch

      # AWS Bedrock (alternative)
      # Use AWS SDK with Bedrock

  fal:
    setup: |
      # Environment variables
      FAL_KEY=...

      # Install
      npm install @fal-ai/serverless-client

      # Fast inference
      # Queue-less architecture

security_checklist:
  - "API keys server-side only"
  - "Content moderation on prompts"
  - "Content moderation on outputs"
  - "Rate limiting per user"
  - "Cost tracking and budgets"
  - "Input validation (dimensions, formats)"
  - "Secure file upload handling"
  - "No user data in logs"

testing_patterns:
  unit_tests: |
    // Mock API for testing
    jest.mock('replicate', () => ({
      run: jest.fn().mockResolvedValue(['https://example.com/image.png']),
      predictions: {
        create: jest.fn().mockResolvedValue({ id: 'test-123' }),
        get: jest.fn().mockResolvedValue({
          id: 'test-123',
          status: 'succeeded',
          output: ['https://example.com/image.png'],
        }),
      },
    }));

    describe('Image Generation', () => {
      it('generates with valid inputs', async () => {
        const result = await generateImage({
          prompt: 'a cat',
          width: 1024,
          height: 1024,
        });

        expect(result.url).toBeDefined();
      });

      it('rejects invalid dimensions', async () => {
        await expect(
          generateImage({ prompt: 'a cat', width: 1000 })
        ).rejects.toThrow('divisible by 8');
      });

      it('blocks unsafe prompts', async () => {
        await expect(
          generateImage({ prompt: 'unsafe content' })
        ).rejects.toThrow('blocked');
      });
    });

  integration_tests: |
    describe('Image Generation API', () => {
      it('returns image URL on success', async () => {
        const res = await fetch('/api/generate', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            prompt: 'a beautiful landscape',
          }),
        });

        expect(res.status).toBe(200);
        const data = await res.json();
        expect(data.url).toMatch(/^https:\/\//);
      });

      it('rate limits excessive requests', async () => {
        // Make many requests
        const requests = Array(20).fill(null).map(() =>
          fetch('/api/generate', {
            method: 'POST',
            body: JSON.stringify({ prompt: 'test' }),
          })
        );

        const responses = await Promise.all(requests);
        const rateLimited = responses.filter(r => r.status === 429);

        expect(rateLimited.length).toBeGreaterThan(0);
      });
    });
