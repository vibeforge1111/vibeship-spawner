# Validations - On-Device AI
# Quality checks for browser-based AI implementations

version: 1.0.0
skill_id: on-device-ai

validations:
  # WebGPU Checks
  - id: no-webgpu-check
    name: No WebGPU Fallback
    severity: error
    description: Must check WebGPU support and provide fallback
    pattern: |
      device.*webgpu(?!.*fallback|wasm|check|navigator\.gpu)
    message: "Check WebGPU support and fallback to WASM for unsupported browsers."
    autofix: false

  - id: no-gpu-capability-check
    name: No GPU Capability Check
    severity: warning
    description: Check device capabilities before loading models
    pattern: |
      pipeline|CreateMLCEngine(?!.*capability|device.*check|memory)
    message: "Check device capabilities before loading large models."
    autofix: false

  # Memory Management
  - id: large-model-mobile
    name: Large Model on Mobile
    severity: error
    description: Don't load 7B+ models on mobile devices
    pattern: |
      7B|8B|13B(?!.*isMobile|mobile.*check|device.*detect)
    message: "Models over 3B parameters are too large for mobile. Add device detection."
    autofix: false

  - id: no-memory-check
    name: No Memory Check
    severity: warning
    description: Check available memory before loading models
    pattern: |
      CreateMLCEngine|pipeline(?!.*memory|checkMemory|available)
    message: "Check available memory before loading models to prevent crashes."
    autofix: false

  - id: multiple-models-loaded
    name: Multiple Models Loaded
    severity: warning
    description: Loading multiple models may exhaust memory
    pattern: |
      pipeline.*await.*pipeline(?!.*unload|dispose)
    message: "Loading multiple models may exhaust memory. Consider unloading between uses."
    autofix: false

  # User Experience
  - id: no-loading-progress
    name: No Loading Progress UI
    severity: warning
    description: Show progress during model download
    pattern: |
      CreateMLCEngine|pipeline(?!.*progress|initProgressCallback|progress_callback)
    message: "Show loading progress to users during model download."
    autofix: false

  - id: blocking-main-thread
    name: Blocking Main Thread
    severity: warning
    description: Heavy inference should use Web Workers
    pattern: |
      await.*generate(?!.*Worker|worker)
    message: "Consider using Web Workers for inference to prevent UI blocking."
    autofix: false

  - id: no-loading-state
    name: No Loading State
    severity: info
    description: Track and display loading state
    pattern: |
      pipeline.*useState(?!.*loading|isLoading|progress)
    message: "Track loading state to show appropriate UI during model initialization."
    autofix: false

  # Error Handling
  - id: no-model-error-handling
    name: No Model Loading Error Handling
    severity: error
    description: Handle model loading failures gracefully
    pattern: |
      await.*pipeline|CreateMLCEngine(?!.*catch|try|error)
    message: "Handle model loading errors with try/catch and fallback UI."
    autofix: false

  - id: no-inference-error-handling
    name: No Inference Error Handling
    severity: warning
    description: Handle inference failures
    pattern: |
      await.*generate|model\((?!.*catch|try|error)
    message: "Handle inference errors gracefully."
    autofix: false

  # Performance
  - id: wrong-quantization
    name: Wrong Quantization for Task
    severity: warning
    description: Use appropriate quantization for task type
    pattern: |
      q4.*code|math|precise|embedding
    message: "Q4 quantization may be too aggressive for code/math tasks. Use Q8 or FP16."
    autofix: false

  - id: no-model-caching
    name: No Model Caching Strategy
    severity: info
    description: Implement caching for faster subsequent loads
    pattern: |
      pipeline|CreateMLCEngine(?!.*cache|useBrowserCache)
    message: "Enable browser caching for faster model loads on subsequent visits."
    autofix: false

  # Concurrent Access
  - id: parallel-inference
    name: Parallel Inference Calls
    severity: error
    description: Avoid concurrent model inference
    pattern: |
      Promise\.all.*generate|Promise\.all.*model\(
    message: "Avoid parallel inference - queue requests instead to prevent memory issues."
    autofix: false

code_smells:
  - id: sync-model-loading
    name: Synchronous Model Loading
    description: Model loading should be async with progress
    pattern: |
      await.*pipeline(?!.*useEffect|onMount)
    suggestion: "Load models in useEffect or onMount with loading state."

  - id: hardcoded-model
    name: Hardcoded Model ID
    description: Make model selection configurable
    pattern: |
      pipeline\(.*"Xenova.*"\)(?!.*config|env|prop)
    suggestion: "Make model ID configurable for different device capabilities."

  - id: no-model-cleanup
    name: No Model Cleanup
    description: Dispose models when unmounting
    pattern: |
      useEffect.*pipeline(?!.*dispose|cleanup|return)
    suggestion: "Dispose model in useEffect cleanup to free GPU memory."

  - id: large-context-mobile
    name: Large Context on Mobile
    description: Limit context size on mobile devices
    pattern: |
      max_tokens.*1000|max_new_tokens.*1000(?!.*isMobile)
    suggestion: "Reduce max tokens on mobile for better performance."

best_practices:
  - id: capability-detection
    name: Device Capability Detection
    check: |
      Check WebGPU, memory, and device type before loading.
    recommendation: |
      interface DeviceProfile {
        webgpu: boolean;
        memory: number; // MB
        isMobile: boolean;
        recommendedModel: string;
        recommendedDtype: string;
      }

      async function detectDeviceProfile(): Promise<DeviceProfile> {
        // Check WebGPU
        let webgpu = false;
        if ("gpu" in navigator) {
          try {
            const adapter = await navigator.gpu.requestAdapter();
            webgpu = !!adapter;
          } catch {}
        }

        // Check memory
        let memory = 4000; // Default 4GB
        if ("memory" in performance) {
          memory = Math.round((performance as any).memory.jsHeapSizeLimit / 1024 / 1024);
        }
        if ((navigator as any).deviceMemory) {
          memory = Math.min(memory, (navigator as any).deviceMemory * 1024);
        }

        // Check mobile
        const isMobile = /iPhone|iPad|Android/i.test(navigator.userAgent);

        // Recommend based on capabilities
        let recommendedModel = "Llama-3.2-1B-Instruct-q4f16_1-MLC";
        let recommendedDtype = "q4";

        if (!isMobile && memory > 8000) {
          recommendedModel = "Phi-3.5-mini-instruct-q4f16_1-MLC";
        } else if (!isMobile && memory > 4000) {
          recommendedModel = "Llama-3.2-3B-Instruct-q4f16_1-MLC";
        }

        if (!webgpu) {
          recommendedDtype = "q8"; // WASM works better with q8
        }

        return {
          webgpu,
          memory,
          isMobile,
          recommendedModel,
          recommendedDtype,
        };
      }

  - id: progressive-enhancement
    name: Progressive Enhancement
    check: |
      Start with cloud API, optionally upgrade to local.
    recommendation: |
      class HybridAI {
        private localModel: any = null;
        private useLocal = false;

        constructor(private cloudFallback: (prompt: string) => Promise<string>) {}

        async enableLocal(
          modelId: string,
          onProgress?: (p: number) => void
        ): Promise<boolean> {
          try {
            const profile = await detectDeviceProfile();

            if (!profile.webgpu && profile.isMobile) {
              console.log("Device not suitable for local AI");
              return false;
            }

            this.localModel = await pipeline("text-generation", modelId, {
              device: profile.webgpu ? "webgpu" : "wasm",
              dtype: profile.recommendedDtype,
              progress_callback: (p) => onProgress?.(p.progress),
            });

            this.useLocal = true;
            return true;
          } catch (error) {
            console.warn("Failed to load local model:", error);
            return false;
          }
        }

        async generate(prompt: string): Promise<string> {
          if (this.useLocal && this.localModel) {
            try {
              const result = await this.localModel(prompt, {
                max_new_tokens: 256,
              });
              return result[0].generated_text;
            } catch (error) {
              console.warn("Local inference failed, using cloud:", error);
              this.useLocal = false;
            }
          }

          // Cloud fallback
          return this.cloudFallback(prompt);
        }
      }

  - id: model-lifecycle
    name: Model Lifecycle Management
    check: |
      Properly load, use, and dispose of models.
    recommendation: |
      import { useEffect, useState, useCallback, useRef } from "react";

      function useModel(modelId: string) {
        const modelRef = useRef<any>(null);
        const [ready, setReady] = useState(false);
        const [loading, setLoading] = useState(false);
        const [error, setError] = useState<Error | null>(null);

        useEffect(() => {
          let cancelled = false;

          async function load() {
            setLoading(true);
            setError(null);

            try {
              const model = await pipeline("text-generation", modelId, {
                device: "webgpu",
                dtype: "q4",
              });

              if (!cancelled) {
                modelRef.current = model;
                setReady(true);
              }
            } catch (e) {
              if (!cancelled) {
                setError(e instanceof Error ? e : new Error(String(e)));
              }
            } finally {
              if (!cancelled) {
                setLoading(false);
              }
            }
          }

          load();

          // Cleanup on unmount
          return () => {
            cancelled = true;
            if (modelRef.current?.dispose) {
              modelRef.current.dispose();
            }
            modelRef.current = null;
          };
        }, [modelId]);

        const generate = useCallback(async (prompt: string) => {
          if (!modelRef.current) {
            throw new Error("Model not ready");
          }
          return modelRef.current(prompt);
        }, []);

        return { ready, loading, error, generate };
      }

testing_checklist:
  compatibility:
    - "WebGPU support detected and fallback works"
    - "WASM fallback functions correctly"
    - "Mobile devices get appropriate models"
    - "Low-memory devices handled gracefully"

  user_experience:
    - "Loading progress shown to user"
    - "Error states displayed clearly"
    - "First-time download warning shown"
    - "Cache status indicated"

  performance:
    - "Inference doesn't block UI"
    - "Memory usage stays within limits"
    - "Model unloads on unmount"
    - "No concurrent inference crashes"

  error_handling:
    - "Model loading failures handled"
    - "Inference errors caught"
    - "OOM situations handled"
    - "Network failures during download handled"

  offline:
    - "Cached models work offline"
    - "Service worker caches model files"
    - "Offline state detected and shown"
