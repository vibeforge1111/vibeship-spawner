id: llm-fine-tuning-collaboration
skill: llm-fine-tuning
version: 1.0.0

receives_from:
  - skill: transformer-architecture
    context: "Base model architecture"
    receives:
      - "Model architecture details"
      - "Layer configurations"
    provides: "Fine-tuning strategy for architecture"

  - skill: distributed-training
    context: "Multi-GPU fine-tuning"
    receives:
      - "FSDP/DDP configuration"
      - "Sharding strategy"
    provides: "Distributed LoRA setup"

delegation_triggers:
  - trigger: "multi.*gpu|distributed|fsdp|ddp"
    delegate_to: distributed-training
    context: "Distributed fine-tuning setup"

  - trigger: "quantiz|deploy|inference.*optim"
    delegate_to: model-optimization
    context: "Post-fine-tuning optimization"

  - trigger: "rlhf|reward|preference|dpo"
    delegate_to: reinforcement-learning
    context: "Alignment after SFT"

  - trigger: "architecture|attention|layers"
    delegate_to: transformer-architecture
    context: "Model architecture questions"

feedback_loops:
  receives_feedback_from:
    - skill: reinforcement-learning
      signal: "SFT model for RLHF"
      action: "Prepare checkpoint for alignment"

    - skill: model-optimization
      signal: "Deployment constraints"
      action: "Consider QLoRA or smaller model"

  sends_feedback_to:
    - skill: distributed-training
      signal: "LoRA configuration"
      action: "Set up distributed LoRA training"

    - skill: model-optimization
      signal: "Trained adapter"
      action: "Merge and quantize for deployment"

common_combinations:
  - name: SFT to Deployment
    skills:
      - llm-fine-tuning
      - model-optimization
    workflow: |
      1. Prepare instruction dataset
      2. Configure LoRA/QLoRA
      3. Train with SFTTrainer
      4. Merge adapters
      5. Quantize (GPTQ/AWQ)
      6. Deploy

  - name: Full Alignment Pipeline
    skills:
      - llm-fine-tuning
      - reinforcement-learning
    workflow: |
      1. SFT on instruction data
      2. Collect preference data
      3. Train reward model
      4. PPO/DPO alignment
      5. Evaluate and iterate

  - name: Distributed Fine-tuning
    skills:
      - llm-fine-tuning
      - distributed-training
    workflow: |
      1. Set up multi-GPU environment
      2. Configure FSDP for large model
      3. Apply LoRA adapters
      4. Train with accelerate/DeepSpeed
      5. Gather and merge adapters

  - name: Domain Adaptation
    skills:
      - llm-fine-tuning
      - transformer-architecture
    workflow: |
      1. Select appropriate base model
      2. Prepare domain-specific data
      3. Continued pretraining (optional)
      4. Instruction fine-tuning
      5. Evaluate on domain tasks

ecosystem:
  fine_tuning_libraries:
    - "PEFT - Parameter-Efficient Fine-Tuning"
    - "TRL - Transformer Reinforcement Learning"
    - "Axolotl - Fine-tuning framework"
    - "Unsloth - Fast LoRA fine-tuning"
    - "LitGPT - Lightning AI"

  techniques:
    - "LoRA - Low-Rank Adaptation"
    - "QLoRA - Quantized LoRA"
    - "DoRA - Weight-Decomposed LoRA"
    - "AdaLoRA - Adaptive LoRA"
    - "IA3 - Infused Adapter by Inhibiting and Amplifying"

  datasets:
    - "Alpaca - Instruction following"
    - "ShareGPT - Conversations"
    - "UltraChat - Dialogues"
    - "OpenOrca - Reasoning"
    - "LIMA - Less Is More for Alignment"

references:
  papers:
    - "LoRA: Low-Rank Adaptation (Hu et al. 2021)"
    - "QLoRA: Efficient Finetuning (Dettmers et al. 2023)"
    - "DoRA: Weight-Decomposed Low-Rank Adaptation (2024)"

  tutorials:
    - "HuggingFace PEFT Documentation"
    - "Axolotl Fine-tuning Guide"
    - "Sebastian Raschka's LLM Fine-tuning"
