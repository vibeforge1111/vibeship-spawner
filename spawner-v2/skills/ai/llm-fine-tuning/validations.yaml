validations:

  - id: no-pad-token
    name: Tokenizer Without Pad Token
    severity: error
    type: regex
    pattern:
      - "AutoTokenizer\\.from_pretrained\\((?!.*pad_token)"
      - "tokenizer\\s*=.*from_pretrained(?!.*\\.pad_token\\s*=)"
    message: "Many LLM tokenizers lack pad tokens. Set tokenizer.pad_token = tokenizer.eos_token."
    fix_action: "Add: tokenizer.pad_token = tokenizer.eos_token"
    applies_to:
      - "**/*train*.py"
      - "**/*fine*.py"

  - id: no-gradient-checkpointing
    name: Large Model Without Gradient Checkpointing
    severity: warning
    type: regex
    pattern:
      - "Trainer\\((?!.*gradient_checkpointing)"
      - "TrainingArguments\\((?!.*gradient_checkpointing)"
    message: "Enable gradient checkpointing for memory savings on large models."
    fix_action: "Add: gradient_checkpointing=True in TrainingArguments"
    applies_to:
      - "**/*train*.py"

  - id: high-learning-rate
    name: Learning Rate Too High for LoRA
    severity: warning
    type: regex
    pattern:
      - "learning_rate\\s*=\\s*[1-9]e-[0-2]\\b"
      - "lr\\s*=\\s*[1-9]e-[0-2]\\b"
    message: "Learning rate may be too high for LoRA (typical: 1e-4 to 3e-4)."
    fix_action: "Use: learning_rate=2e-4"
    applies_to:
      - "**/*lora*.py"
      - "**/*peft*.py"

  - id: no-eval-dataset
    name: Training Without Evaluation Dataset
    severity: warning
    type: regex
    pattern:
      - "Trainer\\((?!.*eval_dataset)"
      - "SFTTrainer\\((?!.*eval_dataset)"
    message: "Training without eval_dataset risks undetected overfitting."
    fix_action: "Add: eval_dataset=validation_set"
    applies_to:
      - "**/*train*.py"

  - id: missing-bf16
    name: Training Without Mixed Precision
    severity: info
    type: regex
    pattern:
      - "TrainingArguments\\((?!.*bf16|.*fp16)"
    message: "Enable bf16=True or fp16=True for faster training and memory savings."
    fix_action: "Add: bf16=True (or fp16=True for older GPUs)"
    applies_to:
      - "**/*train*.py"

  - id: no-warmup
    name: Training Without Warmup
    severity: info
    type: regex
    pattern:
      - "TrainingArguments\\((?!.*warmup)"
    message: "Warmup helps training stability, especially for fine-tuning."
    fix_action: "Add: warmup_ratio=0.03 or warmup_steps=100"
    applies_to:
      - "**/*train*.py"

  - id: lora-without-target-modules
    name: LoRA Config Missing Target Modules
    severity: error
    type: regex
    pattern:
      - "LoraConfig\\((?!.*target_modules)"
    message: "LoraConfig requires target_modules to specify which layers to adapt."
    fix_action: "Add: target_modules=['q_proj', 'v_proj', 'k_proj', 'o_proj']"
    applies_to:
      - "**/*.py"

  - id: qlora-without-prepare
    name: QLoRA Without prepare_model_for_kbit_training
    severity: error
    type: regex
    pattern:
      - "load_in_4bit\\s*=\\s*True(?!.*prepare_model_for_kbit_training)"
      - "BitsAndBytesConfig(?!.*prepare_model_for_kbit_training)"
    message: "4-bit models need prepare_model_for_kbit_training() before LoRA."
    fix_action: "Add: model = prepare_model_for_kbit_training(model)"
    applies_to:
      - "**/*.py"
