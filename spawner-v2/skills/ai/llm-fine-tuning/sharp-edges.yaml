id: llm-fine-tuning-sharp-edges
skill: llm-fine-tuning
version: 1.0.0

sharp_edges:

  - id: format-inconsistency
    severity: critical
    title: "Model Learns Inconsistent Output Format"
    summary: "Training data has mixed formats, model outputs garbage"
    symptoms:
      - "Model sometimes follows format, sometimes doesn't"
      - "Outputs contain format artifacts"
      - "Inconsistent response structure"
    why: |
      LLMs learn patterns from data, including formatting.
      If training data has "### Response:" sometimes and "Answer:" other times,
      model will randomly produce both, breaking downstream parsing.
    gotcha: |
      # Mixed formats in training data
      examples = [
          {"text": "### Instruction: ...\n### Response: ..."},
          {"text": "User: ...\nAssistant: ..."},
          {"text": "Question: ...\nAnswer: ..."},
      ]  # Model learns all three, uses randomly
    solution: |
      # Use ONE consistent format
      TEMPLATE = """### Instruction:
      {instruction}

      ### Response:
      {response}"""

      # Validate all examples match format
      for example in examples:
          assert example['text'].startswith("### Instruction:")
          assert "### Response:" in example['text']

  - id: catastrophic-forgetting
    severity: critical
    title: "Fine-Tuned Model Forgets General Knowledge"
    summary: "Model only knows task domain, fails on everything else"
    symptoms:
      - "Model can't do basic tasks anymore"
      - "Reasoning capability degraded"
      - "Only responds well to training-like prompts"
    why: |
      Fine-tuning updates weights toward task distribution.
      Without general data, model overwrites general knowledge
      with task-specific patterns.
    gotcha: |
      # Only task data
      train_dataset = load_task_dataset()  # 100% task data

      trainer.train(train_dataset)  # Forgets general capabilities
    solution: |
      # Mix task and general data
      task_data = load_task_dataset()
      general_data = load_general_dataset()

      # 70% task, 30% general
      n_general = int(len(task_data) * 0.3 / 0.7)
      general_sampled = general_data.shuffle().select(range(n_general))

      mixed_data = concatenate_datasets([task_data, general_sampled])
      mixed_data = mixed_data.shuffle(seed=42)

  - id: pad-token-missing
    severity: high
    title: "Training Fails or Produces Garbage"
    summary: "Tokenizer has no pad token"
    symptoms:
      - "ValueError: Padding is required"
      - "Model outputs <unk> tokens"
      - "Loss is NaN or very high"
    why: |
      Many LLM tokenizers (Llama, Mistral) don't have pad tokens.
      Training requires padding for batching.
      Using eos_token as pad_token is common but has gotchas.
    gotcha: |
      tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b")
      # tokenizer.pad_token is None!

      tokenizer(texts, padding=True)  # Crashes or pads with wrong token
    solution: |
      tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b")

      # Option 1: Use eos as pad (common)
      tokenizer.pad_token = tokenizer.eos_token

      # Option 2: Add new pad token (safer)
      tokenizer.add_special_tokens({'pad_token': '[PAD]'})
      model.resize_token_embeddings(len(tokenizer))

      # Option 3: Use left padding for generation
      tokenizer.padding_side = 'left'

  - id: target-modules-mismatch
    severity: high
    title: "LoRA Has No Effect or Errors"
    summary: "Target modules don't match model architecture"
    symptoms:
      - "ValueError: target_modules not found"
      - "trainable params: 0"
      - "Fine-tuning has no effect"
    why: |
      Different model architectures use different module names.
      Llama uses "q_proj", GPT-2 uses "c_attn".
      Wrong names means LoRA isn't applied.
    gotcha: |
      # Using Llama module names on GPT-2
      config = LoraConfig(
          target_modules=["q_proj", "v_proj"],  # Llama names
      )

      # GPT-2 has "c_attn", "c_proj" - LoRA won't apply
    solution: |
      # Architecture-specific target modules
      TARGET_MODULES = {
          "llama": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
          "mistral": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
          "gpt2": ["c_attn", "c_proj", "c_fc"],
          "falcon": ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"],
          "phi": ["q_proj", "k_proj", "v_proj", "dense", "fc1", "fc2"],
      }

      # Or auto-detect
      for name, module in model.named_modules():
          print(name)  # Find the right names

  - id: gradient-checkpointing-inference
    severity: medium
    title: "Model Runs Slow After Loading"
    summary: "Gradient checkpointing still enabled during inference"
    symptoms:
      - "Inference is 2-3x slower than expected"
      - "High GPU memory usage during inference"
      - "Same speed as training"
    why: |
      Gradient checkpointing recomputes activations during backward pass.
      If not disabled, it also recomputes during inference (wasteful).
    gotcha: |
      model = prepare_model_for_kbit_training(model)
      # gradient_checkpointing_enable() called internally

      model.eval()
      output = model.generate(...)  # Still checkpointing!
    solution: |
      # Disable before inference
      model.gradient_checkpointing_disable()
      model.eval()

      # Or when loading for inference
      model = AutoModelForCausalLM.from_pretrained(
          model_path,
          use_cache=True,  # Enable KV cache (incompatible with checkpointing)
      )

  - id: merge-oom
    severity: medium
    title: "OOM When Merging LoRA Weights"
    summary: "Merge fails because full model doesn't fit in memory"
    symptoms:
      - "CUDA out of memory during merge"
      - "Merge hangs indefinitely"
      - "System becomes unresponsive"
    why: |
      Merging requires loading full model + adapter into memory.
      For quantized training (QLoRA), the merged model is larger
      than what fit during training.
    gotcha: |
      # Trained with 4-bit, now trying to merge
      base_model = AutoModelForCausalLM.from_pretrained(
          "meta-llama/Llama-2-70b",
          torch_dtype=torch.bfloat16,  # 140GB!
          device_map="auto",
      )

      model = PeftModel.from_pretrained(base_model, adapter_path)
      merged = model.merge_and_unload()  # OOM
    solution: |
      # Merge on CPU (slower but fits)
      base_model = AutoModelForCausalLM.from_pretrained(
          "meta-llama/Llama-2-70b",
          torch_dtype=torch.bfloat16,
          device_map="cpu",  # Merge on CPU
          low_cpu_mem_usage=True,
      )

      model = PeftModel.from_pretrained(base_model, adapter_path)
      merged = model.merge_and_unload()
      merged.save_pretrained(output_path)

      # Or: Use unsloth for efficient merging
      # Or: Keep adapters separate, load at inference

detection:
  file_patterns:
    - "**/*fine*tun*.py"
    - "**/*lora*.py"
    - "**/*peft*.py"
    - "**/*train*.py"
    - "**/*sft*.py"
