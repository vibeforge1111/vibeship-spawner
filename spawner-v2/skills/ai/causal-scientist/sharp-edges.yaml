# Causal Scientist Sharp Edges
# Battle scars from causal inference in production

sharp_edges:
  - id: confounding-everywhere
    summary: Unmeasured confounders invalidate causal estimates
    severity: critical
    situation: |
      You run a causal analysis showing memory type X improves outcomes.
      You ship a feature prioritizing type X. No improvement in production.
      Turns out, expert users (confounder) both use type X and have better outcomes.
    why: |
      Observational data has confounders. If you don't measure and control for
      them, your "causal" effect is actually confounded association. No amount
      of sophisticated estimation fixes unmeasured confounding.
    solution: |
      # Always enumerate potential confounders
      class ConfounderAnalysis:
          COMMON_CONFOUNDERS = [
              "user_expertise",
              "user_activity_level",
              "content_complexity",
              "time_of_day",
              "platform",
              "prior_outcomes",
          ]

          async def check_for_confounding(
              self,
              treatment: str,
              outcome: str,
              data: pd.DataFrame,
          ) -> ConfounderReport:
              potential_confounders = []

              for var in self.COMMON_CONFOUNDERS:
                  if var not in data.columns:
                      potential_confounders.append(
                          UnmeasuredConfounder(var, reason="not in dataset")
                      )
                      continue

                  # Check if variable is associated with both treatment and outcome
                  treatment_corr = data[var].corr(data[treatment])
                  outcome_corr = data[var].corr(data[outcome])

                  if abs(treatment_corr) > 0.1 and abs(outcome_corr) > 0.1:
                      potential_confounders.append(
                          MeasuredConfounder(
                              var,
                              treatment_corr=treatment_corr,
                              outcome_corr=outcome_corr,
                          )
                      )

              return ConfounderReport(
                  treatment=treatment,
                  outcome=outcome,
                  confounders=potential_confounders,
                  recommendation=self._get_recommendation(potential_confounders),
              )

          def _get_recommendation(self, confounders) -> str:
              unmeasured = [c for c in confounders if isinstance(c, UnmeasuredConfounder)]
              if unmeasured:
                  return f"CAUTION: {len(unmeasured)} potential unmeasured confounders"
              return "Include all measured confounders in analysis"

      # Run sensitivity analysis for hidden confounding
      refutation = model.refute_estimate(
          identified_estimand,
          estimate,
          method_name="add_unobserved_common_cause",
          confounders_effect_on_treatment="linear",
          confounders_effect_on_outcome="linear",
      )
    symptoms:
      - "Effect disappears in A/B test"
      - "Effect varies wildly across user segments"
      - "Adding control variables changes estimate significantly"
      - "Domain experts skeptical of finding"
    detection_pattern: 'estimate.*effect(?!.*confounder|.*common_cause)'
    version_range: ">=1.0.0"

  - id: dowhy-identification-ignored
    summary: Proceeding with estimation when effect is not identified
    severity: critical
    situation: |
      You run DoWhy with proceed_when_unidentifiable=True because the default
      blocks your analysis. You get an estimate. It's meaningless.
    why: |
      Identification means: can we answer this causal question from the data
      and assumptions we have? If not identified, no estimator will give you
      the right answer. Proceeding anyway gives you a number without meaning.
    solution: |
      # NEVER proceed when unidentifiable
      model = CausalModel(
          data=data,
          treatment=treatment,
          outcome=outcome,
          common_causes=confounders,
          # Specify instruments if available
          instruments=instruments,
      )

      # Require identification
      identified = model.identify_effect(
          proceed_when_unidentifiable=False  # Default, keep it!
      )

      if not identified:
          # Don't estimate - gather more data or assumptions
          return CausalResult(
              identified=False,
              reason="Causal effect not identifiable with current data",
              suggestion="Need instrument variable or more confounders measured"
          )

      # Only estimate if identified
      estimate = model.estimate_effect(identified_estimand=identified)

      # Alternative: If you MUST proceed, be explicit about assumptions
      class UnidentifiedCausalEstimate:
          estimate: float
          assumptions_required: List[str]
          confidence: str = "LOW - UNIDENTIFIED EFFECT"

          def __str__(self):
              return (
                  f"CAUTION: Unidentified estimate = {self.estimate}\n"
                  f"Required assumptions:\n"
                  + "\n".join(f"  - {a}" for a in self.assumptions_required)
              )
    symptoms:
      - "DoWhy warns about unidentifiable effect"
      - "Different graph structures give same estimate"
      - "Can't explain what assumptions make estimate valid"
      - "Estimate sensitive to small graph changes"
    detection_pattern: 'proceed_when_unidentifiable.*=.*True'
    version_range: ">=1.0.0"

  - id: cyclic-causal-graph
    summary: Accidentally creating cycles in causal DAG
    severity: high
    situation: |
      You model "user satisfaction causes purchases" and "purchases cause
      user satisfaction." Your graph has a cycle. All causal reasoning breaks.
    why: |
      Causal DAGs are acyclic by definition. Cycles represent equilibrium or
      feedback loops over time. To model feedback, unroll over time steps.
      A cycle in a single time slice is a modeling error.
    solution: |
      # Validate DAG is acyclic
      import networkx as nx

      class CausalGraphValidator:
          def validate(self, graph: nx.DiGraph) -> ValidationResult:
              errors = []

              # Check for cycles
              try:
                  cycles = list(nx.simple_cycles(graph))
                  if cycles:
                      errors.append(CycleError(
                          f"Graph has {len(cycles)} cycles: {cycles[:3]}"
                      ))
              except nx.NetworkXNoCycle:
                  pass  # Good, no cycles

              # Check for self-loops
              self_loops = list(nx.nodes_with_selfloops(graph))
              if self_loops:
                  errors.append(SelfLoopError(
                      f"Nodes with self-loops: {self_loops}"
                  ))

              return ValidationResult(valid=len(errors) == 0, errors=errors)

      # Model feedback loops over time
      class TemporalCausalModel:
          """Unroll feedback loops across time steps."""

          def add_feedback_loop(
              self,
              var_a: str,
              var_b: str,
          ) -> None:
              # Instead of A <-> B (invalid), model:
              # A_t -> B_t+1 -> A_t+2 -> B_t+3 ...
              for t in range(self.time_steps - 1):
                  self.graph.add_edge(f"{var_a}_t{t}", f"{var_b}_t{t+1}")
                  self.graph.add_edge(f"{var_b}_t{t}", f"{var_a}_t{t+1}")
    symptoms:
      - "NetworkX raises cycle error"
      - "DoWhy fails with 'not a DAG'"
      - "Conceptually, variables cause each other"
      - "Graph visualization shows arrows in both directions"
    detection_pattern: 'add_edge.*,.*\\).*add_edge.*,.*\\)'
    version_range: ">=1.0.0"

  - id: causal-discovery-data-requirements
    summary: Running causal discovery on insufficient data
    severity: high
    situation: |
      You run PC algorithm on 50 observations. It discovers a sparse graph
      (few edges). You conclude there are few causal relationships.
      Actually, you just didn't have enough data to detect them.
    why: |
      Causal discovery algorithms are statistical tests at heart. With small
      samples, tests have low power. They fail to reject independence, so
      edges are missing. More data = more power = more edges discovered.
    solution: |
      # Check sample size before discovery
      class DataSizedCausalDiscovery:
          # Rule of thumb: need 50-100 samples per variable for PC
          MIN_SAMPLES_PER_VAR = 50

          def validate_data_size(
              self,
              data: pd.DataFrame,
          ) -> DataSizeValidation:
              n_vars = len(data.columns)
              n_samples = len(data)
              required = n_vars * self.MIN_SAMPLES_PER_VAR

              if n_samples < required:
                  return DataSizeValidation(
                      valid=False,
                      message=f"Have {n_samples} samples, need ~{required} for {n_vars} variables",
                      recommendation="Collect more data or reduce variables",
                  )

              # Also check for sufficient variance
              low_variance = [
                  col for col in data.columns
                  if data[col].std() < 0.01
              ]

              if low_variance:
                  return DataSizeValidation(
                      valid=False,
                      message=f"Low variance columns: {low_variance}",
                      recommendation="Remove constant or near-constant columns",
                  )

              return DataSizeValidation(valid=True)

          async def discover(self, data: pd.DataFrame, **kwargs):
              validation = self.validate_data_size(data)

              if not validation.valid:
                  raise InsufficientDataError(validation.message)

              # Adjust alpha based on sample size
              # Larger samples -> can use smaller alpha (stricter)
              n = len(data)
              alpha = 0.05 if n > 1000 else 0.1 if n > 500 else 0.2

              return await self._run_pc(data, alpha=alpha, **kwargs)
    symptoms:
      - "Discovered graph is surprisingly sparse"
      - "Adding more data changes graph structure significantly"
      - "Different random seeds give very different graphs"
      - "p-values all just above significance threshold"
    detection_pattern: 'pc\\(|causal.*discover(?!.*sample|.*size)'
    version_range: ">=1.0.0"

  - id: estimator-assumption-mismatch
    summary: Using estimator with violated assumptions
    severity: medium
    situation: |
      You use linear regression backdoor adjustment. Your treatment effect
      is actually non-linear. Estimate is wrong, and you don't know it.
    why: |
      Each causal estimator has assumptions. Linear regression assumes linear
      effects. Propensity score assumes correct propensity model. Violating
      assumptions biases estimates in unpredictable ways.
    solution: |
      # Use multiple estimators and check agreement
      class RobustEstimation:
          ESTIMATOR_ASSUMPTIONS = {
              "backdoor.linear_regression": [
                  "Linear treatment effect",
                  "No interaction effects",
                  "Correct confounders specified",
              ],
              "backdoor.propensity_score_weighting": [
                  "Correct propensity model",
                  "Positivity (overlap)",
                  "No unmeasured confounders",
              ],
              "iv.instrumental_variable": [
                  "Valid instrument (relevance + exclusion)",
                  "Monotonicity",
                  "No direct effect of instrument on outcome",
              ],
          }

          async def robust_estimate(
              self,
              model: CausalModel,
              identified_estimand,
          ) -> RobustEstimateResult:
              estimates = {}
              failed = {}

              for method in self.ESTIMATOR_ASSUMPTIONS.keys():
                  try:
                      est = model.estimate_effect(
                          identified_estimand,
                          method_name=method,
                      )
                      estimates[method] = est.value
                  except Exception as e:
                      failed[method] = str(e)

              if not estimates:
                  raise NoValidEstimatorError("All estimators failed")

              # Check agreement
              values = list(estimates.values())
              mean_est = np.mean(values)
              std_est = np.std(values)
              cv = std_est / abs(mean_est) if mean_est != 0 else float('inf')

              return RobustEstimateResult(
                  estimates=estimates,
                  mean=mean_est,
                  std=std_est,
                  coefficient_of_variation=cv,
                  agreement="GOOD" if cv < 0.3 else "POOR" if cv > 0.5 else "MODERATE",
                  failed_methods=failed,
              )
    symptoms:
      - "Estimates vary widely across methods"
      - "Residuals show patterns (non-linearity)"
      - "Effect changes when adding interactions"
      - "Propensity scores near 0 or 1"
    detection_pattern: 'estimate_effect.*method_name.*=(?!.*multiple|.*robust)'
    version_range: ">=1.0.0"

  - id: counterfactual-identifiability
    summary: Computing counterfactuals without proper assumptions
    severity: medium
    situation: |
      You want to know "what would have happened if user saw memory X
      instead of Y?" You compute a counterfactual. It's meaningless because
      the counterfactual isn't identifiable from observational data.
    why: |
      Counterfactuals about individuals are fundamentally unidentifiable
      from observational data alone. You need strong assumptions (functional
      form) or randomization. Most counterfactual estimates are modeler's
      projections, not data-driven answers.
    solution: |
      # Be explicit about counterfactual assumptions
      @dataclass
      class CounterfactualQuery:
          observation: Dict[str, float]
          intervention: Dict[str, float]
          target: str

          # Required assumptions
          assumed_mechanisms: List[str]  # E.g., "linear", "additive noise"
          identifiable: bool
          confidence_level: str  # "high", "medium", "low"

      class CarefulCounterfactualReasoner:
          async def compute_counterfactual(
              self,
              query: CounterfactualQuery,
              scm: gcm.StructuralCausalModel,
          ) -> CounterfactualResult:
              # Validate assumptions
              warnings = []

              # 1. Check if mechanisms are correctly specified
              for node in query.intervention.keys():
                  mech = scm.causal_mechanism(node)
                  if isinstance(mech, gcm.ml.create_linear_regressor):
                      warnings.append(
                          f"Assuming linear mechanism for {node}"
                      )

              # 2. Check for extrapolation
              for var, value in query.intervention.items():
                  observed_range = (
                      self.training_data[var].min(),
                      self.training_data[var].max()
                  )
                  if value < observed_range[0] or value > observed_range[1]:
                      warnings.append(
                          f"Intervention {var}={value} outside observed range {observed_range}"
                      )

              # 3. Compute with uncertainty
              samples = gcm.counterfactual_samples(
                  scm,
                  interventions={k: lambda v=v: v for k, v in query.intervention.items()},
                  observed_data=pd.DataFrame([query.observation]),
                  num_samples=1000,
              )

              return CounterfactualResult(
                  point_estimate=samples[query.target].mean(),
                  uncertainty=samples[query.target].std(),
                  warnings=warnings,
                  confidence=query.confidence_level,
                  disclaimer="Counterfactual depends on assumed causal mechanisms",
              )
    symptoms:
      - "Counterfactual far from any observed data"
      - "Small mechanism changes â†’ large counterfactual changes"
      - "Counterfactual outside plausible range"
      - "No uncertainty quantification"
    detection_pattern: 'counterfactual(?!.*assumption|.*uncertaint)'
    version_range: ">=1.0.0"

  - id: selection-bias-ignored
    summary: Analyzing selected sample as if it were random
    severity: medium
    situation: |
      You analyze "users who clicked" to understand click effects. But
      users who clicked are different from those who didn't. Your analysis
      is biased by the selection process.
    why: |
      Selection on a collider (or descendant of collider) induces spurious
      associations. Analyzing selected samples without modeling selection
      gives biased causal estimates.
    solution: |
      # Detect and handle selection bias
      class SelectionBiasChecker:
          async def check_selection_bias(
              self,
              full_population: pd.DataFrame,
              analyzed_sample: pd.DataFrame,
              treatment: str,
              outcome: str,
          ) -> SelectionBiasReport:
              # 1. Check if sample is representative
              representation_issues = []

              for col in full_population.columns:
                  full_dist = full_population[col].describe()
                  sample_dist = analyzed_sample[col].describe()

                  if abs(full_dist['mean'] - sample_dist['mean']) > full_dist['std']:
                      representation_issues.append(col)

              # 2. Check if selection is on collider path
              # (Would need causal graph to fully determine)

              # 3. Estimate selection probability
              selection_indicator = pd.Series(
                  full_population.index.isin(analyzed_sample.index),
                  index=full_population.index,
              )

              # Fit selection model
              from sklearn.linear_model import LogisticRegression
              X = full_population[[treatment, outcome]].values
              selection_model = LogisticRegression().fit(X, selection_indicator)

              selection_prob = selection_model.predict_proba(X)[:, 1]

              return SelectionBiasReport(
                  sample_size_ratio=len(analyzed_sample) / len(full_population),
                  representation_issues=representation_issues,
                  selection_depends_on_treatment=(
                      abs(selection_model.coef_[0][0]) > 0.1
                  ),
                  selection_depends_on_outcome=(
                      abs(selection_model.coef_[0][1]) > 0.1
                  ),
                  recommendation=self._get_recommendation(representation_issues),
              )

          def _get_recommendation(self, issues) -> str:
              if not issues:
                  return "Sample appears representative"
              return (
                  f"Sample differs on: {issues}. "
                  "Consider inverse probability weighting or Heckman correction."
              )
    symptoms:
      - "Results don't replicate in full population"
      - "Sample is non-random subset (e.g., only converters)"
      - "Sample characteristics differ from population"
      - "Selection mechanism related to treatment or outcome"
    detection_pattern: 'filter.*\\[|query.*WHERE(?!.*random)'
    version_range: ">=1.0.0"
