id: transformer-architecture-collaboration
skill: transformer-architecture
version: 1.0.0

receives_from:
  - skill: distributed-training
    context: "Multi-GPU training strategies"
    receives:
      - "FSDP/DDP configuration"
      - "Gradient accumulation settings"
      - "Mixed precision policies"
    provides: "Model architecture for distributed setup"

  - skill: model-optimization
    context: "Inference optimization"
    receives:
      - "Quantization requirements"
      - "Target latency constraints"
    provides: "Architecture details for optimization"

delegation_triggers:
  - trigger: "train.*multi.*gpu|distributed|fsdp|ddp"
    delegate_to: distributed-training
    context: "Multi-GPU training setup"

  - trigger: "quantiz|optimi.*inference|tensorrt|onnx"
    delegate_to: model-optimization
    context: "Model optimization for deployment"

  - trigger: "fine.?tun|lora|qlora|peft|adapt"
    delegate_to: llm-fine-tuning
    context: "Fine-tuning transformer models"

  - trigger: "rlhf|reward.*model|ppo|alignment"
    delegate_to: reinforcement-learning
    context: "RLHF alignment training"

  - trigger: "ner|relation.*extract|entity|nlp.*pipeline"
    delegate_to: nlp-advanced
    context: "NLP information extraction"

  - trigger: "object.*detect|segment|yolo|sam|vision"
    delegate_to: computer-vision-deep
    context: "Vision transformer applications"

feedback_loops:
  receives_feedback_from:
    - skill: distributed-training
      signal: "Memory constraints from sharding"
      action: "Adjust model architecture or use gradient checkpointing"

    - skill: model-optimization
      signal: "Quantization accuracy loss"
      action: "Consider QAT or adjust architecture for quantization-friendliness"

  sends_feedback_to:
    - skill: llm-fine-tuning
      signal: "Base model architecture details"
      action: "Use appropriate LoRA target modules"

    - skill: model-optimization
      signal: "Attention pattern information"
      action: "Enable FlashAttention or sparse attention optimization"

common_combinations:
  - name: LLM Training Pipeline
    skills:
      - transformer-architecture
      - distributed-training
      - llm-fine-tuning
    workflow: |
      1. Design transformer architecture (layers, heads, dim)
      2. Set up distributed training (FSDP/DeepSpeed)
      3. Pre-train or continue training
      4. Fine-tune with LoRA/QLoRA
      5. Evaluate and deploy

  - name: Transformer for Vision
    skills:
      - transformer-architecture
      - computer-vision-deep
    workflow: |
      1. Adapt transformer for vision (ViT patches)
      2. Add task-specific heads
      3. Train with appropriate augmentations
      4. Optimize for inference

  - name: Efficient Inference Deployment
    skills:
      - transformer-architecture
      - model-optimization
    workflow: |
      1. Analyze model architecture
      2. Enable FlashAttention/GQA
      3. Quantize to INT8/INT4
      4. Export to ONNX/TensorRT
      5. Benchmark and validate

ecosystem:
  model_libraries:
    - "HuggingFace Transformers - Pre-trained models"
    - "vLLM - High-throughput inference"
    - "TGI - Text Generation Inference"

  training_tools:
    - "PyTorch - Core framework"
    - "DeepSpeed - Distributed training"
    - "Megatron-LM - Large-scale training"

  efficiency:
    - "FlashAttention - Memory-efficient attention"
    - "xformers - Optimized transformers"
    - "bitsandbytes - Quantization"

references:
  architecture:
    - "Attention Is All You Need (2017)"
    - "GPT-3: Language Models are Few-Shot Learners"
    - "LLaMA: Open and Efficient Foundation Language Models"

  efficiency:
    - "FlashAttention: Fast and Memory-Efficient Attention"
    - "GQA: Training Generalized Multi-Query Transformers"
