id: nlp-advanced-sharp-edges
skill: nlp-advanced
version: 1.0.0

sharp_edges:

  - id: subword-label-mismatch
    severity: critical
    title: "NER Labels Don't Align with Subword Tokens"
    summary: "BERT tokenization breaks word boundaries"
    symptoms:
      - "Training crashes with shape mismatch"
      - "Entity boundaries are wrong"
      - "Model predicts partial words as entities"
    why: |
      BERT tokenizes "Cupertino" as ["Cup", "##ert", "##ino"].
      If you have one label for "Cupertino", it doesn't match 3 tokens.
      Training fails or learns wrong alignments.
    gotcha: |
      # Word-level labels don't match subword tokens
      words = ["Tim", "Cook", "works", "at", "Apple"]
      labels = ["B-PER", "I-PER", "O", "O", "B-ORG"]

      tokens = tokenizer(words, is_split_into_words=True)
      # tokens: ["Tim", "Cook", "works", "at", "App", "##le"]
      # labels: 5 labels for 6 tokens - MISMATCH
    solution: |
      def tokenize_and_align_labels(examples, tokenizer):
          tokenized = tokenizer(
              examples["tokens"],
              truncation=True,
              is_split_into_words=True,
          )

          labels = []
          for i, label in enumerate(examples["ner_tags"]):
              word_ids = tokenized.word_ids(batch_index=i)
              label_ids = []
              previous_word_idx = None

              for word_idx in word_ids:
                  if word_idx is None:
                      label_ids.append(-100)  # Special tokens - ignore
                  elif word_idx != previous_word_idx:
                      label_ids.append(label[word_idx])  # First token of word
                  else:
                      label_ids.append(-100)  # Subword - ignore in loss

                  previous_word_idx = word_idx

              labels.append(label_ids)

          tokenized["labels"] = labels
          return tokenized

  - id: entity-boundary-errors
    severity: high
    title: "NER Finds Partial or Extended Entities"
    summary: "Entity boundaries don't match actual mentions"
    symptoms:
      - "Entities include extra words"
      - "Entities cut off early"
      - "Different boundaries in train vs inference"
    why: |
      Entity boundaries depend on tokenization.
      If training uses word-level and inference uses character-level,
      boundaries won't match.
    gotcha: |
      # Training with word tokenization
      train_entities = extract_entities_word_level(text)

      # Inference with character offsets
      pred_entities = model.predict(text)  # Character spans

      # "New York City" might become "New York" or "York City"
    solution: |
      # Consistent span extraction
      def extract_spans_consistent(text, entities):
          spans = []
          for ent in entities:
              # Always use character offsets
              start_char = ent['start_char']
              end_char = ent['end_char']

              # Validate span matches text
              extracted = text[start_char:end_char]
              assert extracted == ent['text'], f"Mismatch: {extracted} vs {ent['text']}"

              spans.append({
                  'text': extracted,
                  'start': start_char,
                  'end': end_char,
                  'label': ent['label'],
              })

          return spans

  - id: relation-no-negatives
    severity: high
    title: "Relation Model Predicts Everything as Related"
    summary: "No negative examples in training"
    symptoms:
      - "Model predicts relations between unrelated entities"
      - "High recall, terrible precision"
      - "Random entity pairs get high confidence"
    why: |
      Relation extraction is often imbalanced.
      If you only train on positive (related) pairs,
      model never learns to say "no relation".
    gotcha: |
      # Only positive pairs
      train_data = [
          (e1, e2, "works_at") for e1, e2 in positive_pairs
      ]  # No "no_relation" examples!

      # Model predicts everything as related
    solution: |
      # Add negative sampling
      def create_re_dataset(entities, positive_relations):
          data = []

          # Positive pairs
          for rel in positive_relations:
              data.append({
                  'entity1': rel['subject'],
                  'entity2': rel['object'],
                  'relation': rel['type'],
              })

          # Negative pairs (unrelated entity pairs)
          related_pairs = {(r['subject'], r['object']) for r in positive_relations}

          for e1 in entities:
              for e2 in entities:
                  if e1 != e2 and (e1, e2) not in related_pairs:
                      data.append({
                          'entity1': e1,
                          'entity2': e2,
                          'relation': 'no_relation',
                      })

          # Balance: 1:3 positive to negative ratio
          return balance_dataset(data, ratio=3)

  - id: coref-missing-context
    severity: medium
    title: "Coreference Fails Across Sentences"
    summary: "Context window too short for document-level coref"
    symptoms:
      - "Pronouns not linked to antecedents"
      - "Works in single sentences, fails on paragraphs"
      - "Coreference chains incomplete"
    why: |
      BERT-based models have 512 token limit.
      Document-level coreference needs full document context.
      Truncation breaks long-range dependencies.
    gotcha: |
      # BERT with 512 token limit
      model = AutoModel.from_pretrained("bert-base-uncased")

      # Long document gets truncated
      doc = "..." * 1000  # 1000 tokens
      # Only first 512 tokens processed, later pronouns unresolved
    solution: |
      # Option 1: Use Longformer for long documents
      from transformers import LongformerForTokenClassification

      model = LongformerForTokenClassification.from_pretrained(
          "shtoshni/longformer_coreference_joint"
      )  # 4096 token context

      # Option 2: Sliding window with merging
      def process_long_document(doc, window_size=512, overlap=128):
          chunks = []
          for i in range(0, len(doc), window_size - overlap):
              chunk = doc[i:i + window_size]
              chunks.append((i, chunk))

          # Process each chunk
          chunk_results = [model(chunk) for _, chunk in chunks]

          # Merge results, resolving overlaps
          return merge_coref_chains(chunk_results, overlap)

  - id: kg-entity-duplication
    severity: medium
    title: "Knowledge Graph Has Duplicate Entities"
    summary: "Same entity with different mentions not merged"
    symptoms:
      - "Apple Inc. and Apple are separate nodes"
      - "Graph has more entities than expected"
      - "Relations duplicated across mentions"
    why: |
      NER extracts surface forms, not canonical entities.
      "Apple", "Apple Inc.", "the tech giant" are all Apple.
      Without entity linking/clustering, they're separate.
    gotcha: |
      # Each mention becomes a node
      for entity in entities:
          graph.add_node(entity['text'])  # Creates duplicates

      # "Apple Inc." and "Apple" are separate nodes
    solution: |
      class EntityResolver:
          def __init__(self):
              self.canonical_forms = {}  # surface -> canonical

          def resolve(self, entity_text, entity_type):
              # Normalize
              normalized = entity_text.lower().strip()
              normalized = re.sub(r'\s+(inc|corp|llc|ltd)\.?$', '', normalized)

              # Check if we've seen similar
              if normalized in self.canonical_forms:
                  return self.canonical_forms[normalized]

              # Or use entity linking
              linked = entity_linker.link(entity_text)
              if linked:
                  canonical = linked.wikidata_id
              else:
                  canonical = normalized

              self.canonical_forms[normalized] = canonical
              return canonical

      # Usage
      resolver = EntityResolver()
      for entity in entities:
          canonical_id = resolver.resolve(entity['text'], entity['type'])
          graph.add_node(canonical_id, mentions=[entity['text']])

detection:
  file_patterns:
    - "**/*ner*.py"
    - "**/*entity*.py"
    - "**/*relation*.py"
    - "**/*coref*.py"
    - "**/*knowledge*graph*.py"
    - "**/*nlp*.py"
