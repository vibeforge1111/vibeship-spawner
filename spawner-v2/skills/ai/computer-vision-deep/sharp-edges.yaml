id: computer-vision-deep-sharp-edges
skill: computer-vision-deep
version: 1.0.0

sharp_edges:

  - id: resolution-small-objects
    severity: critical
    title: "Missing Small Object Detections"
    summary: "Input resolution too low for small objects"
    symptoms:
      - "Small objects not detected"
      - "Works on large objects, fails on small"
      - "Recall drops dramatically for small objects"
    why: |
      Object detectors need minimum feature map size to detect.
      At 640x640, a 10x10 pixel object becomes 1x1 on final feature map.
      Below threshold, object becomes undetectable.
    gotcha: |
      # Default resolution misses small objects
      model = YOLO('yolov8n.pt')
      results = model(image, imgsz=640)  # 10px objects invisible

      # Looking for defects, small products, distant objects
    solution: |
      # Higher resolution for small objects
      results = model(image, imgsz=1280)  # 2x resolution

      # Or use tiled inference
      from sahi import AutoDetectionModel, get_sliced_prediction

      detection_model = AutoDetectionModel.from_pretrained(
          model_type="yolov8",
          model_path="yolov8n.pt",
      )

      result = get_sliced_prediction(
          image,
          detection_model,
          slice_height=640,
          slice_width=640,
          overlap_height_ratio=0.2,
          overlap_width_ratio=0.2,
      )

  - id: augmentation-mask-mismatch
    severity: critical
    title: "Segmentation Masks Don't Match Augmented Images"
    summary: "Geometric augmentations not applied to masks"
    symptoms:
      - "Masks are offset from objects"
      - "Training loss doesn't decrease"
      - "Model predicts wrong regions"
    why: |
      Geometric augmentations (rotation, flip, crop) change pixel locations.
      If mask isn't transformed identically, supervision is wrong.
      Model learns to predict wrong regions.
    gotcha: |
      # Augmenting only the image
      augmented_image = A.RandomRotate90()(image=image)['image']
      # mask is NOT rotated!

      model.train(image=augmented_image, mask=mask)  # Mask misaligned
    solution: |
      import albumentations as A

      # Apply SAME transform to both
      transform = A.Compose([
          A.RandomRotate90(p=0.5),
          A.HorizontalFlip(p=0.5),
          A.RandomResizedCrop(512, 512, scale=(0.5, 1.0)),
      ])

      # Transform together
      transformed = transform(image=image, mask=mask)
      augmented_image = transformed['image']
      augmented_mask = transformed['mask']

  - id: nms-threshold-wrong
    severity: high
    title: "Duplicate or Missing Detections"
    summary: "NMS threshold not tuned for use case"
    symptoms:
      - "Multiple boxes on same object"
      - "Overlapping objects miss detections"
      - "Confidence seems wrong"
    why: |
      NMS (Non-Maximum Suppression) removes duplicate boxes.
      Too high IoU threshold: duplicates remain.
      Too low: overlapping objects get suppressed.
    gotcha: |
      # Default IoU threshold
      results = model(image, iou=0.7)  # Default

      # For crowded scenes, correct box suppressed
      # For sparse scenes, duplicates remain
    solution: |
      # Tune for your use case
      results = model(
          image,
          conf=0.25,  # Confidence threshold
          iou=0.45,   # IoU for NMS (lower = more suppression)
      )

      # For crowded scenes (pedestrians, products)
      iou=0.5  # Higher threshold, keep overlapping boxes

      # For sparse scenes (vehicles, large objects)
      iou=0.3  # Lower threshold, aggressive duplicate removal

      # Alternative: Soft-NMS for crowded scenes
      # Reduces score instead of removing

  - id: sam-encoding-slow
    severity: medium
    title: "SAM Inference Very Slow"
    summary: "Re-encoding image for every prompt"
    symptoms:
      - "Interactive segmentation is slow"
      - "Batch segmentation takes forever"
      - "GPU utilization spiky"
    why: |
      SAM has two stages: image encoding and mask decoding.
      Image encoding is expensive (~150ms on GPU).
      If you re-encode for each prompt, it's 150ms Ã— N prompts.
    gotcha: |
      for box in bounding_boxes:
          # Re-encodes image every iteration!
          masks = predictor.predict(image=image, box=box)
    solution: |
      # Encode once, decode many
      predictor = SamPredictor(sam)
      predictor.set_image(image)  # Encode once (~150ms)

      for box in bounding_boxes:
          # Only decode (~10ms each)
          masks, scores, _ = predictor.predict(box=box)

      # For batch processing
      from segment_anything import SamPredictor

      predictor.set_image(image)
      all_masks = []

      for prompt in prompts:
          mask, _, _ = predictor.predict(**prompt)
          all_masks.append(mask)

  - id: video-tracking-id-swap
    severity: medium
    title: "Tracking IDs Switch Between Objects"
    summary: "Similar objects swap identities"
    symptoms:
      - "Person A becomes Person B mid-video"
      - "Objects crossing paths swap IDs"
      - "Consistent tracking breaks on occlusion"
    why: |
      Tracking algorithms rely on appearance + motion.
      When objects look similar and cross paths,
      the tracker can confuse their identities.
    gotcha: |
      # Basic tracking fails on similar objects
      results = model.track(video_path, tracker="bytetrack.yaml")

      # When two similar people cross paths,
      # IDs may swap
    solution: |
      # Use more robust tracker
      results = model.track(
          video_path,
          tracker="botsort.yaml",  # Better re-ID
          persist=True,
      )

      # Or use appearance-based re-identification
      # Add ReID model for feature matching

      # Tune tracking parameters
      # botsort.yaml:
      # track_high_thresh: 0.5  # Higher for fewer false positives
      # track_low_thresh: 0.1
      # match_thresh: 0.8
      # new_track_thresh: 0.6

  - id: depth-scale-ambiguity
    severity: medium
    title: "Monocular Depth Has Wrong Scale"
    summary: "Relative depth, not metric depth"
    symptoms:
      - "Depth values don't match real distances"
      - "Scale changes between frames"
      - "Can't use for measurement"
    why: |
      Single-image depth estimation is inherently ambiguous.
      A toy car close up looks like a real car far away.
      Most models output relative depth, not metric.
    gotcha: |
      depth = depth_model(image)

      # depth is relative (0-1 range typically)
      # NOT actual meters

      distance = depth[y, x]  # This is NOT 5.2 meters!
    solution: |
      # Option 1: Use metric depth models
      from transformers import pipeline

      pipe = pipeline("depth-estimation", model="LiheYoung/depth-anything-large-hf")
      # Still relative, but better calibrated

      # Option 2: Calibrate with known reference
      known_distance = 10.0  # meters
      known_depth_value = depth[ref_y, ref_x]
      scale = known_distance / known_depth_value

      metric_depth = depth * scale

      # Option 3: Use stereo or structured light
      # For actual metric depth, need multiple views

detection:
  file_patterns:
    - "**/*detect*.py"
    - "**/*segment*.py"
    - "**/*yolo*.py"
    - "**/*sam*.py"
    - "**/*vision*.py"
    - "**/*depth*.py"
