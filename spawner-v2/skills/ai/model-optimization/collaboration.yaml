id: model-optimization-collaboration
skill: model-optimization
version: 1.0.0

receives_from:
  - skill: distributed-training
    context: "Trained model to optimize"
    receives:
      - "Model checkpoints"
      - "Training configuration"
    provides: "Optimized deployment-ready model"

  - skill: llm-fine-tuning
    context: "Fine-tuned model to deploy"
    receives:
      - "LoRA/merged weights"
      - "Model architecture"
    provides: "Quantized inference model"

  - skill: transformer-architecture
    context: "Model design for optimization"
    receives:
      - "Architecture specifications"
      - "Layer configurations"
    provides: "Optimization-friendly architecture advice"

delegation_triggers:
  - trigger: "train|fine.?tun|adapt"
    delegate_to: llm-fine-tuning
    context: "Training before optimization"

  - trigger: "distributed|multi.*gpu|scale"
    delegate_to: distributed-training
    context: "Distributed training setup"

  - trigger: "transformer|attention|layer.*design"
    delegate_to: transformer-architecture
    context: "Architecture modifications"

  - trigger: "inference.*server|deploy.*prod|serve"
    delegate_to: backend
    context: "Deployment infrastructure"

feedback_loops:
  receives_feedback_from:
    - skill: backend
      signal: "Latency/memory constraints"
      action: "Choose appropriate optimization level"

    - skill: transformer-architecture
      signal: "Architecture constraints"
      action: "Select compatible optimization techniques"

  sends_feedback_to:
    - skill: distributed-training
      signal: "Checkpoint format requirements"
      action: "Save in optimization-friendly format"

    - skill: llm-fine-tuning
      signal: "Quantization-aware training need"
      action: "Apply QLoRA during fine-tuning"

common_combinations:
  - name: Full Optimization Pipeline
    skills:
      - llm-fine-tuning
      - model-optimization
    workflow: |
      1. Fine-tune base model (LoRA/QLoRA)
      2. Merge adapter weights
      3. Apply quantization (GPTQ/AWQ)
      4. Export to ONNX
      5. Optimize with TensorRT
      6. Benchmark on target hardware

  - name: Knowledge Distillation
    skills:
      - transformer-architecture
      - model-optimization
    workflow: |
      1. Design student architecture
      2. Train student with teacher outputs
      3. Quantize student model
      4. Deploy lightweight model

  - name: Edge Deployment
    skills:
      - model-optimization
      - computer-vision-deep
    workflow: |
      1. Train/select vision model
      2. Structured pruning for size
      3. INT8 quantization
      4. Export to TensorRT/Core ML
      5. Deploy to edge device

ecosystem:
  quantization_libraries:
    - "PyTorch Quantization"
    - "bitsandbytes"
    - "GPTQ"
    - "AWQ"
    - "llama.cpp"

  export_formats:
    - "ONNX"
    - "TensorRT"
    - "OpenVINO"
    - "Core ML"
    - "TFLite"

  inference_engines:
    - "ONNX Runtime"
    - "TensorRT"
    - "vLLM"
    - "TGI (Text Generation Inference)"
    - "llama.cpp"

references:
  tutorials:
    - "PyTorch Quantization Tutorial"
    - "ONNX Runtime Optimization"
    - "TensorRT Best Practices"

  papers:
    - "GPTQ: Accurate Post-Training Quantization"
    - "AWQ: Activation-aware Weight Quantization"
    - "Knowledge Distillation: A Survey"
