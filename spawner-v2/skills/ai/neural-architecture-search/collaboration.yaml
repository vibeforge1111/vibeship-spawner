id: neural-architecture-search-collaboration
skill: neural-architecture-search
version: 1.0.0

receives_from:
  - skill: transformer-architecture
    context: "Architecture constraints and patterns"
    receives:
      - "Valid transformer configurations"
      - "Known good hyperparameter ranges"
    provides: "Searched optimal architecture"

  - skill: distributed-training
    context: "Distributed evaluation"
    receives:
      - "Multi-GPU training setup"
      - "Parallel trial execution"
    provides: "Architecture evaluation workload"

delegation_triggers:
  - trigger: "transformer|attention|positional"
    delegate_to: transformer-architecture
    context: "Transformer-specific search space"

  - trigger: "distributed|multi.*gpu|parallel.*train"
    delegate_to: distributed-training
    context: "Distributed NAS execution"

  - trigger: "quantiz|deploy|latency.*optim"
    delegate_to: model-optimization
    context: "Hardware-aware NAS"

  - trigger: "fine.?tun|adapt|transfer"
    delegate_to: llm-fine-tuning
    context: "Fine-tuning found architecture"

feedback_loops:
  receives_feedback_from:
    - skill: model-optimization
      signal: "Deployment constraints (latency, memory)"
      action: "Add hardware constraints to search objective"

    - skill: distributed-training
      signal: "Training efficiency feedback"
      action: "Filter out architectures that don't parallelize well"

  sends_feedback_to:
    - skill: transformer-architecture
      signal: "Optimal architecture found"
      action: "Update architecture design patterns"

    - skill: model-optimization
      signal: "Architecture for optimization"
      action: "Apply quantization/pruning to found model"

common_combinations:
  - name: Hardware-Aware NAS
    skills:
      - neural-architecture-search
      - model-optimization
    workflow: |
      1. Define hardware constraints (latency, memory)
      2. Add latency predictor to objective
      3. Search with multi-objective optimization
      4. Pareto-optimal architectures
      5. Deploy to target hardware

  - name: Distributed NAS
    skills:
      - neural-architecture-search
      - distributed-training
    workflow: |
      1. Set up Ray or Optuna distributed
      2. Parallel trial execution across GPUs
      3. Shared experiment database
      4. Aggregate results

  - name: Transformer NAS
    skills:
      - neural-architecture-search
      - transformer-architecture
    workflow: |
      1. Define transformer search space
      2. Constraints from architecture knowledge
      3. Search depth, width, heads
      4. Evaluate on target task
      5. Retrain best from scratch

ecosystem:
  optimization_frameworks:
    - "Optuna - Modern hyperparameter optimization"
    - "Ray Tune - Distributed tuning"
    - "Hyperopt - Bayesian optimization"

  nas_specific:
    - "NNI - Microsoft NAS toolkit"
    - "Auto-PyTorch - Automated deep learning"
    - "BOHB - Bayesian + Hyperband"

  tracking:
    - "Weights & Biases - Experiment tracking"
    - "MLflow - ML lifecycle platform"
    - "Neptune - Experiment management"

references:
  surveys:
    - "Neural Architecture Search: A Survey (2019)"
    - "A Survey on Neural Architecture Search (2021)"

  methods:
    - "DARTS: Differentiable Architecture Search"
    - "Hyperband: Efficient Hyperparameter Optimization"
    - "ProxylessNAS: Direct Neural Architecture Search"
