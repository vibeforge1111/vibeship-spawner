id: distributed-training-sharp-edges
skill: distributed-training
version: 1.0.0

sharp_edges:

  - id: gradient-sync-deadlock
    severity: critical
    title: "Distributed Training Hangs on Gradient Sync"
    summary: "One process waits forever for others that diverged"
    symptoms:
      - "Training hangs indefinitely"
      - "No error message, just frozen"
      - "GPU utilization drops to 0%"
      - "NCCL timeout errors after long wait"
    why: |
      All processes must call the same operations in the same order.
      If one process takes a different code path (e.g., skips a batch),
      other processes wait forever for a synchronization that never comes.
    gotcha: |
      for batch in dataloader:
          if batch['size'] < min_batch_size:
              continue  # WRONG: Other processes don't skip!

          loss = model(batch)
          loss.backward()  # Hangs - other processes at different batch
    solution: |
      # All processes must make same decisions
      # Use distributed-aware filtering

      # Option 1: Filter before distributed sampler
      filtered_dataset = [x for x in dataset if valid(x)]

      # Option 2: Pad/mask instead of skip
      if batch['size'] < min_batch_size:
          # Pad to min_batch_size instead of skipping
          batch = pad_batch(batch)

      # Option 3: Collective decision
      should_skip = batch['size'] < min_batch_size
      all_should_skip = dist.all_reduce(should_skip, op=dist.ReduceOp.MAX)
      if all_should_skip:
          continue  # All processes skip together

  - id: fsdp-cpu-offload-oom
    severity: high
    title: "FSDP CPU Offload Still OOMs"
    summary: "CPU offload doesn't help if params don't fit during forward"
    symptoms:
      - "OOM during forward pass despite CPU offload"
      - "Memory spikes when gathering parameters"
      - "Offload seems to have no effect"
    why: |
      FSDP CPU offload stores parameters on CPU between forward/backward.
      But during forward, parameters must be gathered to GPU.
      If the full layer doesn't fit, you still OOM.
    gotcha: |
      # CPU offload enabled but model still OOMs
      fsdp_model = FSDP(
          model,
          cpu_offload=CPUOffload(offload_params=True),
      )
      # OOM on first forward - layer too large!
    solution: |
      # 1. Use finer-grained wrapping
      # Wrap individual layers, not whole model
      for layer in model.layers:
          FSDP(layer, cpu_offload=CPUOffload(offload_params=True))

      # 2. Combine with activation checkpointing
      model.gradient_checkpointing_enable()

      # 3. Consider DeepSpeed with NVMe offload
      # For truly massive models

      # 4. Use model parallelism for huge layers
      # Some layers must be split across GPUs

  - id: mixed-precision-loss-scaling
    severity: high
    title: "fp16 Training Produces NaN/Inf"
    summary: "Gradient underflow/overflow without proper loss scaling"
    symptoms:
      - "Loss becomes NaN after some steps"
      - "Gradients are all zeros"
      - "Model parameters become inf"
    why: |
      fp16 has limited dynamic range (5.96e-8 to 65504).
      Small gradients underflow to zero.
      Without loss scaling, training fails silently.
    gotcha: |
      # fp16 without loss scaling
      with torch.autocast(dtype=torch.float16):
          loss = model(batch)

      loss.backward()  # Gradients may underflow!
      optimizer.step()
    solution: |
      # Option 1: Use bf16 (no scaling needed, wider range)
      with torch.autocast(dtype=torch.bfloat16):
          loss = model(batch)
      loss.backward()
      optimizer.step()

      # Option 2: fp16 with GradScaler
      from torch.cuda.amp import GradScaler
      scaler = GradScaler()

      with torch.autocast(dtype=torch.float16):
          loss = model(batch)

      scaler.scale(loss).backward()
      scaler.step(optimizer)
      scaler.update()

  - id: distributed-sampler-shuffle
    severity: medium
    title: "Same Data Order Every Epoch"
    summary: "Forgetting set_epoch() on DistributedSampler"
    symptoms:
      - "Model sees same batch order every epoch"
      - "Validation metrics oscillate without improving"
      - "Training seems to plateau early"
    why: |
      DistributedSampler uses epoch as random seed.
      Without set_epoch(), same shuffling every epoch.
      Model overfits to the order, not the data.
    gotcha: |
      train_sampler = DistributedSampler(dataset)
      train_loader = DataLoader(dataset, sampler=train_sampler)

      for epoch in range(epochs):
          for batch in train_loader:  # Same order every epoch!
              train(batch)
    solution: |
      train_sampler = DistributedSampler(dataset)
      train_loader = DataLoader(dataset, sampler=train_sampler)

      for epoch in range(epochs):
          train_sampler.set_epoch(epoch)  # Different shuffle each epoch!

          for batch in train_loader:
              train(batch)

  - id: checkpoint-rank-mismatch
    severity: medium
    title: "Loading Checkpoint on Wrong Number of GPUs"
    summary: "Sharded checkpoint doesn't match current world size"
    symptoms:
      - "Checkpoint loading fails"
      - "Shape mismatch errors"
      - "Missing or extra keys in state dict"
    why: |
      FSDP/DeepSpeed shard checkpoints across processes.
      Loading on different number of GPUs requires resharding.
      Naive torch.load() doesn't handle this.
    gotcha: |
      # Saved on 8 GPUs, loading on 4
      state_dict = torch.load("checkpoint.pt")
      model.load_state_dict(state_dict)  # Shape mismatch!
    solution: |
      # Use FSDP's checkpoint utilities
      from torch.distributed.checkpoint import load_state_dict, FileSystemReader

      # Load with resharding
      state_dict = {"model": model.state_dict()}
      load_state_dict(
          state_dict=state_dict,
          storage_reader=FileSystemReader("checkpoint_dir"),
      )
      model.load_state_dict(state_dict["model"])

      # Or: Save full state dict for portability
      # FSDP: use_orig_params=True, state_dict_type=FULL_STATE_DICT

detection:
  file_patterns:
    - "**/*distributed*.py"
    - "**/*train*.py"
    - "**/*fsdp*.py"
    - "**/*deepspeed*.py"
