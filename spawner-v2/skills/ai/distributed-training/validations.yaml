validations:

  - id: no-distributed-sampler
    name: DataLoader Without DistributedSampler
    severity: error
    type: regex
    pattern:
      - "DataLoader\\((?!.*DistributedSampler|.*sampler)"
      - "DataLoader\\(.*shuffle\\s*=\\s*True(?!.*DistributedSampler)"
    message: "Distributed training requires DistributedSampler for proper data splitting."
    fix_action: "Add: sampler=DistributedSampler(dataset)"
    applies_to:
      - "**/*train*.py"

  - id: missing-set-epoch
    name: DistributedSampler Without set_epoch()
    severity: warning
    type: regex
    pattern:
      - "DistributedSampler(?!.*set_epoch)"
    message: "Call set_epoch() each epoch for proper shuffling."
    fix_action: "Add: train_sampler.set_epoch(epoch)"
    applies_to:
      - "**/*train*.py"

  - id: no-gradient-checkpointing
    name: Large Model Without Gradient Checkpointing
    severity: info
    type: regex
    pattern:
      - "FSDP\\((?!.*gradient_checkpointing)"
      - "AutoModelForCausalLM(?!.*gradient_checkpointing_enable)"
    message: "Large models benefit from gradient checkpointing for memory efficiency."
    fix_action: "Add: model.gradient_checkpointing_enable()"
    applies_to:
      - "**/*.py"

  - id: no-mixed-precision
    name: Distributed Training Without Mixed Precision
    severity: warning
    type: regex
    pattern:
      - "DDP\\((?!.*MixedPrecision|.*autocast)"
      - "FSDP\\((?!.*mixed_precision)"
    message: "Mixed precision (bf16/fp16) provides 2x memory savings."
    fix_action: "Add: mixed_precision=MixedPrecision(param_dtype=torch.bfloat16)"
    applies_to:
      - "**/*train*.py"

  - id: no-pin-memory
    name: DataLoader Without pin_memory
    severity: info
    type: regex
    pattern:
      - "DataLoader\\((?!.*pin_memory\\s*=\\s*True)"
    message: "pin_memory=True speeds up CPU to GPU data transfer."
    fix_action: "Add: pin_memory=True"
    applies_to:
      - "**/*train*.py"

  - id: gloo-backend-gpu
    name: Using Gloo Backend for GPU Training
    severity: warning
    type: regex
    pattern:
      - "init_process_group.*backend\\s*=\\s*['\"]gloo['\"].*cuda"
    message: "Use NCCL backend for GPU training (faster collectives)."
    fix_action: "Change: backend='nccl'"
    applies_to:
      - "**/*.py"

  - id: dataparallel-not-ddp
    name: Using DataParallel Instead of DDP
    severity: warning
    type: regex
    pattern:
      - "nn\\.DataParallel\\("
      - "torch\\.nn\\.DataParallel"
    message: "DDP is faster and more scalable than DataParallel."
    fix_action: "Use: DistributedDataParallel instead"
    applies_to:
      - "**/*.py"
