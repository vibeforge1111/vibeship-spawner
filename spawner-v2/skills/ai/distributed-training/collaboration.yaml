id: distributed-training-collaboration
skill: distributed-training
version: 1.0.0

receives_from:
  - skill: transformer-architecture
    context: "Model to train"
    receives:
      - "Model architecture and size"
      - "Memory requirements"
    provides: "Distributed training configuration"

  - skill: llm-fine-tuning
    context: "Fine-tuning workload"
    receives:
      - "LoRA/QLoRA configuration"
      - "Training data"
    provides: "Distributed fine-tuning infrastructure"

delegation_triggers:
  - trigger: "transformer|model.*architecture"
    delegate_to: transformer-architecture
    context: "Model design"

  - trigger: "fine.?tun|lora|qlora|adapt"
    delegate_to: llm-fine-tuning
    context: "Fine-tuning strategy"

  - trigger: "quantiz|optim.*inference|deploy"
    delegate_to: model-optimization
    context: "Post-training optimization"

  - trigger: "rlhf|reward|alignment"
    delegate_to: reinforcement-learning
    context: "Distributed RLHF"

feedback_loops:
  receives_feedback_from:
    - skill: transformer-architecture
      signal: "Model size and memory requirements"
      action: "Choose FSDP sharding strategy"

    - skill: model-optimization
      signal: "Deployment constraints"
      action: "Save checkpoint format for optimization"

  sends_feedback_to:
    - skill: transformer-architecture
      signal: "Training bottlenecks"
      action: "Adjust model for better parallelization"

    - skill: llm-fine-tuning
      signal: "Distributed config"
      action: "Use for fine-tuning setup"

common_combinations:
  - name: Large Model Pre-training
    skills:
      - transformer-architecture
      - distributed-training
    workflow: |
      1. Design model architecture
      2. Calculate memory requirements
      3. Choose FSDP vs DeepSpeed
      4. Configure sharding and offload
      5. Set up multi-node training
      6. Train with proper checkpointing

  - name: Distributed Fine-tuning
    skills:
      - distributed-training
      - llm-fine-tuning
    workflow: |
      1. Load base model with FSDP
      2. Apply LoRA adapters
      3. Configure gradient accumulation
      4. Train across GPUs
      5. Merge and save

  - name: Distributed RLHF
    skills:
      - distributed-training
      - reinforcement-learning
    workflow: |
      1. Distribute policy model
      2. Distribute reward model
      3. Parallel experience collection
      4. Synchronized PPO updates

ecosystem:
  distributed_frameworks:
    - "PyTorch DDP/FSDP"
    - "DeepSpeed"
    - "Megatron-LM"
    - "ColossalAI"

  launchers:
    - "torchrun - PyTorch native"
    - "deepspeed - DeepSpeed launcher"
    - "accelerate launch - HuggingFace"

  orchestration:
    - "SLURM - HPC clusters"
    - "Kubernetes - Cloud native"
    - "Ray - Python-native"

references:
  tutorials:
    - "PyTorch FSDP Tutorial"
    - "DeepSpeed Getting Started"
    - "HuggingFace Accelerate Guide"

  papers:
    - "ZeRO: Memory Optimizations"
    - "Megatron-LM: Training Multi-Billion Parameter Models"
