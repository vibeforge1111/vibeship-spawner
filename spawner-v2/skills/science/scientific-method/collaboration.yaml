id: scientific-method-collaboration
skill: scientific-method
version: 1.0.0

# ============================================================================
# RECEIVES FROM (Who delegates TO this skill)
# ============================================================================
receives_from:
  - skill: ml-ops
    context: "Need rigorous experimental methodology for ML experiments"
    receives:
      - "Experiment hypothesis"
      - "Baseline metrics"
      - "Evaluation requirements"
    provides: "Experimental design with proper controls and reproducibility"

  - skill: data-pipeline
    context: "Need to validate data analysis methodology"
    receives:
      - "Analysis requirements"
      - "Data characteristics"
      - "Business questions"
    provides: "Statistically rigorous analysis framework"

  - skill: backend
    context: "Need to run A/B tests or experiments"
    receives:
      - "Feature flags setup"
      - "User segments"
      - "Success metrics"
    provides: "Experimental design, sample size, and analysis plan"

  - skill: performance-profiling
    context: "Need to statistically compare performance results"
    receives:
      - "Benchmark data"
      - "Multiple runs"
      - "Comparison requirements"
    provides: "Statistical comparison framework and significance testing"

# ============================================================================
# DELEGATION TRIGGERS
# ============================================================================
delegation_triggers:
  - trigger: "statistics|p-value|hypothesis test"
    delegate_to: statistical-analysis
    pattern: sequential
    context: "Need detailed statistical analysis"
    handoff_data:
      - "Experimental data"
      - "Analysis plan"
      - "Multiple comparison context"
    receive: "Statistical results with effect sizes and CIs"

  - trigger: "reproducible|docker|environment"
    delegate_to: data-reproducibility
    pattern: parallel
    context: "Need reproducible computational environment"
    handoff_data:
      - "Dependencies"
      - "Random seeds"
      - "Data versions"
    receive: "Reproducibility infrastructure"

  - trigger: "paper|publication|writing"
    delegate_to: research-paper-writing
    pattern: sequential
    context: "Ready to write up research findings"
    handoff_data:
      - "Results"
      - "Figures"
      - "Methods section"
    receive: "Publication-ready manuscript"

  - trigger: "grant|funding|NIH|NSF"
    delegate_to: grant-writing
    pattern: sequential
    context: "Need to write research proposal"
    handoff_data:
      - "Research hypothesis"
      - "Preliminary data"
      - "Timeline"
    receive: "Grant proposal draft"

  - trigger: "literature|citation|systematic"
    delegate_to: literature-review
    pattern: parallel
    context: "Need comprehensive background research"
    handoff_data:
      - "Research question"
      - "Key terms"
      - "Inclusion criteria"
    receive: "Literature synthesis"

  - trigger: "design of experiments|DOE|factorial"
    delegate_to: experimental-design
    pattern: sequential
    context: "Need formal experimental design"
    handoff_data:
      - "Factors to test"
      - "Constraints"
      - "Power requirements"
    receive: "Optimal experimental design"

# ============================================================================
# FEEDBACK LOOPS
# ============================================================================
feedback_loops:
  receives_feedback_from:
    - skill: statistical-analysis
      signal: "Effect size too small to detect with current N"
      action: "Increase sample size or accept lower power"

    - skill: statistical-analysis
      signal: "Multiple comparisons need correction"
      action: "Apply Bonferroni or FDR adjustment"

    - skill: data-reproducibility
      signal: "Results differ across runs"
      action: "Check random seed coverage, document hardware"

    - skill: literature-review
      signal: "Similar study already exists"
      action: "Differentiate hypothesis or build on prior work"

    - skill: ml-ops
      signal: "Model performance varies significantly"
      action: "Increase number of runs, report variance"

  sends_feedback_to:
    - skill: backend
      signal: "A/B test sample size insufficient"
      action: "Extend test duration or increase traffic allocation"

    - skill: ml-ops
      signal: "Experiment design has confounds"
      action: "Revise train/test split or add controls"

    - skill: data-pipeline
      signal: "Analysis assumes normality but data is skewed"
      action: "Use non-parametric methods or transform data"

    - skill: performance-profiling
      signal: "Single benchmark run insufficient"
      action: "Run multiple iterations with statistical analysis"

# ============================================================================
# CROSS-DOMAIN INSIGHTS
# ============================================================================
cross_domain_insights:
  - domain: Clinical Research
    insight: |
      Clinical researchers developed rigorous trial methodology:
      - Randomization prevents selection bias
      - Blinding prevents expectation effects
      - ITT analysis preserves randomization
      - Pre-registration prevents outcome switching

      These principles apply to ALL experimental research.
    applies_when: "Designing any comparative experiment"

  - domain: Physics
    insight: |
      Particle physics requires 5-sigma (p < 0.0000003) for discovery.
      They learned from false discoveries with p < 0.05.

      For important claims, 0.05 is NOT enough.
      Consider:
      - Replication before major claims
      - Multiple independent methods
      - Sensitivity analyses
    applies_when: "Making strong claims from statistical evidence"

  - domain: Psychology (Replication Crisis)
    insight: |
      Psychology's replication crisis revealed:
      - 97% of studies claimed significant results
      - Only ~40% replicated
      - Causes: p-hacking, HARKing, publication bias

      The entire field reformed methodology.
      Learn from their mistakes.
    applies_when: "Publishing or trusting research findings"

  - domain: Metascience
    insight: |
      Meta-researchers found:
      - Studies with p-values just below 0.05 rarely replicate
      - Effect sizes shrink dramatically in replications
      - Registered reports have higher replication rates

      Pre-registration is the single most effective reform.
    applies_when: "Planning research for publication"

  - domain: Software Engineering
    insight: |
      Software performance experiments have unique challenges:
      - CPU frequency scaling affects timing
      - JIT compilation affects early measurements
      - Garbage collection causes variance
      - Cloud VMs have noisy neighbors

      Use warmup periods, isolated environments, many runs.
    applies_when: "Benchmarking or performance experiments"

# ============================================================================
# COMMON COMBINATIONS
# ============================================================================
common_combinations:
  - name: Research Project Full Cycle
    skills:
      - scientific-method
      - literature-review
      - statistical-analysis
      - data-reproducibility
      - research-paper-writing
    workflow: |
      1. Literature review to identify gaps (literature-review)
      2. Formulate hypothesis and design (scientific-method)
      3. Pre-register study (scientific-method)
      4. Set up reproducible environment (data-reproducibility)
      5. Collect data (data-pipeline if computational)
      6. Analyze with proper statistics (statistical-analysis)
      7. Write up findings (research-paper-writing)

  - name: ML Experiment Pipeline
    skills:
      - scientific-method
      - ml-ops
      - statistical-analysis
      - data-reproducibility
    workflow: |
      1. Define hypothesis and metrics (scientific-method)
      2. Calculate required sample/runs (scientific-method)
      3. Set up experiment tracking (ml-ops)
      4. Ensure reproducibility (data-reproducibility)
      5. Run experiments with proper controls (ml-ops)
      6. Statistical analysis of results (statistical-analysis)
      7. Document and share (data-reproducibility)

  - name: A/B Testing Suite
    skills:
      - scientific-method
      - backend
      - statistical-analysis
    workflow: |
      1. Define hypothesis and primary metric (scientific-method)
      2. Power analysis for sample size (scientific-method)
      3. Implement feature flags (backend)
      4. Run test for calculated duration (backend)
      5. Analyze with proper tests (statistical-analysis)
      6. Document decision and learnings (scientific-method)

  - name: Benchmarking Study
    skills:
      - scientific-method
      - performance-profiling
      - statistical-analysis
    workflow: |
      1. Define comparison hypothesis (scientific-method)
      2. Design controlled benchmark (scientific-method)
      3. Run with proper warmup and iterations (performance-profiling)
      4. Statistical comparison of results (statistical-analysis)
      5. Report effect sizes and CIs (scientific-method)

  - name: Grant-Funded Research
    skills:
      - scientific-method
      - grant-writing
      - literature-review
      - experimental-design
    workflow: |
      1. Literature review for positioning (literature-review)
      2. Design rigorous methodology (scientific-method)
      3. Create detailed experimental plan (experimental-design)
      4. Write compelling proposal (grant-writing)
      5. Submit pre-registration (scientific-method)

# ============================================================================
# ECOSYSTEM
# ============================================================================
ecosystem:
  statistical_software:
    - name: R / RStudio
      use_when: "Primary statistical analysis"
      features: "Comprehensive stats packages, reproducible notebooks"

    - name: Python scipy/statsmodels
      use_when: "Statistical analysis in Python workflows"
      features: "Integration with ML pipelines, power analysis"

    - name: G*Power
      use_when: "Power analysis and sample size calculation"
      features: "Visual interface, many test types"

    - name: JASP
      use_when: "Bayesian statistics with GUI"
      features: "Free, open source, Bayesian and frequentist"

  pre_registration:
    - name: OSF (Open Science Framework)
      use_when: "Academic research pre-registration"
      features: "Time-stamped, public or embargoed, data storage"

    - name: AsPredicted
      use_when: "Quick pre-registration for experiments"
      features: "Simple form, 8 questions, PDF output"

    - name: ClinicalTrials.gov
      use_when: "Clinical research (required for FDA)"
      features: "Regulatory compliance, public registry"

  reproducibility:
    - name: Docker
      use_when: "Exact environment reproduction"
      features: "Container isolation, version pinning"

    - name: DVC (Data Version Control)
      use_when: "Versioning datasets and models"
      features: "Git-like for data, cloud storage"

    - name: MLflow
      use_when: "ML experiment tracking"
      features: "Metrics, artifacts, model registry"

    - name: Jupyter + Binder
      use_when: "Sharing reproducible notebooks"
      features: "Web-based reproduction, zero setup"

  collaboration:
    - name: Overleaf
      use_when: "Collaborative paper writing"
      features: "Real-time LaTeX, version history"

    - name: Zotero / Mendeley
      use_when: "Reference management"
      features: "Citation management, collaboration"

    - name: GitHub
      use_when: "Code and analysis collaboration"
      features: "Version control, issues, code review"
