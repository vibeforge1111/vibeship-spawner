# Collaboration - GCP Cloud Run
# How this skill works with other skills

version: 1.0.0
skill_id: gcp-cloud-run

prerequisites:
  required: []

  recommended:
    - skill: backend
      reason: "API design and error handling patterns"
      what_to_know:
        - "REST API design"
        - "Error handling strategies"
        - "Database patterns"

    - skill: devops
      reason: "CI/CD, monitoring, infrastructure"
      what_to_know:
        - "Cloud Build"
        - "Artifact Registry"
        - "Monitoring and alerting"

delegation_triggers:
  - trigger: "user needs AWS serverless"
    delegate_to: aws-serverless
    context: "Lambda, API Gateway, SAM"

  - trigger: "user needs Azure containers"
    delegate_to: azure-functions
    context: "Azure Container Apps, Functions"

  - trigger: "user needs database design"
    delegate_to: postgres-wizard
    context: "Cloud SQL design, AlloyDB"

  - trigger: "user needs authentication"
    delegate_to: auth-specialist
    context: "Firebase Auth, Identity Platform"

  - trigger: "user needs AI integration"
    delegate_to: llm-architect
    context: "Vertex AI, Cloud Run + LLM"

  - trigger: "user needs workflow orchestration"
    delegate_to: workflow-automation
    context: "Cloud Workflows, Eventarc"

receives_context_from:
  - skill: backend
    receives:
      - "API design patterns"
      - "Error handling strategies"
      - "Data modeling"

  - skill: devops
    receives:
      - "Infrastructure as code patterns"
      - "CI/CD best practices"
      - "Monitoring setup"

  - skill: auth-specialist
    receives:
      - "Identity Platform integration"
      - "IAP configuration"
      - "Service-to-service auth"

provides_context_to:
  - skill: aws-serverless
    provides:
      - "Container patterns"
      - "Concurrency strategies"
      - "Cold start optimization"

  - skill: azure-functions
    provides:
      - "Serverless container patterns"
      - "Auto-scaling strategies"
      - "Event-driven architecture"

  - skill: workflow-automation
    provides:
      - "Event sources"
      - "Pub/Sub integration"
      - "Service invocation patterns"

escalation_paths:
  - situation: "Complex GKE requirements"
    escalate_to: devops
    context: "Kubernetes, GKE Autopilot, Anthos"

  - situation: "Database design for Cloud SQL"
    escalate_to: postgres-wizard
    context: "Cloud SQL Postgres, AlloyDB, connection pooling"

  - situation: "Multi-cloud requirements"
    escalate_to: aws-serverless
    context: "Cross-cloud patterns, vendor abstraction"

  - situation: "AI/ML model serving"
    escalate_to: llm-architect
    context: "Vertex AI, model deployment, GPU"

workflow_integration:
  typical_sequence:
    1:
      step: "Create Cloud Run service"
      skills: [gcp-cloud-run]
      output: "Dockerfile, service configuration"

    2:
      step: "Set up Cloud Build pipeline"
      skills: [gcp-cloud-run, devops]
      output: "cloudbuild.yaml, triggers"

    3:
      step: "Configure Cloud SQL"
      skills: [gcp-cloud-run, postgres-wizard]
      output: "Database connection, Cloud SQL Proxy"

    4:
      step: "Add Secret Manager"
      skills: [gcp-cloud-run]
      output: "Secrets configuration"

    5:
      step: "Set up Pub/Sub integration"
      skills: [gcp-cloud-run]
      output: "Event-driven triggers"

    6:
      step: "Configure identity and auth"
      skills: [gcp-cloud-run, auth-specialist]
      output: "IAM, Identity Platform"

    7:
      step: "Set up monitoring"
      skills: [gcp-cloud-run, devops]
      output: "Cloud Monitoring, alerts"

  decision_points:
    - question: "Cloud Run services vs Cloud Run functions?"
      guidance: |
        Cloud Run services (container-based):
        - Full Dockerfile control
        - Any language/framework
        - Persistent connections
        - Multiple endpoints
        - Best for: APIs, web apps

        Cloud Run functions (source-based):
        - Automatic containerization
        - Event-driven by design
        - Simpler deployment
        - Single function per deployment
        - Best for: Webhooks, event processing

    - question: "Cloud Run vs Cloud Functions (2nd gen)?"
      guidance: |
        Cloud Run:
        - Full container control
        - Up to 60 min timeout
        - Concurrency up to 1000
        - VPC, GPU support
        - Best for: Long-running, complex apps

        Cloud Functions (2nd gen):
        - Built on Cloud Run
        - Source-based deployment
        - Native event triggers
        - Simpler for small functions
        - Best for: Event handlers, simple APIs

    - question: "When to use GPU?"
      guidance: |
        Cloud Run with GPU (L4):
        - ML model inference
        - Real-time AI applications
        - Video/image processing
        - LLM serving

        Requirements:
        - Second-gen execution environment
        - Specific regions only
        - Higher cost

        ```bash
        gcloud run deploy ml-service \
          --gpu=1 \
          --gpu-type=nvidia-l4 \
          --execution-environment=gen2
        ```

collaboration_patterns:
  with_cloud_sql:
    when: "Need relational database"
    approach: |
      Cloud SQL with Unix socket connection:

      ```yaml
      # cloudbuild.yaml deployment
      steps:
        - name: 'gcr.io/cloud-builders/gcloud'
          args:
            - 'run'
            - 'deploy'
            - 'my-service'
            - '--add-cloudsql-instances=${PROJECT_ID}:${REGION}:${INSTANCE}'
            - '--set-env-vars=INSTANCE_CONNECTION_NAME=${PROJECT_ID}:${REGION}:${INSTANCE}'
      ```

      ```python
      # Python with Cloud SQL Connector
      from google.cloud.sql.connector import Connector
      import sqlalchemy

      connector = Connector()

      def getconn():
          return connector.connect(
              os.environ["INSTANCE_CONNECTION_NAME"],
              "pg8000",
              user=os.environ["DB_USER"],
              password=os.environ["DB_PASSWORD"],
              db=os.environ["DB_NAME"]
          )

      engine = sqlalchemy.create_engine(
          "postgresql+pg8000://",
          creator=getconn,
          pool_size=5,
          max_overflow=2,
          pool_recycle=300
      )
      ```

  with_pubsub:
    when: "Event-driven processing"
    approach: |
      Pub/Sub push subscription to Cloud Run:

      ```bash
      # Create subscription that pushes to Cloud Run
      gcloud pubsub subscriptions create my-subscription \
        --topic=my-topic \
        --push-endpoint=https://my-service-xxx.run.app/events \
        --push-auth-service-account=cloud-run-invoker@project.iam.gserviceaccount.com
      ```

      ```python
      from fastapi import FastAPI, Request
      import base64
      import json

      app = FastAPI()

      @app.post("/events")
      async def handle_pubsub(request: Request):
          envelope = await request.json()
          message = envelope.get("message", {})

          if message:
              data = base64.b64decode(message.get("data", "")).decode()
              payload = json.loads(data)
              await process_event(payload)

          return {"status": "ok"}
      ```

  with_cloud_tasks:
    when: "Reliable async processing, long-running tasks"
    approach: |
      Cloud Tasks for guaranteed delivery:

      ```python
      from google.cloud import tasks_v2
      from google.protobuf import timestamp_pb2
      import json
      from datetime import datetime, timedelta

      def create_task(payload: dict, delay_seconds: int = 0):
          client = tasks_v2.CloudTasksClient()
          parent = client.queue_path(PROJECT_ID, REGION, QUEUE_NAME)

          task = {
              "http_request": {
                  "http_method": tasks_v2.HttpMethod.POST,
                  "url": f"https://{SERVICE_URL}/tasks",
                  "body": json.dumps(payload).encode(),
                  "headers": {"Content-Type": "application/json"},
                  "oidc_token": {
                      "service_account_email": SERVICE_ACCOUNT
                  }
              }
          }

          if delay_seconds > 0:
              schedule_time = timestamp_pb2.Timestamp()
              schedule_time.FromDatetime(
                  datetime.utcnow() + timedelta(seconds=delay_seconds)
              )
              task["schedule_time"] = schedule_time

          return client.create_task(parent=parent, task=task)
      ```

  with_eventarc:
    when: "React to GCP events (Cloud Storage, BigQuery, etc.)"
    approach: |
      Eventarc trigger for Cloud Storage events:

      ```bash
      # Trigger Cloud Run on new Cloud Storage objects
      gcloud eventarc triggers create storage-trigger \
        --destination-run-service=my-service \
        --destination-run-region=us-central1 \
        --event-filters="type=google.cloud.storage.object.v1.finalized" \
        --event-filters="bucket=my-bucket" \
        --service-account=my-sa@project.iam.gserviceaccount.com
      ```

      ```python
      from cloudevents.http import from_http
      from fastapi import FastAPI, Request

      app = FastAPI()

      @app.post("/")
      async def handle_event(request: Request):
          event = from_http(dict(request.headers), await request.body())

          # CloudEvent attributes
          event_type = event["type"]
          subject = event["subject"]

          # Event data
          data = event.data
          bucket = data["bucket"]
          name = data["name"]

          await process_file(bucket, name)
          return {"status": "processed"}
      ```

  with_vertex_ai:
    when: "AI/ML model serving"
    approach: |
      Cloud Run calling Vertex AI:

      ```python
      from google.cloud import aiplatform
      from fastapi import FastAPI

      app = FastAPI()

      # Initialize once
      aiplatform.init(project=PROJECT_ID, location=REGION)

      @app.post("/predict")
      async def predict(data: dict):
          endpoint = aiplatform.Endpoint(ENDPOINT_ID)

          response = endpoint.predict(
              instances=[data["instances"]]
          )

          return {
              "predictions": response.predictions
          }
      ```

      Or deploy model directly to Cloud Run:

      ```dockerfile
      FROM python:3.11-slim

      # Install model serving dependencies
      RUN pip install fastapi uvicorn transformers torch

      COPY model/ /app/model/
      COPY main.py /app/

      WORKDIR /app
      CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8080"]
      ```

platform_integration:
  cloud_build:
    setup: |
      # cloudbuild.yaml
      steps:
        # Build
        - name: 'gcr.io/cloud-builders/docker'
          args: ['build', '-t', 'gcr.io/$PROJECT_ID/my-service:$COMMIT_SHA', '.']

        # Push
        - name: 'gcr.io/cloud-builders/docker'
          args: ['push', 'gcr.io/$PROJECT_ID/my-service:$COMMIT_SHA']

        # Deploy
        - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
          entrypoint: gcloud
          args:
            - 'run'
            - 'deploy'
            - 'my-service'
            - '--image=gcr.io/$PROJECT_ID/my-service:$COMMIT_SHA'
            - '--region=us-central1'
            - '--allow-unauthenticated'

      # Trigger configuration
      trigger:
        branch:
          name: 'main'

  github_actions:
    setup: |
      # .github/workflows/deploy.yml
      name: Deploy to Cloud Run

      on:
        push:
          branches: [main]

      jobs:
        deploy:
          runs-on: ubuntu-latest

          permissions:
            contents: read
            id-token: write

          steps:
            - uses: actions/checkout@v4

            - uses: google-github-actions/auth@v2
              with:
                workload_identity_provider: ${{ secrets.WIF_PROVIDER }}
                service_account: ${{ secrets.SERVICE_ACCOUNT }}

            - uses: google-github-actions/setup-gcloud@v2

            - name: Build and push
              run: |
                gcloud builds submit --tag gcr.io/$PROJECT_ID/my-service

            - name: Deploy
              run: |
                gcloud run deploy my-service \
                  --image gcr.io/$PROJECT_ID/my-service \
                  --region us-central1

  terraform:
    setup: |
      # main.tf
      resource "google_cloud_run_v2_service" "default" {
        name     = "my-service"
        location = "us-central1"

        template {
          containers {
            image = "gcr.io/${var.project_id}/my-service"

            resources {
              limits = {
                cpu    = "1"
                memory = "512Mi"
              }
            }

            env {
              name  = "LOG_LEVEL"
              value = "info"
            }

            env {
              name = "DB_PASSWORD"
              value_source {
                secret_key_ref {
                  secret  = google_secret_manager_secret.db_password.secret_id
                  version = "latest"
                }
              }
            }
          }

          scaling {
            min_instance_count = 0
            max_instance_count = 100
          }

          vpc_access {
            connector = google_vpc_access_connector.connector.id
            egress    = "PRIVATE_RANGES_ONLY"
          }
        }

        traffic {
          type    = "TRAFFIC_TARGET_ALLOCATION_TYPE_LATEST"
          percent = 100
        }
      }

      resource "google_cloud_run_service_iam_member" "public" {
        location = google_cloud_run_v2_service.default.location
        service  = google_cloud_run_v2_service.default.name
        role     = "roles/run.invoker"
        member   = "allUsers"
      }

cost_optimization:
  - "Use min-instances=0 for dev/staging environments"
  - "Right-size memory (you're charged for memory allocated)"
  - "Use CPU always-on only when necessary (costs more)"
  - "Set appropriate concurrency to reduce instance count"
  - "Use committed use discounts for steady workloads"
  - "Consider Cloud Functions for simple event handlers"

security_checklist:
  - "No hardcoded credentials"
  - "Service account with minimum permissions"
  - "Secrets in Secret Manager"
  - "Container runs as non-root"
  - "Ingress settings restrict traffic"
  - "VPC connector for private resources"
  - "Cloud Armor for DDoS protection"
  - "Binary Authorization for image verification"
