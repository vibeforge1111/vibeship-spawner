# Upstash QStash Sharp Edges
# Critical pitfalls for serverless message queue - handle with care

sharp_edges:
  - id: signature-verification-skip
    summary: Not verifying QStash webhook signatures
    severity: critical
    situation: |
      Endpoint accepts any POST request. Attacker discovers your callback URL.
      Fake messages flood your system. Malicious payloads processed as trusted.
    why: |
      QStash endpoints are public URLs. Without signature verification, anyone
      can send requests. This is a direct path to unauthorized message processing
      and potential data manipulation.
    solution: |
      # Always verify signatures with both keys:
      ```typescript
      import { Receiver } from '@upstash/qstash';

      const receiver = new Receiver({
        currentSigningKey: process.env.QSTASH_CURRENT_SIGNING_KEY!,
        nextSigningKey: process.env.QSTASH_NEXT_SIGNING_KEY!,
      });

      export async function POST(req: NextRequest) {
        const signature = req.headers.get('upstash-signature');
        const body = await req.text();  // Raw body required

        const isValid = await receiver.verify({
          signature: signature!,
          body,
          url: req.url,
        });

        if (!isValid) {
          return NextResponse.json({ error: 'Invalid signature' }, { status: 401 });
        }

        // Safe to process
      }
      ```

      # Why two keys?
      - QStash rotates signing keys
      - nextSigningKey becomes current during rotation
      - Both must be checked for seamless key rotation
    symptoms:
      - No Receiver import in webhook handler
      - Missing upstash-signature header check
      - Processing request before verification
    detection_pattern: "Receiver|upstash-signature|verify\\("

  - id: callback-timeout-exceeded
    summary: Callback endpoint taking too long to respond
    severity: high
    situation: |
      Webhook handler does heavy processing. Takes 30+ seconds. QStash times out.
      Marks message as failed. Retries. Double processing begins.
    why: |
      QStash has a 30-second timeout for callbacks. If your endpoint doesn't respond
      in time, QStash considers it failed and retries. Long-running handlers create
      duplicate message processing and wasted retries.
    solution: |
      # Design for fast acknowledgment:
      ```typescript
      export async function POST(req: NextRequest) {
        // 1. Verify signature first (fast)
        // 2. Parse and validate message (fast)
        // 3. Queue for async processing (fast)

        const message = await parseMessage(req);

        // Don't do this:
        // await processHeavyWork(message);  // Could timeout!

        // Do this instead:
        await db.jobs.create({ data: message, status: 'pending' });
        // Or use another QStash message for the heavy work

        return NextResponse.json({ queued: true });  // Respond fast
      }
      ```

      # Alternative: Use QStash for the heavy work
      ```typescript
      // Webhook receives trigger
      await qstash.publishJSON({
        url: 'https://myapp.com/api/heavy-process',
        body: { jobId: message.id },
      });
      return NextResponse.json({ delegated: true });
      ```

      # For Vercel: Consider using Edge runtime for faster cold starts
    symptoms:
      - Webhook timeouts in QStash dashboard
      - Messages marked failed then retried
      - Duplicate processing of same message
    detection_pattern: null

  - id: rate-limit-exhaustion
    summary: Hitting QStash rate limits unexpectedly
    severity: high
    situation: |
      Burst of events triggers mass message publishing. QStash rate limit hit.
      Messages rejected. Users don't get notifications. Critical tasks delayed.
    why: |
      QStash has plan-based rate limits. Free tier: 500 messages/day. Pro: higher
      but still limited. Bursts can exhaust limits quickly. Without monitoring,
      you won't know until users complain.
    solution: |
      # Check your plan limits:
      - Free: 500 messages/day
      - Pay as you go: Check dashboard
      - Pro: Higher limits, check dashboard

      # Implement rate limit handling:
      ```typescript
      try {
        await qstash.publishJSON({ url, body });
      } catch (error) {
        if (error.message?.includes('rate limit')) {
          // Queue locally and retry later
          await localQueue.add('qstash-retry', { url, body });
        }
        throw error;
      }
      ```

      # Batch messages when possible:
      ```typescript
      // Instead of 100 individual publishes
      await qstash.batchJSON({
        messages: items.map(item => ({
          url: 'https://myapp.com/api/process',
          body: { itemId: item.id },
        })),
      });
      ```

      # Monitor in dashboard:
      Upstash Console shows usage and limits
    symptoms:
      - 429 errors from QStash
      - Messages not being delivered
      - Sudden drop in processing during peak times
    detection_pattern: null

  - id: missing-idempotency-key
    summary: Not using deduplication for critical operations
    severity: high
    situation: |
      Network hiccup during publish. SDK retries. Same message sent twice.
      Customer charged twice. Email sent twice. Data corrupted.
    why: |
      Network failures and retries happen. Without deduplication, the same logical
      message can be sent multiple times. QStash provides deduplication, but you
      must use it for critical operations.
    solution: |
      # Use deduplication for critical messages:
      ```typescript
      // Custom ID (best for business operations)
      await qstash.publishJSON({
        url: 'https://myapp.com/api/charge',
        body: { orderId: '123', amount: 5000 },
        deduplicationId: `charge-${orderId}`,  // Same ID = same message
      });

      // Content-based (good for notifications)
      await qstash.publishJSON({
        url: 'https://myapp.com/api/notify',
        body: { userId: '456', type: 'welcome' },
        contentBasedDeduplication: true,  // Hash of body
      });
      ```

      # Deduplication window:
      - Default: 60 seconds
      - Messages with same ID in window are deduplicated
      - Plan for this in your retry logic

      # Also make endpoints idempotent:
      Check if operation already completed before processing
    symptoms:
      - Duplicate charges or emails
      - Double processing of same event
      - User complaints about duplicates
    detection_pattern: "publishJSON\\([^)]*\\)(?!.*deduplicationId)"

  - id: private-endpoint-assumption
    summary: Expecting QStash to reach private/localhost endpoints
    severity: critical
    situation: |
      Development works with local server. Deploy to production with internal URL.
      QStash can't reach it. All messages fail silently. No processing happens.
    why: |
      QStash runs in Upstash's cloud. It can only reach public, internet-accessible
      URLs. localhost, internal IPs, and private networks are unreachable. This is
      a fundamental architecture requirement, not a configuration issue.
    solution: |
      # Production requirements:
      - URL must be publicly accessible
      - HTTPS required (HTTP will fail)
      - No localhost, 127.0.0.1, or private IPs

      # Local development options:

      # Option 1: ngrok/localtunnel
      ```bash
      ngrok http 3000
      # Use the ngrok URL for QStash testing
      ```

      # Option 2: QStash local development mode
      ```typescript
      // In development, skip QStash and call directly
      if (process.env.NODE_ENV === 'development') {
        await fetch('http://localhost:3000/api/process', {
          method: 'POST',
          body: JSON.stringify(data),
        });
      } else {
        await qstash.publishJSON({ url, body: data });
      }
      ```

      # Option 3: Use Vercel preview URLs
      Preview deploys give you public URLs for testing
    symptoms:
      - Messages show "failed" in QStash dashboard
      - Works locally but fails in "production"
      - Using http:// instead of https://
    detection_pattern: "localhost|127\\.0\\.0\\.1|http://"

  - id: missing-retry-configuration
    summary: Using default retry behavior for all message types
    severity: medium
    situation: |
      Critical payment webhook uses defaults. 3 retries over minutes. Payment
      processor is temporarily down for 15 minutes. Message marked as failed.
      Payment reconciliation manual work required.
    why: |
      Default retry behavior (3 attempts, short backoff) works for many cases but
      not all. Some endpoints need more attempts, longer backoff, or different
      strategies. One size doesn't fit all.
    solution: |
      # Configure retries per message:
      ```typescript
      // Critical operations: more retries, longer backoff
      await qstash.publishJSON({
        url: 'https://myapp.com/api/payment-webhook',
        body: { paymentId: '123' },
        retries: 5,
        // Backoff: 10s, 30s, 1m, 5m, 30m
      });

      // Non-critical notifications: fewer retries
      await qstash.publishJSON({
        url: 'https://myapp.com/api/analytics',
        body: { event: 'pageview' },
        retries: 1,  // Fail fast, not critical
      });
      ```

      # Consider your endpoint's recovery time:
      - Database down: May need 5+ minutes
      - Third-party API: May need hours
      - Internal service: Usually quick

      # Use failure callbacks for dead letter handling:
      ```typescript
      await qstash.publishJSON({
        url: 'https://myapp.com/api/critical',
        body: data,
        failureCallback: 'https://myapp.com/api/dead-letter',
      });
      ```
    symptoms:
      - Critical messages marked failed
      - Manual intervention needed for retries
      - Temporary outages causing permanent failures
    detection_pattern: null

  - id: large-message-body
    summary: Sending large payloads instead of references
    severity: medium
    situation: |
      Message contains entire document (5MB). QStash rejects - body too large.
      Even if accepted, slow to transmit. Expensive. Wastes bandwidth.
    why: |
      QStash has message size limits (around 500KB body). Large payloads slow
      delivery, increase costs, and can fail entirely. Messages should be
      lightweight triggers, not data carriers.
    solution: |
      # Send references, not data:
      ```typescript
      // BAD: Large payload
      await qstash.publishJSON({
        url: 'https://myapp.com/api/process',
        body: { document: largeDocumentContent },  // 5MB!
      });

      // GOOD: Reference only
      await qstash.publishJSON({
        url: 'https://myapp.com/api/process',
        body: { documentId: 'doc_123' },  // Fetch in handler
      });
      ```

      # In your handler:
      ```typescript
      export async function POST(req: NextRequest) {
        const { documentId } = await req.json();
        const document = await storage.get(documentId);  // Fetch actual data
        await processDocument(document);
      }
      ```

      # Large data storage options:
      - S3/R2/Blob storage for files
      - Database for structured data
      - Redis for temporary data (Upstash Redis pairs well)
    symptoms:
      - Message publish failures
      - Slow message delivery
      - High bandwidth costs
    detection_pattern: null

  - id: callback-response-ignored
    summary: Not using callback/failureCallback for critical flows
    severity: medium
    situation: |
      Important task published. QStash delivers. Endpoint processes. But your
      system doesn't know it succeeded. User stuck waiting. No feedback loop.
    why: |
      QStash is fire-and-forget by default. Without callbacks, you don't know
      if messages were delivered successfully. For critical flows, you need
      the feedback loop to update state and handle failures.
    solution: |
      # Use callbacks for critical operations:
      ```typescript
      await qstash.publishJSON({
        url: 'https://myapp.com/api/send-email',
        body: { userId: '123', template: 'welcome' },
        callback: 'https://myapp.com/api/email-callback',
        failureCallback: 'https://myapp.com/api/email-failed',
      });
      ```

      # Handle the callback:
      ```typescript
      // app/api/email-callback/route.ts
      export async function POST(req: NextRequest) {
        // Verify signature first!
        const data = await req.json();

        // data.sourceMessageId - original message
        // data.status - HTTP status code
        // data.body - response from endpoint

        await db.emailLogs.update({
          where: { messageId: data.sourceMessageId },
          data: { status: 'delivered' },
        });

        return NextResponse.json({ received: true });
      }
      ```

      # Failure callback for alerting:
      ```typescript
      // app/api/email-failed/route.ts
      export async function POST(req: NextRequest) {
        const data = await req.json();
        await alerting.notify(`Email failed: ${data.sourceMessageId}`);
        await db.emailLogs.update({
          where: { messageId: data.sourceMessageId },
          data: { status: 'failed', error: data.body },
        });
      }
      ```
    symptoms:
      - No visibility into message delivery
      - Users waiting for actions that completed
      - No alerting on failures
    detection_pattern: null

  - id: schedule-timezone-confusion
    summary: Cron schedules using wrong timezone
    severity: medium
    situation: |
      Scheduled daily report at "9am". But 9am in which timezone? QStash uses UTC.
      Report runs at 4am local time. Users confused. Support tickets filed.
    why: |
      QStash cron schedules run in UTC. If you think in local time but configure
      in UTC, schedules will run at unexpected times. This is especially tricky
      with daylight saving time changes.
    solution: |
      # QStash uses UTC:
      ```typescript
      // This runs at 9am UTC, not local time
      await qstash.schedules.create({
        destination: 'https://myapp.com/api/daily-report',
        cron: '0 9 * * *',  // 9am UTC
      });
      ```

      # Convert to UTC:
      - 9am EST = 2pm UTC (winter) / 1pm UTC (summer)
      - 9am PST = 5pm UTC (winter) / 4pm UTC (summer)

      # Document timezone in schedule name:
      ```typescript
      await qstash.schedules.create({
        destination: 'https://myapp.com/api/daily-report',
        cron: '0 14 * * *',  // 9am EST (14:00 UTC)
        body: JSON.stringify({
          timezone: 'America/New_York',
          localTime: '9:00 AM',
        }),
      });
      ```

      # Handle DST programmatically if needed:
      Update schedules when DST changes, or accept UTC timing
    symptoms:
      - Schedules running at unexpected times
      - Off-by-one-hour issues during DST
      - User complaints about report timing
    detection_pattern: null

  - id: url-group-stale-endpoints
    summary: URL groups with dead or outdated endpoints
    severity: medium
    situation: |
      URL group has 5 endpoints. One service deprecated months ago. Messages
      still fan out to it. Failures in dashboard. Wasted attempts. Slower delivery.
    why: |
      URL groups persist until explicitly updated. When services change, endpoints
      become stale. QStash tries to deliver to dead URLs, wastes retries, and
      the failure noise obscures real issues.
    solution: |
      # Audit URL groups regularly:
      ```typescript
      const groups = await qstash.urlGroups.list();
      for (const group of groups) {
        console.log(`Group: ${group.name}`);
        for (const endpoint of group.endpoints) {
          // Check if endpoint is still valid
          try {
            await fetch(endpoint.url, { method: 'HEAD' });
            console.log(`  OK: ${endpoint.url}`);
          } catch {
            console.log(`  DEAD: ${endpoint.url}`);
          }
        }
      }
      ```

      # Update groups when services change:
      ```typescript
      // Remove dead endpoint
      await qstash.urlGroups.removeEndpoints({
        name: 'order-processors',
        endpoints: [{ url: 'https://old-service.myapp.com/api/process' }],
      });
      ```

      # Automate in CI/CD:
      Check URL group health as part of deployment
    symptoms:
      - Failed deliveries in URL groups
      - Messages to deprecated services
      - Slow fan-out due to timeouts
    detection_pattern: null
