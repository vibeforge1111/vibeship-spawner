# Collaboration - Agent Evaluation
# How this skill works with other skills

version: 1.0.0
skill_id: agent-evaluation

prerequisites:
  required: []

  recommended:
    - skill: autonomous-agents
      reason: "Understanding agent architecture for testing"
      what_to_know:
        - "Agent loop patterns (ReAct, Plan-Execute)"
        - "State management and checkpointing"
        - "Tool calling patterns"

    - skill: llm-architect
      reason: "LLM evaluation concepts"
      what_to_know:
        - "Prompt evaluation"
        - "Model selection for judges"
        - "Token and cost tracking"

delegation_triggers:
  - trigger: "user needs load testing"
    delegate_to: devops
    context: "Performance and scale testing"

  - trigger: "user needs security testing"
    delegate_to: security-specialist
    context: "Adversarial testing, injection attacks"

  - trigger: "user needs RAG evaluation"
    delegate_to: llm-architect
    context: "Retrieval quality metrics (ragas)"

  - trigger: "user needs accessibility testing"
    delegate_to: accessibility-specialist
    context: "Accessibility compliance testing"

  - trigger: "user needs UI testing"
    delegate_to: frontend
    context: "Agent UI/UX testing"

receives_context_from:
  - skill: autonomous-agents
    receives:
      - "Agent implementation to evaluate"
      - "Expected behaviors and constraints"
      - "Tool specifications"

  - skill: agent-tool-builder
    receives:
      - "Tool schemas for validation"
      - "Expected tool call patterns"
      - "Error handling expectations"

  - skill: product-strategy
    receives:
      - "Success criteria from user perspective"
      - "Quality expectations"
      - "Deployment requirements"

  - skill: agent-memory-systems
    receives:
      - "Memory retrieval to evaluate"
      - "Context quality expectations"

provides_context_to:
  - skill: autonomous-agents
    provides:
      - "Quality metrics and benchmarks"
      - "Failure modes identified"
      - "Improvement recommendations"

  - skill: devops
    provides:
      - "CI/CD integration requirements"
      - "Deployment gates"
      - "Monitoring requirements"

  - skill: product-strategy
    provides:
      - "Agent performance reports"
      - "User satisfaction metrics"
      - "Quality trends"

escalation_paths:
  - situation: "Security vulnerabilities found"
    escalate_to: security-specialist
    context: "Prompt injection, data leaks"

  - situation: "Performance issues"
    escalate_to: devops
    context: "Latency optimization, scaling"

  - situation: "Agent reliability too low"
    escalate_to: autonomous-agents
    context: "Architecture improvements needed"

  - situation: "Tool failures"
    escalate_to: agent-tool-builder
    context: "Tool implementation issues"

  - situation: "Retrieval quality issues"
    escalate_to: agent-memory-systems
    context: "Memory/RAG problems"

workflow_integration:
  typical_sequence:
    1:
      step: "Define success criteria"
      skills: [product-strategy, agent-evaluation]
      output: "Evaluation requirements"

    2:
      step: "Create initial test cases"
      skills: [agent-evaluation]
      output: "Core test dataset"

    3:
      step: "Implement evaluation pipeline"
      skills: [agent-evaluation]
      output: "Working eval suite"

    4:
      step: "Integrate with CI/CD"
      skills: [agent-evaluation, devops]
      output: "Automated evaluation"

    5:
      step: "Run baseline evaluation"
      skills: [agent-evaluation]
      output: "Baseline metrics"

    6:
      step: "Iterate and improve"
      skills: [autonomous-agents, agent-evaluation]
      output: "Improved agent quality"

    7:
      step: "Production monitoring"
      skills: [agent-evaluation, devops]
      output: "Continuous evaluation"

  decision_points:
    - question: "Offline or online evaluation?"
      guidance: |
        Offline (dataset-based):
        - Repeatable, consistent
        - Good for CI/CD
        - Fast iteration
        - May not reflect real usage
        - Use for: regression testing, development

        Online (production):
        - Real user behavior
        - Catches edge cases
        - More expensive
        - Slower feedback
        - Use for: production monitoring, validation

        Recommended: Both
        - Offline for CI/CD gates
        - Online for production monitoring

    - question: "Human or automated evaluation?"
      guidance: |
        Human review:
        - High quality, nuanced judgment
        - Expensive, slow
        - Essential for high-stakes
        - Use for: 10% sample, calibration

        LLM-as-judge:
        - Scalable, fast
        - Has biases (position, length, self)
        - Good for iteration
        - Use for: bulk evaluation, CI/CD

        Recommended: Combination
        - LLM judges for scale
        - Human review for calibration
        - Human override for high-stakes

    - question: "Which evaluation platform?"
      guidance: |
        LangSmith:
        - Best for LangChain/LangGraph
        - Multi-turn evals, insights agent
        - Tracing built-in

        Braintrust:
        - General purpose
        - Good prompt iteration
        - Cost tracking

        Langfuse:
        - Open source, self-hostable
        - Good observability
        - Growing eval features

        Custom:
        - Maximum flexibility
        - More engineering effort
        - Good for specific needs

collaboration_patterns:
  with_agents:
    when: "Evaluating autonomous agents"
    approach: |
      Agent evaluation requires:

      1. Tool call validation:
         - Correct tools called?
         - Right order?
         - Proper arguments?

      2. State management:
         - Checkpoints work?
         - Resume correctly?
         - State consistent?

      3. Multi-turn behavior:
         - Context maintained?
         - Goal achieved?
         - Graceful errors?

      ## Example evaluation
      def evaluate_agent_task(agent, task):
          # Run agent
          result = agent.run(task.input)

          # Check tool usage
          tool_score = evaluate_tools(
              actual=result.tool_calls,
              expected=task.expected_tools
          )

          # Check final output
          output_score = llm_judge.evaluate(
              output=result.output,
              reference=task.expected_output
          )

          # Check trajectory
          trajectory_score = evaluate_trajectory(
              actual=result.steps,
              expected=task.expected_trajectory
          )

          return {
              "tool_score": tool_score,
              "output_score": output_score,
              "trajectory_score": trajectory_score,
              "overall": (tool_score + output_score + trajectory_score) / 3
          }

  with_tools:
    when: "Evaluating tool usage"
    approach: |
      Tool evaluation dimensions:

      1. Correctness:
         - Right tool for task?
         - Correct arguments?
         - Valid output parsing?

      2. Efficiency:
         - Minimal tool calls?
         - No redundant calls?
         - Optimal order?

      3. Error handling:
         - Recovers from failures?
         - Appropriate retries?
         - Graceful fallbacks?

      ## Tool evaluation metrics
      def evaluate_tool_usage(actual_calls, expected_calls):
          metrics = {
              "precision": len(set(actual) & set(expected)) / len(actual),
              "recall": len(set(actual) & set(expected)) / len(expected),
              "order_accuracy": sequence_similarity(actual, expected),
              "argument_accuracy": argument_match_rate(actual, expected),
          }
          return metrics

  with_memory:
    when: "Evaluating memory/RAG quality"
    approach: |
      Memory evaluation dimensions:

      1. Retrieval quality:
         - Relevant memories found?
         - Right context retrieved?

      2. Context usage:
         - Memory used correctly?
         - No hallucination?

      3. Memory formation:
         - Important info stored?
         - Correct categorization?

      ## Memory evaluation with ragas
      from ragas import evaluate
      from ragas.metrics import context_relevancy, answer_relevancy

      def evaluate_memory(agent, test_cases):
          results = []
          for case in test_cases:
              response = agent.query(case.query)
              retrieved = agent.last_retrieved

              result = evaluate(
                  query=case.query,
                  response=response,
                  contexts=retrieved,
                  metrics=[context_relevancy, answer_relevancy]
              )
              results.append(result)

          return aggregate(results)

  with_ci_cd:
    when: "Integrating evaluation into deployment pipeline"
    approach: |
      CI/CD integration patterns:

      1. Pre-commit (fast):
         - Run on local changes
         - <2 minutes
         - 20 core cases

      2. PR merge (standard):
         - Run on pull requests
         - <10 minutes
         - 100 cases, blocks merge

      3. Deployment gate:
         - Run before production
         - Full suite
         - Hard threshold

      4. Post-deployment:
         - Sample production traffic
         - Continuous monitoring
         - Alert on degradation

      ## GitHub Actions example
      name: Agent Evaluation
      on: [push, pull_request]

      jobs:
        fast-eval:
          runs-on: ubuntu-latest
          steps:
            - uses: actions/checkout@v3
            - name: Fast Eval
              run: pytest tests/eval_fast.py --threshold=0.95

        full-eval:
          if: github.event_name == 'pull_request'
          runs-on: ubuntu-latest
          steps:
            - name: Full Eval
              run: pytest tests/eval_full.py --threshold=0.85

platform_integration:
  langsmith:
    setup: |
      pip install langsmith

      from langsmith import Client, evaluate

      client = Client()

      # Create dataset
      dataset = client.create_dataset("agent-eval")
      client.create_examples(
          inputs=[{"query": "..."}],
          outputs=[{"expected": "..."}],
          dataset_id=dataset.id
      )

      # Define evaluator
      def my_evaluator(run, example):
          return {
              "key": "correctness",
              "score": score(run.outputs, example.outputs)
          }

      # Run evaluation
      results = evaluate(
          agent.invoke,
          data="agent-eval",
          evaluators=[my_evaluator]
      )
    considerations:
      - "Multi-turn requires online eval setup"
      - "Insights Agent needs trace volume"
      - "Consider sampling for cost"

  openevals:
    setup: |
      pip install openevals

      from openevals import create_evaluator

      # Quick correctness check
      evaluator = create_evaluator(
          type="labeled_criteria",
          criteria="correctness"
      )

      result = evaluator(
          prediction="The capital is Paris",
          reference="Paris is the capital",
          input="What is the capital of France?"
      )
    considerations:
      - "Good for quick start"
      - "Customize for specific needs"
      - "Combine with agentevals for agents"

  agentevals:
    setup: |
      pip install agentevals

      from agentevals import trajectory_match

      result = trajectory_match(
          actual_trajectory=agent_steps,
          expected_trajectory=expected_steps,
          match_type="semantic"  # or "exact"
      )
    considerations:
      - "Designed for agent trajectories"
      - "Supports tool call validation"
      - "Works with LangSmith"

cost_optimization:
  llm_judge_costs:
    - "Use smaller models (gpt-4o-mini) for simple checks"
    - "Batch evaluations for efficiency"
    - "Cache repeated evaluations"
    - "Sample instead of evaluating everything"

  infrastructure:
    - "Run fast evals locally, full in CI"
    - "Parallelize evaluation runs"
    - "Use spot instances for nightly"

  dataset_management:
    - "Curate quality over quantity"
    - "Deduplicate similar cases"
    - "Archive old test cases"
