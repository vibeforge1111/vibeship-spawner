# Collaboration - Computer Use Agents
# How this skill works with other skills

version: 1.0.0
skill_id: computer-use-agents

prerequisites:
  required: []

  recommended:
    - skill: autonomous-agents
      reason: "Agent loop patterns and state management"
      what_to_know:
        - "ReAct pattern for action selection"
        - "Checkpointing for long-running tasks"
        - "Error recovery strategies"

    - skill: security-specialist
      reason: "Sandboxing and security critical for computer use"
      what_to_know:
        - "Container security best practices"
        - "Prompt injection defenses"
        - "Credential management"

delegation_triggers:
  - trigger: "user needs web-only automation"
    delegate_to: browser-automation
    context: "Playwright/Selenium more efficient for web"

  - trigger: "user needs security review"
    delegate_to: security-specialist
    context: "Review sandboxing, prompt injection defenses"

  - trigger: "user needs container orchestration"
    delegate_to: devops
    context: "Kubernetes, Docker Swarm for scaling"

  - trigger: "user needs vision model optimization"
    delegate_to: llm-architect
    context: "Model selection, prompt engineering"

  - trigger: "user needs multi-agent coordination"
    delegate_to: multi-agent-orchestration
    context: "Multiple computer use agents working together"

receives_context_from:
  - skill: autonomous-agents
    receives:
      - "Agent loop patterns"
      - "State management approaches"
      - "Error handling strategies"

  - skill: browser-automation
    receives:
      - "Playwright/Selenium patterns (for hybrid approach)"
      - "DOM access fallback strategies"
      - "Web-specific best practices"

  - skill: security-specialist
    receives:
      - "Container security requirements"
      - "Prompt injection defense patterns"
      - "Credential management strategies"

  - skill: devops
    receives:
      - "Container orchestration patterns"
      - "Resource management"
      - "Monitoring and alerting"

provides_context_to:
  - skill: workflow-automation
    provides:
      - "Computer use as automation capability"
      - "Desktop app integration patterns"
      - "Visual verification capabilities"

  - skill: agent-evaluation
    provides:
      - "Computer use specific test cases"
      - "Success criteria for UI automation"
      - "Reliability metrics"

  - skill: devops
    provides:
      - "Container requirements"
      - "Resource needs (CPU, memory, GPU)"
      - "Monitoring requirements"

escalation_paths:
  - situation: "Security vulnerability discovered"
    escalate_to: security-specialist
    context: "Prompt injection, sandbox escape"

  - situation: "Infrastructure scaling needed"
    escalate_to: devops
    context: "Container orchestration, resource management"

  - situation: "Complex multi-agent workflows"
    escalate_to: multi-agent-orchestration
    context: "Coordinating multiple computer use agents"

  - situation: "Vision model quality issues"
    escalate_to: llm-architect
    context: "Model selection, prompt optimization"

  - situation: "Web automation failures"
    escalate_to: browser-automation
    context: "DOM-based fallback strategies"

workflow_integration:
  typical_sequence:
    1:
      step: "Define automation requirements"
      skills: [product-strategy, computer-use-agents]
      output: "Task specification, success criteria"

    2:
      step: "Security planning"
      skills: [computer-use-agents, security-specialist]
      output: "Sandbox configuration, security controls"

    3:
      step: "Implement sandbox environment"
      skills: [computer-use-agents, devops]
      output: "Docker container with restrictions"

    4:
      step: "Implement agent logic"
      skills: [computer-use-agents, autonomous-agents]
      output: "Working computer use agent"

    5:
      step: "Add safety controls"
      skills: [computer-use-agents]
      output: "Confirmation gates, logging, cost limits"

    6:
      step: "Test in sandbox"
      skills: [computer-use-agents, agent-evaluation]
      output: "Validated automation"

    7:
      step: "Production deployment"
      skills: [computer-use-agents, devops]
      output: "Deployed with monitoring"

  decision_points:
    - question: "Vision-based or DOM-based?"
      guidance: |
        Vision-based (screenshots):
        - Use for: Desktop apps, visual verification, complex UIs
        - Cost: High (image tokens expensive)
        - Reliability: Medium (UI changes can break)
        - Speed: Slow (2-5x slower than human)

        DOM-based (structured access):
        - Use for: Web automation, form filling, data extraction
        - Cost: Low (text tokens only)
        - Reliability: High (semantic selectors)
        - Speed: Fast (100x faster than vision)

        Hybrid approach:
        - Use DOM for web, fall back to vision when needed
        - Best of both worlds but more complex

    - question: "Which model for computer use?"
      guidance: |
        Claude Opus 4.5 (claude-opus-4-5-...):
        - Best quality for computer use
        - Highest cost (~$15/million input tokens)
        - Use for: Complex tasks, high accuracy needed

        Claude Sonnet 4 (claude-sonnet-4-20250514):
        - Good balance of quality and cost
        - ~$3/million input tokens
        - Use for: Most production tasks

        GPT-4V (OpenAI):
        - Good alternative
        - Similar pricing
        - Use for: OpenAI ecosystem

        Open source (ScreenAgent, etc):
        - Self-hosted, no API cost
        - Lower quality
        - Use for: Cost-sensitive, privacy requirements

    - question: "What sandboxing level?"
      guidance: |
        Development:
        - Docker with basic restrictions
        - Network access for development
        - Logging but no confirmation gates

        Staging:
        - Docker with full restrictions
        - Network allowlist only
        - Confirmation for sensitive actions

        Production:
        - Docker with seccomp, capability drops
        - Strict network isolation
        - All actions logged, high-risk confirmed
        - Cost limits enforced

collaboration_patterns:
  with_browser_automation:
    when: "Hybrid web automation approach"
    approach: |
      Use structured DOM access when possible, fall back to vision:

      ```python
      class HybridWebAgent:
          def __init__(self):
              self.playwright_agent = PlaywrightAgent()
              self.vision_agent = ComputerUseAgent()

          async def execute(self, task: str) -> Result:
              # Try DOM-based first (faster, cheaper)
              try:
                  result = await self.playwright_agent.execute(task)
                  if result.success:
                      return result
              except (SelectorError, ElementNotFoundError):
                  pass

              # Fall back to vision (more flexible)
              return await self.vision_agent.execute(task)
      ```

  with_multi_agent:
    when: "Multiple computer use agents working together"
    approach: |
      Coordinate multiple agents through shared state:

      ```python
      class MultiAgentComputer:
          def __init__(self):
              self.agents = {}
              self.shared_state = SharedState()

          async def spawn_agent(self, agent_id: str, sandbox_config: dict):
              # Each agent gets isolated sandbox
              container = await create_sandbox(sandbox_config)
              self.agents[agent_id] = ComputerUseAgent(container)

          async def coordinate(self, tasks: list[Task]):
              # Distribute tasks
              for task in tasks:
                  agent = self.select_best_agent(task)
                  await agent.execute(task)
                  # Share results via shared state
                  self.shared_state.update(task.id, agent.result)
      ```

  with_workflow_automation:
    when: "Computer use as part of larger workflow"
    approach: |
      Integrate computer use into workflow orchestration:

      ```python
      # n8n/Temporal-style workflow
      @workflow
      async def automated_data_entry():
          # 1. Get data from API
          data = await fetch_data_from_api()

          # 2. Use computer use to enter into legacy system
          for record in data:
              result = await computer_use_agent.execute(
                  f"Enter {record} into the form and submit"
              )
              if not result.success:
                  await notify_human(record, result.error)

          # 3. Verify entries
          verification = await computer_use_agent.execute(
              "Take screenshot of the confirmation page"
          )
          return verification
      ```

platform_integration:
  anthropic_computer_use:
    setup: |
      # Using Anthropic's reference implementation
      git clone https://github.com/anthropics/anthropic-quickstarts
      cd computer-use-demo

      # Build Docker container
      docker build -t computer-use-demo .

      # Run with VNC for observation
      docker run -p 5900:5900 -p 8080:8080 \
          -e ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY \
          computer-use-demo
    considerations:
      - "Beta feature - API may change"
      - "Requires betas=['computer-use-2024-10-22'] flag"
      - "Dropdowns and scrollbars are tricky"
      - "Screenshots accumulate in context"

  openai_operator:
    setup: |
      # Currently requires $200/month subscription
      # API access planned but not yet available

      # When available, expected pattern:
      from openai import OpenAI

      client = OpenAI()

      response = client.computer.run(
          task="Search for weather in NYC",
          browser_only=True  # Operator is browser-focused
      )
    considerations:
      - "Currently limited to Pro/ChatGPT Plus subscribers"
      - "Browser-only, no desktop apps"
      - "Pricing TBD for API access"

  playwright_mcp:
    setup: |
      # MCP server for browser control
      npm install @anthropic/mcp-playwright

      # In Claude Desktop config:
      {
        "mcpServers": {
          "playwright": {
            "command": "npx",
            "args": ["@anthropic/mcp-playwright"]
          }
        }
      }

      # LLM can now control browser via MCP
      # Uses accessibility snapshots, not screenshots
    considerations:
      - "Much faster and cheaper than vision"
      - "Web only, no desktop apps"
      - "Requires MCP-capable client"

  screenagent:
    setup: |
      # Open-source vision agent
      git clone https://github.com/niuzaisheng/ScreenAgent
      cd ScreenAgent

      # Install dependencies
      pip install -r requirements.txt

      # Run with your VLM
      python run_agent.py \
          --model qwen-vl-plus \
          --task "Open Chrome and search for weather"
    considerations:
      - "Self-hosted, no API costs"
      - "Quality depends on VLM choice"
      - "Significant setup required"
      - "Less reliable than commercial options"

cost_optimization:
  screenshot_costs:
    - "Resize to 1280x800 or smaller (50% token savings)"
    - "Use JPEG instead of PNG for non-critical screenshots"
    - "Crop to region of interest when possible"
    - "Don't include screenshot every turn if not needed"

  context_management:
    - "Limit screenshots in context to 10 most recent"
    - "Summarize old screenshots to text"
    - "Use checkpoints to reset context on long tasks"
    - "Break large tasks into smaller subtasks"

  model_selection:
    - "Use Haiku for simple decisions (is task done?)"
    - "Use Sonnet for action selection (most tasks)"
    - "Reserve Opus for complex reasoning"
    - "Consider open-source for cost-sensitive apps"

  infrastructure:
    - "Spin up containers only when needed"
    - "Use spot/preemptible instances for batch tasks"
    - "Cache screenshots for repeated analyses"
    - "Parallelize independent tasks across containers"

testing_patterns:
  unit_testing:
    approach: "Mock screenshot and action execution"
    example: |
      def test_action_parsing():
          response = '{"type": "click", "x": 100, "y": 200}'
          action = parse_action(response)
          assert action.type == "click"
          assert action.x == 100

  integration_testing:
    approach: "Run against headless browser in container"
    example: |
      @pytest.fixture
      def sandbox():
          container = create_test_sandbox()
          yield container
          container.destroy()

      def test_navigation(sandbox):
          agent = ComputerUseAgent(sandbox)
          result = agent.run("Navigate to example.com")
          assert result.success
          assert "example.com" in sandbox.get_url()

  e2e_testing:
    approach: "Full task execution in isolated environment"
    example: |
      @pytest.mark.slow
      def test_full_workflow():
          with create_sandbox() as sandbox:
              agent = ComputerUseAgent(sandbox)
              result = agent.run(
                  "Open Chrome, go to weather.com, "
                  "search for NYC weather"
              )
              assert result.success
              # Verify by screenshot analysis
              screenshot = sandbox.capture()
              assert "New York" in ocr_extract(screenshot)
