id: queue-workers-collaboration
skill: queue-workers
version: 1.0.0

# ============================================================================
# RECEIVES FROM (Who delegates TO this skill)
# ============================================================================
receives_from:
  - skill: backend
    context: "Backend needs async processing"
    receives:
      - "Task types to process"
      - "Processing requirements"
      - "Throughput expectations"
    provides: "Job queue implementation with reliable processing"

  - skill: websocket-realtime
    context: "Real-time needs async message handling"
    receives:
      - "Message types"
      - "Processing requirements"
      - "Delivery guarantees needed"
    provides: "Message queue integration for async operations"

  - skill: api-designer
    context: "API needs background processing"
    receives:
      - "Long-running operations"
      - "Webhook delivery needs"
      - "Async response patterns"
    provides: "Background job processing patterns"

  - skill: microservices-patterns
    context: "Microservices need async communication"
    receives:
      - "Service communication patterns"
      - "Event types"
      - "Delivery requirements"
    provides: "Message queue architecture for service communication"

# ============================================================================
# DELEGATION TRIGGERS
# ============================================================================
delegation_triggers:
  - trigger: "rate limit|throttle|api limit"
    delegate_to: rate-limiting
    pattern: parallel
    context: "Need to rate limit job processing"
    handoff_data:
      - "API rate limits"
      - "Per-tenant limits"
      - "Backoff requirements"
    receive: "Rate limiting strategy for workers"

  - trigger: "redis|cache|connection"
    delegate_to: caching-patterns
    pattern: parallel
    context: "Need Redis configuration for queues"
    handoff_data:
      - "Connection requirements"
      - "Persistence needs"
      - "Cluster requirements"
    receive: "Redis configuration and patterns"

  - trigger: "deploy|kubernetes|docker"
    delegate_to: infrastructure-as-code
    pattern: sequential
    context: "Need to deploy worker infrastructure"
    handoff_data:
      - "Worker count"
      - "Scaling requirements"
      - "Resource limits"
    receive: "Worker deployment configuration"

  - trigger: "monitor|alert|metrics"
    delegate_to: observability-sre
    pattern: parallel
    context: "Need queue monitoring"
    handoff_data:
      - "Key metrics"
      - "Alert thresholds"
      - "Dashboard requirements"
    receive: "Queue monitoring and alerting setup"

  - trigger: "database|persist|store results"
    delegate_to: database-schema-design
    pattern: parallel
    context: "Need to persist job results"
    handoff_data:
      - "Result schema"
      - "Retention requirements"
      - "Query patterns"
    receive: "Database schema for job results"

  - trigger: "schedule|cron|recurring"
    delegate_to: backend
    pattern: parallel
    context: "Need job scheduling"
    handoff_data:
      - "Schedule patterns"
      - "Timezone requirements"
      - "Overlap handling"
    receive: "Job scheduling implementation"

# ============================================================================
# FEEDBACK LOOPS
# ============================================================================
feedback_loops:
  receives_feedback_from:
    - skill: observability-sre
      signal: "Queue depth growing"
      action: "Add more workers, investigate slow jobs, implement backpressure"

    - skill: observability-sre
      signal: "High failure rate"
      action: "Review error patterns, adjust retry strategy, check external deps"

    - skill: performance-optimization
      signal: "Worker memory growing"
      action: "Check for memory leaks, review job size, implement chunking"

    - skill: infrastructure-as-code
      signal: "Worker pods crashing"
      action: "Review resource limits, check graceful shutdown, investigate OOM"

    - skill: rate-limiting
      signal: "External API rate limited"
      action: "Reduce worker concurrency, add rate limiter, implement backoff"

  sends_feedback_to:
    - skill: backend
      signal: "Job payload too large"
      action: "Store payload in database/S3, pass reference in job"

    - skill: backend
      signal: "Job processing slow"
      action: "Consider splitting into smaller jobs, add batching"

    - skill: infrastructure-as-code
      signal: "Need more Redis memory"
      action: "Increase Redis instance size or add job cleanup"

    - skill: observability-sre
      signal: "Queue metrics available"
      action: "Add dashboards for queue depth, processing time, failure rate"

    - skill: database-schema-design
      signal: "Need job history table"
      action: "Design table for completed/failed job tracking"

# ============================================================================
# CROSS-DOMAIN INSIGHTS
# ============================================================================
cross_domain_insights:
  - domain: Distributed Systems
    insight: |
      Distributed systems engineers know queues are deceptively complex:
      - At-least-once is easy, exactly-once is a lie
      - Ordering guarantees have hidden costs
      - Backpressure is essential, not optional
      - Every network partition reveals edge cases

      Message queues are distributed systems problems.
    applies_when: "Building reliable job processing"

  - domain: Site Reliability Engineering
    insight: |
      SREs understand queue failure modes:
      - Queue backing up is the canary for system problems
      - Worker crashes often indicate upstream issues
      - Memory leaks manifest slowly then catastrophically
      - Retry storms can cascade through systems

      Queue health is system health.
    applies_when: "Operating queues in production"

  - domain: Data Engineering
    insight: |
      Data engineers know about job processing at scale:
      - Partitioning is key to horizontal scaling
      - Ordering requirements limit parallelism
      - Backfill jobs need different handling than live jobs
      - Idempotency is non-negotiable

      Data pipelines are specialized queues.
    applies_when: "Processing large volumes of jobs"

  - domain: Payment Systems
    insight: |
      Payment engineers understand critical job requirements:
      - Idempotency keys prevent double charging
      - Retry with backoff handles gateway issues
      - Dead letter queues catch persistent failures
      - Audit logging is mandatory

      Payment jobs require paranoid engineering.
    applies_when: "Processing financial transactions"

# ============================================================================
# COMMON COMBINATIONS
# ============================================================================
common_combinations:
  - name: Email Sending System
    skills:
      - queue-workers
      - backend
      - rate-limiting
    workflow: |
      1. Design job structure (queue-workers)
      2. Implement email service (backend)
      3. Add rate limiting for ESP (rate-limiting)
      4. Set up retry with backoff (queue-workers)
      5. Add dead letter queue (queue-workers)
      6. Monitor delivery rates (observability-sre)

  - name: Image Processing Pipeline
    skills:
      - queue-workers
      - backend
      - caching-patterns
    workflow: |
      1. Design job for image operations (queue-workers)
      2. Implement processing workers (backend)
      3. Cache processed images (caching-patterns)
      4. Handle large files with streaming (backend)
      5. Add progress updates (queue-workers)
      6. Set up cleanup jobs (queue-workers)

  - name: Webhook Delivery System
    skills:
      - queue-workers
      - backend
      - rate-limiting
      - database-schema-design
    workflow: |
      1. Design webhook job structure (queue-workers)
      2. Implement delivery with retries (queue-workers)
      3. Rate limit per destination (rate-limiting)
      4. Store delivery history (database-schema-design)
      5. Handle failures gracefully (queue-workers)
      6. Add DLQ and alerting (queue-workers)

  - name: Order Processing System
    skills:
      - queue-workers
      - database-schema-design
      - backend
      - observability-sre
    workflow: |
      1. Design order workflow (queue-workers)
      2. Implement saga pattern for transactions (queue-workers)
      3. Store order state (database-schema-design)
      4. Add idempotency for payments (queue-workers)
      5. Monitor order throughput (observability-sre)
      6. Handle refunds and compensations (backend)

  - name: Data Sync Pipeline
    skills:
      - queue-workers
      - backend
      - caching-patterns
      - rate-limiting
    workflow: |
      1. Design sync job structure (queue-workers)
      2. Implement incremental sync (backend)
      3. Rate limit API calls (rate-limiting)
      4. Cache sync state (caching-patterns)
      5. Handle conflicts (backend)
      6. Add monitoring (observability-sre)

# ============================================================================
# ECOSYSTEM
# ============================================================================
ecosystem:
  primary_tools:
    - "BullMQ - Redis-based job queue for Node.js"
    - "ioredis - Redis client"
    - "node-cron - Job scheduling"
    - "prom-client - Prometheus metrics"

  queue_libraries:
    - name: BullMQ
      platform: "Node.js"
      use_when: "Production job processing with Redis"
      features: "Retries, delays, priorities, rate limiting, workflows"

    - name: pg-boss
      platform: "Node.js"
      use_when: "PostgreSQL already in stack"
      features: "ACID guarantees, no Redis needed"

    - name: Agenda
      platform: "Node.js"
      use_when: "MongoDB already in stack"
      features: "Job scheduling, persistence"

    - name: Bee-Queue
      platform: "Node.js"
      use_when: "Simple, fast queue needs"
      features: "Lightweight, Redis-based"

  managed_services:
    - name: AWS SQS
      use_when: "AWS ecosystem, serverless"
      features: "Fully managed, DLQ, FIFO queues"

    - name: Google Cloud Tasks
      use_when: "GCP ecosystem"
      features: "HTTP targets, scheduling"

    - name: Azure Service Bus
      use_when: "Azure ecosystem"
      features: "Topics, subscriptions, sessions"

    - name: RabbitMQ Cloud
      use_when: "Need managed RabbitMQ"
      features: "Full AMQP, clustering"

  monitoring_tools:
    - name: Bull Board
      purpose: "BullMQ dashboard"
      features: "Queue status, job management"

    - name: Arena
      purpose: "Bull/BullMQ UI"
      features: "Multiple queue support"

    - name: Grafana + Prometheus
      purpose: "Queue metrics"
      features: "Custom dashboards, alerting"

  infrastructure:
    - name: Redis
      purpose: "Queue storage for BullMQ"
      note: "Use Redis Cluster for high availability"

    - name: RabbitMQ
      purpose: "Message broker"
      note: "Use for complex routing patterns"

    - name: Kubernetes Jobs
      purpose: "Worker deployment"
      note: "Use HPA for auto-scaling workers"


