# Event Architect Sharp Edges
# Battle-tested gotchas from building event-driven systems at scale

sharp_edges:
  - id: event-schema-breaking-change
    summary: Removing or renaming event fields breaks replay
    severity: critical
    situation: |
      You need to change an event schema - maybe rename a field or change its type.
      The "easy" fix is to just update the event class. This breaks everything.
    why: |
      Old events in the store use the old schema. When you replay (for projections,
      debugging, or disaster recovery), deserialization fails. You've corrupted your
      entire event history. Recovery requires downtime and data migration.
    solution: |
      # NEVER remove or rename fields. NEVER change types.

      # Step 1: Add new fields as optional with defaults
      @dataclass
      class MemoryCreatedV1:
          memory_id: UUID
          content: str
          created_at: datetime

      # V2 adds embedding - existing events still work
      @dataclass
      class MemoryCreatedV2:
          memory_id: UUID
          content: str
          created_at: datetime
          embedding: Optional[List[float]] = None  # New, optional

      # Step 2: Use explicit version numbers
      event_type = "MemoryCreated"
      event_version = 2

      # Step 3: Upcasters transform old events to new schema
      def upcast_memory_created(event: dict, version: int) -> dict:
          if version == 1:
              event["embedding"] = None
          return event
    symptoms:
      - "Deserialization error during replay"
      - "KeyError on event field"
      - "Projections stuck at old position"
      - "TypeError: unexpected keyword argument"
    detection_pattern: 'class.*Event.*:\s*\n(?!.*version)'
    version_range: ">=1.0.0"

  - id: nats-ack-timeout-too-short
    summary: Default NATS ack timeout is too short for ML workloads
    severity: critical
    situation: |
      You're using NATS JetStream to process events that trigger ML operations
      (embeddings, LLM calls). Events keep getting redelivered even though
      processing succeeds.
    why: |
      Default ack timeout is 30 seconds. Embedding generation or LLM calls can
      take 60+ seconds. NATS times out, redelivers, and you process the same
      event multiple times. At scale, this creates duplicate entries and
      wastes expensive API calls.
    solution: |
      # Set appropriate ack timeout for ML workloads
      subscription = await js.subscribe(
          "memories.extract",
          durable="memory-extractor",
          ack_wait=120,  # 2 minutes for ML operations
          max_deliver=3,  # Limit retries
      )

      # For long operations, use heartbeat pattern
      async def process_with_heartbeat(msg):
          async def heartbeat():
              while True:
                  await asyncio.sleep(10)
                  await msg.in_progress()  # Reset ack timeout

          heartbeat_task = asyncio.create_task(heartbeat())
          try:
              await expensive_ml_operation(msg.data)
              await msg.ack()
          finally:
              heartbeat_task.cancel()
    symptoms:
      - "Events processed multiple times"
      - "Duplicate entries in database"
      - "High API costs from repeated LLM calls"
      - "NATS logs show 'redelivery' messages"
    detection_pattern: 'subscribe.*(?!ack_wait)'
    version_range: ">=1.0.0"

  - id: projection-not-idempotent
    summary: Projection handlers that aren't safe to replay
    severity: critical
    situation: |
      Your projection works fine normally, but when you rebuild it (deploy new
      version, fix bug, add new projection), data is corrupted or duplicated.
    why: |
      Projections MUST be replayable from position 0. If your handler uses
      INSERT without ON CONFLICT, or increments counters without checking
      event position, replay creates duplicates or wrong counts.
    solution: |
      # BAD: Not idempotent
      async def handle_memory_created(event):
          await db.execute("INSERT INTO memories ...")  # Fails on replay

      # GOOD: Idempotent with upsert
      async def handle_memory_created(event):
          await db.execute(
              """
              INSERT INTO memories (id, content, created_at)
              VALUES ($1, $2, $3)
              ON CONFLICT (id) DO UPDATE SET
                  content = EXCLUDED.content,
                  created_at = EXCLUDED.created_at
              """,
              event.memory_id, event.content, event.created_at
          )

      # For counters, use event-sourced approach
      async def handle_memory_accessed(event):
          await db.execute(
              """
              INSERT INTO memory_access_log (event_id, memory_id, accessed_at)
              VALUES ($1, $2, $3)
              ON CONFLICT (event_id) DO NOTHING
              """,
              event.event_id, event.memory_id, event.accessed_at
          )
          # Derive count from log, don't increment
    symptoms:
      - "Duplicate rows after projection rebuild"
      - "Counts are wrong after replay"
      - "Unique constraint violations during replay"
      - "Fear of rebuilding projections"
    detection_pattern: 'INSERT(?!.*ON CONFLICT)'
    version_range: ">=1.0.0"

  - id: stream-retention-silent-drop
    summary: NATS stream retention limits silently drop events
    severity: high
    situation: |
      You set up a NATS stream with limits (max_msgs, max_bytes, max_age).
      Months later, you try to replay from the beginning and events are missing.
    why: |
      When limits are exceeded, NATS silently drops old events. There's no error,
      no warning. Your event store just lost data. If you depend on complete
      history for projections or audit, you're in trouble.
    solution: |
      # Option 1: Archive to permanent storage
      async def archive_and_delete(js, stream_name):
          # Read events before they're dropped
          consumer = await js.subscribe(stream_name)
          async for msg in consumer.messages:
              # Write to permanent store (S3, DB, etc.)
              await archive_store.write(msg.data)
              await msg.ack()

      # Option 2: Use unlimited retention with external archival
      stream_config = StreamConfig(
          name="memories",
          subjects=["memories.>"],
          retention=RetentionPolicy.LIMITS,
          max_msgs=-1,  # Unlimited
          max_bytes=-1,  # Unlimited
          max_age=0,     # Never expire
          # Use separate archival job to move old events to cold storage
      )

      # Option 3: Monitor and alert
      async def monitor_stream_usage():
          info = await js.stream_info("memories")
          if info.state.messages > threshold:
              alert("Stream approaching limits - archive needed")
    symptoms:
      - "Events missing when replaying from start"
      - "Projection rebuild produces different results"
      - "Gaps in event sequence numbers"
      - "Audit trail incomplete"
    detection_pattern: 'StreamConfig.*max_msgs.*[^-]\\d+'
    version_range: ">=1.0.0"

  - id: event-ordering-assumption
    summary: Assuming events arrive in order across partitions
    severity: high
    situation: |
      You have multiple producers writing to the same event stream. Your handler
      assumes EventA always comes before EventB. Occasionally, logic breaks.
    why: |
      Distributed systems don't guarantee global ordering. Even with Kafka
      partitioning, events from different producers can interleave. If your
      handler assumes sequence, it fails intermittently.
    solution: |
      # Design handlers to handle out-of-order events

      # BAD: Assumes order
      async def handle_event(event):
          if event.type == "MemoryCreated":
              self.memory = event.memory
          elif event.type == "MemoryUpdated":
              self.memory.update(event.changes)  # Fails if Created not received

      # GOOD: Idempotent and order-independent
      async def handle_event(event):
          if event.type == "MemoryCreated":
              await db.execute(
                  "INSERT INTO memories (id, ...) ON CONFLICT DO NOTHING",
                  ...
              )
          elif event.type == "MemoryUpdated":
              await db.execute(
                  """
                  UPDATE memories SET ...
                  WHERE id = $1 AND updated_at < $2  -- Only apply if newer
                  """,
                  event.memory_id, event.occurred_at
              )

      # Or use sequence numbers per entity
      async def handle_with_sequence(event):
          await db.execute(
              """
              UPDATE memories SET ..., version = $2
              WHERE id = $1 AND version < $2
              """,
              event.memory_id, event.sequence
          )
    symptoms:
      - "Intermittent 'not found' errors"
      - "Updates applied to wrong state"
      - "Tests pass but production fails"
      - "Errors correlate with high load"
    detection_pattern: null
    version_range: ">=1.0.0"

  - id: projection-rebuild-blocks-writes
    summary: Projection rebuild takes hours and blocks new events
    severity: high
    situation: |
      You need to rebuild a projection (bug fix, schema change). You stop the
      old projector, start replay from position 0. Meanwhile, new events
      pile up. Users see stale data for hours.
    why: |
      Naive rebuild processes events sequentially from the beginning.
      With millions of events, this takes hours. During this time, the
      projection is either stale or inconsistent.
    solution: |
      # Use blue-green projection pattern

      # 1. Create new projection table (v2)
      await db.execute("CREATE TABLE memories_v2 (LIKE memories)")

      # 2. Start new projector writing to v2, from position 0
      async def rebuild_projector():
          await project_all_events(target_table="memories_v2")

      # 3. Keep old projector running on memories (v1)
      # Users still see live data

      # 4. When v2 catches up to live, atomic swap
      async def swap_when_caught_up():
          while True:
              v2_pos = await get_position("memories_v2")
              live_pos = await get_current_position()
              if live_pos - v2_pos < 100:  # Close enough
                  break
              await asyncio.sleep(60)

          async with db.transaction():
              await db.execute("ALTER TABLE memories RENAME TO memories_old")
              await db.execute("ALTER TABLE memories_v2 RENAME TO memories")

      # 5. Clean up old table after validation
    symptoms:
      - "Stale data during deploys"
      - "Users complain about 'missing' recent items"
      - "Fear of changing projections"
      - "Deployment windows limited by rebuild time"
    detection_pattern: null
    version_range: ">=1.0.0"

  - id: consumer-position-not-stored
    summary: Consumer position lost on restart
    severity: high
    situation: |
      Your event consumer crashes or restarts. When it comes back up, it either
      reprocesses all events or skips to the latest, missing events.
    why: |
      If you don't durably store consumer position, restart means either
      full replay (slow, duplicate processing) or data loss (missed events).
    solution: |
      # Store position in database, not memory

      class DurableConsumer:
          def __init__(self, consumer_name: str):
              self.name = consumer_name

          async def get_position(self) -> int:
              result = await db.fetchval(
                  "SELECT position FROM consumer_positions WHERE name = $1",
                  self.name
              )
              return result or 0

          async def process_from_position(self):
              position = await self.get_position()

              async for event in event_store.read_from(position):
                  await self.handle(event)

                  # Store position after successful processing
                  await db.execute(
                      """
                      INSERT INTO consumer_positions (name, position)
                      VALUES ($1, $2)
                      ON CONFLICT (name) DO UPDATE SET position = $2
                      """,
                      self.name, event.position
                  )

      # With NATS JetStream, use durable consumers
      subscription = await js.subscribe(
          "memories.>",
          durable="my-consumer",  # Position stored in NATS
      )
    symptoms:
      - "Duplicate processing after restart"
      - "Events missing after restart"
      - "Inconsistent data between instances"
      - "Full replay on every deploy"
    detection_pattern: 'subscribe.*(?!durable)'
    version_range: ">=1.0.0"

  - id: event-too-many-fields
    summary: Event payloads that contain the world
    severity: medium
    situation: |
      Your event contains 50+ fields including nested objects, full entity
      snapshots, and "might need this later" data. Storage grows fast.
    why: |
      Fat events cause storage bloat, slow serialization, and schema rigidity.
      Every field is a contract - you can never remove it. Changes require
      migrating millions of events.
    solution: |
      # WRONG: Kitchen sink event
      class UserSignedUp:
          user_id: UUID
          email: str
          name: str
          address: Address  # Full nested object
          preferences: Dict  # Everything they might ever want
          session_data: Dict  # Why is this here?
          request_headers: Dict  # Definitely not needed

      # RIGHT: Minimal event with references
      class UserSignedUp:
          user_id: UUID
          email: str
          correlation_id: UUID
          occurred_at: datetime
          # That's it. Profile details go in UserProfileUpdated.
          # Address goes in AddressAdded. Single responsibility.

      # If you need denormalized data for a projection, derive it
      # in the projection, don't store it in events.
    symptoms:
      - "Events are several KB each"
      - "Schema changes require migration scripts"
      - "Storage costs growing faster than users"
      - "Serialization is slow"
    detection_pattern: '@dataclass[\\s\\S]*?:\\s*\\w+[\\s\\S]{500,}?def to_bytes'
    version_range: ">=1.0.0"

  - id: sync-projection-in-handler
    summary: Blocking on projection update in event handler
    severity: medium
    situation: |
      Your event handler updates all projections synchronously before
      acknowledging the event. As you add projections, throughput drops.
    why: |
      Event ingestion should be fast. Projections can be eventually consistent.
      Synchronous projection updates create cascading slowdowns and timeouts.
    solution: |
      # BAD: Synchronous projection
      async def handle_event(event):
          await update_projection_1(event)
          await update_projection_2(event)
          await update_projection_3(event)
          await msg.ack()  # Takes 3x as long

      # GOOD: Async projection with separate consumers
      # Event store handles ingestion only
      async def handle_event(event):
          await event_store.append(event)
          await msg.ack()  # Fast

      # Separate projector processes for each projection
      # projection_1_consumer.py
      async def project_1():
          async for event in event_store.subscribe():
              await update_projection_1(event)

      # projection_2_consumer.py
      async def project_2():
          async for event in event_store.subscribe():
              await update_projection_2(event)
    symptoms:
      - "Event ingestion slows as projections increase"
      - "Ack timeouts during high load"
      - "Adding new projection slows everything"
      - "Single projection failure blocks all events"
    detection_pattern: null
    version_range: ">=1.0.0"
