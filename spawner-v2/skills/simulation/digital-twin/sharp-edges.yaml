id: digital-twin-sharp-edges
skill: digital-twin
version: 1.0.0

sharp_edges:

  - id: model-reality-divergence
    severity: critical
    title: "Twin Diverges From Reality Without Detection"
    summary: "Digital twin predictions no longer match physical system"
    symptoms:
      - "Twin shows normal, physical system fails"
      - "Control based on twin causes problems"
      - "Predictions become increasingly wrong over time"
    why: |
      Digital twins are only as good as their models.
      Physical systems change: wear, damage, environment shifts.
      Without monitoring divergence, the twin becomes fiction.

      Common causes:
      - Model parameters drift (friction, efficiency degrade)
      - Unmodeled physics (missing failure mode)
      - Sensor drift (calibration changes)
      - Environmental changes not captured
    gotcha: |
      # Twin predicts motor temperature based on load
      predicted_temp = model.predict(load_history)
      # Actual temp 20% higher but no alarm

      # Weeks later: motor overheats and fails
      # Twin never predicted it because model was wrong
    solution: |
      # 1. Monitor residuals continuously
      class DivergenceMonitor:
          def __init__(self, threshold: float = 0.1):
              self.threshold = threshold
              self.residual_history = []

          def check(self, predicted: float, actual: float) -> bool:
              residual = abs(predicted - actual) / (abs(actual) + 1e-6)
              self.residual_history.append(residual)

              # Check for systematic drift
              if len(self.residual_history) > 100:
                  recent = self.residual_history[-100:]
                  if np.mean(recent) > self.threshold:
                      self.trigger_recalibration()
                      return False
              return True

          def trigger_recalibration(self):
              # Flag for model update
              logging.warning("Twin divergence detected, recalibration needed")

      # 2. Use Bayesian model updating
      # Parameters adapt based on observed data

      # 3. A/B testing: run shadow twin with different params

  - id: sensor-failure-blindness
    severity: critical
    title: "Twin Continues Operating on Failed Sensor Data"
    summary: "Sensor failure not detected, twin operates on stale/wrong data"
    symptoms:
      - "Twin state frozen while physical changes"
      - "Sudden large jumps when sensor recovers"
      - "Decisions made on wrong state"
    why: |
      Sensors fail: stuck values, drift, communication loss.
      Without health monitoring, twin trusts bad data.
      "Garbage in, garbage out" at machine speed.

      Failure modes:
      - Stuck at last value (communication failure)
      - Drift (calibration degradation)
      - Spike/noise (electrical interference)
      - Complete loss (hardware failure)
    gotcha: |
      # Sensor returns same value for hours
      async def read_sensor():
          value = await sensor.read()  # Always returns 25.0
          return value  # No freshness check!

      # Twin thinks temperature stable at 25C
      # Actually: sensor dead, temp is 80C, fire starts
    solution: |
      # 1. Track sensor health
      @dataclass
      class SensorHealth:
          last_update: datetime
          last_change: datetime
          stuck_count: int = 0
          variance_window: List[float] = field(default_factory=list)

          def update(self, value: float) -> float:
              """Return quality score 0-1."""
              now = datetime.now()
              quality = 1.0

              # Check staleness
              age = (now - self.last_update).total_seconds()
              if age > 10:  # No update in 10s
                  quality *= max(0.1, 1.0 - age / 60)

              # Check for stuck value
              self.variance_window.append(value)
              if len(self.variance_window) > 20:
                  self.variance_window.pop(0)
                  if np.var(self.variance_window) < 1e-6:
                      self.stuck_count += 1
                      quality *= 0.5 ** min(self.stuck_count, 5)
                  else:
                      self.stuck_count = 0

              self.last_update = now
              return quality

      # 2. Require minimum quality for decisions
      if sensor_quality < 0.5:
          use_backup_sensor() or enter_safe_mode()

      # 3. Cross-validate with physics model
      if abs(sensor_value - model_prediction) > 3 * sigma:
          flag_sensor_anomaly()

  - id: latency-hidden-by-buffering
    severity: high
    title: "Real-Time Requirements Not Met Due to Hidden Latency"
    summary: "Buffering and queuing hide latency until control fails"
    symptoms:
      - "Control oscillates or becomes unstable"
      - "Twin lags behind physical system"
      - "Events processed in wrong order"
    why: |
      Every queue adds latency. MQTT, databases, processing pipelines.
      Latency accumulates through the stack.
      By the time twin updates, physical state has changed.

      For control: latency > control period = instability
      For monitoring: latency > event duration = missed events

      Hidden in:
      - Message broker buffers
      - Database write queues
      - Network transmission
      - Processing backlogs
    gotcha: |
      # Looks real-time but isn't
      async def update_twin():
          message = await mqtt_client.receive()  # Already 50ms old
          await database.write(message)  # +20ms
          state = await twin.compute(message)  # +30ms
          await publish_state(state)  # +10ms

          # 110ms total, but message timestamp says "now"
    solution: |
      # 1. End-to-end latency measurement
      class LatencyTracker:
          def __init__(self, max_latency_ms: float):
              self.max_latency = max_latency_ms

          def check(self, message_timestamp: datetime) -> bool:
              latency = (datetime.now() - message_timestamp).total_seconds() * 1000
              if latency > self.max_latency:
                  logging.warning(f"Latency {latency}ms exceeds limit {self.max_latency}ms")
                  return False
              return True

      # 2. Skip stale messages in real-time path
      def process_if_fresh(message):
          if latency_tracker.check(message.timestamp):
              return process(message)
          else:
              # Log and skip, or use for batch analytics
              return None

      # 3. Direct sensor connection for control loop
      # Bypass message broker for <10ms requirements

      # 4. Track latency percentiles, alert on degradation

  - id: state-explosion
    severity: high
    title: "Twin State Grows Unbounded With Asset Fleet"
    summary: "Memory/compute grows linearly or worse with asset count"
    symptoms:
      - "Performance degrades as fleet grows"
      - "Memory exhaustion"
      - "Update frequency drops"
    why: |
      Naive: one twin instance per physical asset.
      1000 assets = 1000 model instances.
      Each with history, state, predictions.

      Worse: N^2 if assets interact (fleet optimization).

      Production fleets: 10,000+ assets common.
      Won't fit in single process memory.
    gotcha: |
      # Simple but doesn't scale
      twins = {}
      for asset_id in all_assets:  # 10,000 assets
          twins[asset_id] = DigitalTwin(
              model=load_model(),  # 100MB each
              history_days=30       # Growing forever
          )
      # 1TB memory, still growing
    solution: |
      # 1. Lazy loading with LRU cache
      from functools import lru_cache

      @lru_cache(maxsize=1000)
      def get_twin(asset_id: str) -> DigitalTwin:
          return load_twin_from_storage(asset_id)

      # 2. Shared model instances
      class TwinFactory:
          def __init__(self):
              self.models = {}  # Model type -> shared instance

          def create_twin(self, asset_id: str, model_type: str) -> DigitalTwin:
              if model_type not in self.models:
                  self.models[model_type] = load_model(model_type)

              return DigitalTwin(
                  model=self.models[model_type],  # Shared!
                  state=load_state(asset_id)      # Per-asset
              )

      # 3. Tiered storage
      # Hot: in-memory for active assets
      # Warm: Redis for recent
      # Cold: database for historical

      # 4. Streaming state (don't keep all history in memory)
      class StreamingState:
          def __init__(self, window_size: int = 100):
              self.window = deque(maxlen=window_size)

          def add(self, state):
              self.window.append(state)
              if should_archive(state):
                  archive_to_storage(state)

  - id: clock-drift-distributed
    severity: medium
    title: "Time Synchronization Errors Across Distributed Twin"
    summary: "Clock differences cause event ordering errors and fusion failures"
    symptoms:
      - "Sensor fusion produces wrong results"
      - "Events processed out of order"
      - "Impossible causality (effect before cause)"
    why: |
      Edge devices, cloud servers, sensors have different clocks.
      Without sync, timestamps meaningless for comparison.

      1ms drift per hour = 24ms per day
      Sensor fusion assumes synchronized timestamps.
      Control loops need sub-ms timing.

      GPS: ~100ns accuracy
      NTP: ~1ms to ~50ms
      Unsynchronized: arbitrary drift
    gotcha: |
      # Edge device drifted 500ms
      edge_timestamp = datetime.now()  # Edge clock: 10:00:00.500
      cloud_timestamp = datetime.now()  # Cloud clock: 10:00:00.000

      # Cloud receives edge message, thinks it's from the future!
      if edge_timestamp > cloud_timestamp:
          # Impossible! Edge message arrived before it was sent?
          pass

      # Sensor fusion uses wrong time order
      fused_state = kalman.update([
          (sensor_a, timestamp_a),  # Actually newer
          (sensor_b, timestamp_b),  # Actually older but higher timestamp
      ])
      # Wrong temporal ordering = wrong fusion
    solution: |
      # 1. Use centralized time source
      import ntplib

      def get_synchronized_time() -> datetime:
          """Get NTP-synchronized time."""
          client = ntplib.NTPClient()
          response = client.request('pool.ntp.org')
          return datetime.fromtimestamp(response.tx_time)

      # 2. Include clock offset in messages
      @dataclass
      class TimestampedMessage:
          local_time: datetime
          ntp_offset_ms: float  # Local - NTP
          sequence_number: int  # For ordering

      # 3. Logical clocks for ordering
      class LamportClock:
          def __init__(self):
              self.counter = 0

          def tick(self) -> int:
              self.counter += 1
              return self.counter

          def receive(self, remote_counter: int):
              self.counter = max(self.counter, remote_counter) + 1

      # 4. Vector clocks for causality

  - id: calibration-temperature-drift
    severity: medium
    title: "Model Calibration Drifts With Operating Conditions"
    summary: "Model calibrated at one condition fails at another"
    symptoms:
      - "Accuracy varies with temperature/load/age"
      - "Good in lab, wrong in field"
      - "Seasonal accuracy variations"
    why: |
      Models calibrated under specific conditions.
      Real operations span wide condition range.
      Physics changes with temperature, wear, load.

      Thermal expansion changes dimensions.
      Wear changes friction coefficients.
      Load changes dynamic behavior.
    gotcha: |
      # Calibrated at room temperature
      def calibrate_model():
          # Run at 20C, collect data, fit parameters
          model.friction = 0.1  # At 20C

      # Deployed in hot environment
      # At 60C: actual friction = 0.05 (lubrication thins)
      # Model predicts wrong force, control fails
    solution: |
      # 1. Condition-dependent parameters
      class AdaptiveModel:
          def __init__(self):
              self.param_table = {}  # (temp_bin, load_bin) -> params

          def get_params(self, temperature: float, load: float):
              temp_bin = int(temperature / 10) * 10
              load_bin = int(load / 100) * 100
              key = (temp_bin, load_bin)

              if key in self.param_table:
                  return self.param_table[key]
              else:
                  # Interpolate from nearest known conditions
                  return self.interpolate_params(temperature, load)

      # 2. Online parameter estimation
      # EKF with parameters in state vector

      # 3. Physics-based temperature compensation
      def compensate_friction(base_friction: float, temperature: float) -> float:
          # Arrhenius-like temperature dependence
          return base_friction * np.exp(-0.01 * (temperature - 20))

      # 4. Ensemble models for different conditions

detection:
  file_patterns:
    - "**/*twin*.py"
    - "**/*digital*.py"
    - "**/*sync*.py"
    - "**/*sensor*.py"
    - "**/*iot*.py"
