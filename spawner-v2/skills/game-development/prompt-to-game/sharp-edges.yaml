id: prompt-to-game
skill: Prompt-to-Game Development
version: "1.0"

sharp_edges:
  - id: hallucinated-apis
    severity: critical
    title: AI Hallucinated APIs and Methods
    description: >
      5.2-21.7% of AI suggestions include hallucinated dependencies.
      Methods that don't exist, wrong signatures, made-up libraries.
      AI is confident but wrong.
    detection_pattern: "import|require|from.*(?!react|phaser|three|kaboom|godot)"
    symptoms:
      - "X is not a function" errors
      - "Cannot find module" errors
      - Method exists but signature is wrong
    solution: |
      1. Verify every unfamiliar method in official docs
      2. Use TypeScript for compile-time catching
      3. Test imports immediately after generation
      4. Keep official docs open while prompting
      5. Ask AI: "Are you sure this method exists in [framework] [version]?"
    technical_detail: >
      Claude is better than GPT at avoiding hallucinations but
      still does it. Always verify.

  - id: security-vulnerabilities-45-percent
    severity: critical
    title: 45% of AI Code Has Security Vulnerabilities
    description: >
      Veracode 2025: 45% of AI-generated code contains security issues.
      Java: 72% security failure rate. 86% fail XSS protection.
      88% vulnerable to log injection. AI doesn't think about security.
    detection_pattern: "eval|innerHTML|dangerouslySetInnerHTML|document\\.write"
    symptoms:
      - XSS vulnerabilities in user-facing code
      - SQL/command injection in backend
      - Exposed API keys in client code
      - Insecure randomness for security features
    solution: |
      1. Treat ALL AI code as untrusted
      2. Run SAST tools (ESLint security rules, Semgrep)
      3. Never use eval() or innerHTML with user input
      4. Keep secrets server-side only
      5. Add security review step before shipping
    references:
      - https://www.veracode.com/blog/genai-code-security-report/

  - id: version-mismatch-disaster
    severity: high
    title: AI Generates Code for Wrong Framework Version
    description: >
      AI trained on old docs. Phaser 2 vs Phaser 3, Three.js r100 vs r162.
      Subtle bugs from deprecated methods, wrong parameter orders,
      removed features.
    detection_pattern: "phaser|three|godot|kaboom|unity"
    symptoms:
      - Deprecation warnings everywhere
      - "This worked in the tutorial but not in my version"
      - Methods with slightly wrong parameters
      - Missing required arguments
    solution: |
      1. ALWAYS specify version in prompts: "Using Phaser 3.90..."
      2. Paste current API examples in context
      3. Check changelogs for deprecated methods
      4. Verify imports match your package.json
      5. Test with latest version before shipping
    code_example: |
      // Phaser 2 (wrong for Phaser 3)
      game.add.sprite(100, 100, 'player');

      // Phaser 3 (correct)
      this.add.sprite(100, 100, 'player');

  - id: context-window-forgetting
    severity: high
    title: AI Forgets Earlier Code in Long Sessions
    description: >
      Context windows have limits. Large games exceed them.
      AI "forgets" earlier functions, creates duplicates,
      contradicts previous implementations.
    detection_pattern: "context|window|token|memory"
    symptoms:
      - Duplicate function definitions
      - Contradictory implementations
      - References to code that doesn't exist
      - "I'll create this function" when it already exists
    solution: |
      1. Keep files under 500 lines
      2. Split early and often
      3. Start new conversations for new features
      4. Paste relevant code snippets in new contexts
      5. Use tools with larger context (Claude Code)
    technical_detail: >
      Magic.dev claims 100M token context, but most tools
      are 128k-200k. Summarize and split for safety.

  - id: sunk-cost-fallacy-loop
    severity: high
    title: Sunk-Cost Prompting Fallacy Loop
    description: >
      "I've spent 2 hours prompting, I can't stop now."
      Continuing to prompt when you should reset or code manually.
      Each failed attempt adds broken context.
    detection_pattern: "try.*again|not.*working|still.*broken"
    symptoms:
      - Same error after 3+ attempts
      - Prompts getting longer and more desperate
      - AI reverting previous fixes
      - Increasingly complex workarounds
    solution: |
      1. Set attempt limit: 3 tries max for same issue
      2. After limit: reset context, start fresh
      3. Consider: "Can I code this in 5 minutes manually?"
      4. Sometimes the answer is to just write the code
      5. Document what doesn't work to avoid repeating
    references:
      - https://dev.to/samuelfaure/the-ai-programming-sunk-cost-fallacy-loop-and-how-to-break-it-13d6

  - id: works-on-ai-machine
    severity: high
    title: "Works on AI's Machine" - Missing Local Context
    description: >
      AI doesn't have access to your file structure, installed
      dependencies, environment variables, or current state.
      Code works in AI's imagination but fails locally.
    detection_pattern: "file.*path|require|import|env"
    symptoms:
      - "File not found" errors
      - "Module not installed" errors
      - Works in AI preview, breaks locally
      - Incorrect relative paths
    solution: |
      1. Share your package.json in context
      2. Provide explicit file paths
      3. List existing files when asking about structure
      4. Test in clean environment (Docker)
      5. Verify imports against actual node_modules

  - id: consistency-between-runs
    severity: medium
    title: Same Prompt Produces Different Games
    description: >
      Running identical prompt 10 times produces 10 different games.
      Non-deterministic outputs. Can't reproduce exact behavior.
      Hard to debug systematically.
    detection_pattern: "random|generate|create"
    symptoms:
      - Yesterday's prompt doesn't work today
      - Can't reproduce successful generation
      - Inconsistent code style across sessions
    solution: |
      1. Save working code IMMEDIATELY
      2. Document exact prompts used
      3. Git commit after every working state
      4. Don't rely on regeneration
      5. Consider temperature=0 for more consistency
    references:
      - https://www.tabulamag.com/p/i-asked-claude-code-to-write-the

  - id: ai-spaghetti-code
    severity: medium
    title: AI Generates Spaghetti Code
    description: >
      AI adds features wherever convenient. Tightly coupled components.
      Redundant logic. Single-file chaos. Technical debt accumulates
      faster than with human-written code.
    detection_pattern: "add.*feature|implement|create"
    symptoms:
      - File exceeds 500 lines
      - Functions exceed 100 lines
      - Nested callbacks 4+ levels deep
      - Duplicate code across functions
    solution: |
      1. Set refactoring thresholds (500 lines, 3 deep nesting)
      2. Prompt for refactoring explicitly
      3. Apply Single Responsibility Principle
      4. Split files by concern
      5. Refactor BEFORE adding new features when threshold hit
    code_example: |
      // Refactoring prompt
      "Refactor this into separate modules:
      - player.js: Player class
      - enemies.js: Enemy class
      - world.js: World generation
      Use ES6 imports. Maintain all functionality."

  - id: false-confidence-syndrome
    severity: medium
    title: AI's False Confidence in Generated Code
    description: >
      AI presents code with extreme confidence even when wrong.
      "Here's the working implementation" for code that doesn't run.
      No uncertainty markers for unreliable sections.
    detection_pattern: "here.*is|working|complete|ready"
    symptoms:
      - AI says "this will work" but it doesn't
      - No caveats or warnings
      - Confident explanations of buggy code
    solution: |
      1. Never trust "working" claim without testing
      2. Test EVERYTHING before accepting
      3. Ask "What could go wrong with this code?"
      4. Look for edge cases AI might miss
      5. Maintain healthy skepticism

  - id: browser-only-vs-node
    severity: medium
    title: Browser-Only APIs in Node.js Code (and Vice Versa)
    description: >
      AI mixes browser and Node.js APIs. Uses `window` in Node.
      Uses `fs` in browser code. Doesn't consider runtime environment.
    detection_pattern: "window|document|fs\.|process\.|require.*fs"
    symptoms:
      - "window is not defined" in Node
      - "require is not defined" in browser
      - "document is not defined" in tests
    solution: |
      1. Specify runtime in prompt: "for browser" or "for Node.js"
      2. Check generated imports
      3. Use environment-aware patterns
      4. Test in target runtime immediately
    code_example: |
      // Browser-only
      const data = localStorage.getItem('key');

      // Node.js-only
      const data = fs.readFileSync('file.json');

      // Universal (with check)
      const isBrowser = typeof window !== 'undefined';

symptom_index:
  - error_pattern: "is not a function|is not defined"
    edge_id: hallucinated-apis
    quick_fix: "Check if method exists in official docs"
    solution:
      - "Verify import statements"
      - "Check framework version"
      - "Use TypeScript for type checking"

  - error_pattern: "Module not found|Cannot find module"
    edge_id: works-on-ai-machine
    quick_fix: "Check package.json for dependency"
    solution:
      - "npm install missing package"
      - "Verify import path is correct"
      - "Check if AI hallucinated the package"

  - error_pattern: "deprecated|will be removed"
    edge_id: version-mismatch-disaster
    quick_fix: "Check migration guide for new API"
    solution:
      - "Specify correct version in prompt"
      - "Update to modern patterns"

  - error_pattern: "duplicate.*definition|already defined"
    edge_id: context-window-forgetting
    quick_fix: "Start new conversation, paste only relevant code"
    solution:
      - "Split code into smaller files"
      - "Keep conversation focused"

  - error_pattern: "window is not defined|document is not"
    edge_id: browser-only-vs-node
    quick_fix: "Check runtime environment, add guards"
    solution:
      - "Specify runtime in prompts"
      - "Use environment checks"

red_flags:
  - id: eval-usage
    pattern: 'eval\s*\('
    severity: critical
    risk: "Remote code execution vulnerability"
    action: "STOP - Never use eval with any user input"

  - id: exposed-secret
    pattern: '(?:api[_-]?key|secret|token)\s*[=:]\s*["\'][a-zA-Z0-9]{16,}'
    severity: critical
    risk: "Exposed API key in client code"
    action: "Move to environment variable on server-side"

  - id: inner-html-user-input
    pattern: 'innerHTML\s*=.*(?:user|input|param|query)'
    severity: critical
    risk: "XSS vulnerability"
    action: "Use textContent or sanitize input"

  - id: no-input-validation
    pattern: '(?:req|request)\.(?:body|query|params)(?!\s*\?\.[^\s]*validate)'
    severity: high
    risk: "Unvalidated user input"
    action: "Add input validation before use"
