id: disaster-recovery-collaboration
skill: disaster-recovery
version: 1.0.0

receives_from:
  - skill: enterprise-architecture
    context: "System architecture and dependencies"
    receives:
      - "Service criticality tiers"
      - "Dependency maps"
    provides: "DR requirements per tier"

  - skill: multi-tenancy
    context: "Per-tenant recovery needs"
    receives:
      - "Tenant isolation model"
      - "Data residency requirements"
    provides: "Tenant-aware DR strategy"

  - skill: compliance-automation
    context: "Compliance requirements for DR"
    receives:
      - "Regulatory DR requirements (SOC 2, ISO, etc.)"
      - "Evidence collection needs"
    provides: "Compliant DR procedures and evidence"

  - skill: devops
    context: "Infrastructure automation context"
    receives:
      - "Infrastructure-as-code definitions"
      - "Deployment pipeline configuration"
    provides: "DR automation integrated with CI/CD"

delegation_triggers:
  - trigger: "enterprise.architecture|system.design"
    delegate_to: enterprise-architecture
    context: "Architecture decisions affecting DR"

  - trigger: "multi.tenant|per.tenant.backup"
    delegate_to: multi-tenancy
    context: "Tenant-specific DR requirements"

  - trigger: "compliance|audit|evidence"
    delegate_to: compliance-automation
    context: "DR compliance requirements"

  - trigger: "infrastructure.automation|deployment.pipeline"
    delegate_to: devops
    context: "Infrastructure automation for DR"

  - trigger: "database.replication|data.sync"
    delegate_to: postgres-wizard
    context: "Database-specific replication setup"

  - trigger: "security.incident|breach.response"
    delegate_to: security-specialist
    context: "Security incident as DR trigger"

feedback_loops:
  receives_feedback_from:
    - skill: enterprise-architecture
      signal: "Architecture changes"
      action: "Update DR procedures"

    - skill: multi-tenancy
      signal: "New tenant requirements"
      action: "Adjust per-tenant DR"

    - skill: devops
      signal: "Infrastructure changes"
      action: "Update failover automation"

  sends_feedback_to:
    - skill: enterprise-architecture
      signal: "DR test results"
      action: "Improve system resilience"

    - skill: compliance-automation
      signal: "DR evidence"
      action: "Support audit requirements"

    - skill: devops
      signal: "Failover gaps identified"
      action: "Improve infrastructure resilience"

escalation_paths:
  - situation: "Active production incident requiring failover"
    escalate_to: devops
    context: "Immediate infrastructure action needed"

  - situation: "Data corruption detected in primary"
    escalate_to: postgres-wizard
    context: "Database expertise for recovery decisions"

  - situation: "Security breach triggering DR"
    escalate_to: security-specialist
    context: "Coordinate DR with incident response"

  - situation: "DR test reveals compliance gaps"
    escalate_to: compliance-automation
    context: "Compliance remediation required"

  - situation: "Cross-region failover affecting data residency"
    escalate_to: multi-tenancy
    context: "Tenant data location compliance"

  - situation: "Architecture not supporting required RTO/RPO"
    escalate_to: enterprise-architecture
    context: "Architecture changes needed for DR requirements"

  - situation: "Cost of DR strategy exceeds budget"
    escalate_to: enterprise-architecture
    context: "Re-tier services to optimize DR costs"

  - situation: "Third-party dependency failure"
    escalate_to: integration-patterns
    context: "External service failover strategies"

workflow_integration:
  typical_sequence:
    1:
      step: "Business impact analysis"
      skills: [disaster-recovery, enterprise-architecture]
      output: "Service criticality tiers with RTO/RPO"
      details: |
        - Inventory all services and dependencies
        - Interview stakeholders for business impact
        - Define criticality tiers (Tier 1: mission-critical, etc.)
        - Assign RTO/RPO to each tier
        - Document revenue/reputation impact of downtime

    2:
      step: "Select DR strategy per tier"
      skills: [disaster-recovery]
      output: "DR strategy for each service"
      details: |
        - Tier 1: Active-active or hot standby
        - Tier 2: Warm standby or pilot light
        - Tier 3: Backup and restore
        - Map strategy to cost/complexity tradeoffs
        - Get stakeholder approval on strategies

    3:
      step: "Design failover architecture"
      skills: [disaster-recovery, devops, enterprise-architecture]
      output: "Technical DR architecture"
      details: |
        - Design multi-region or multi-zone setup
        - Configure data replication paths
        - Design DNS/load balancer failover
        - Plan for stateful service recovery
        - Document network failover paths

    4:
      step: "Implement backup automation"
      skills: [disaster-recovery, devops]
      output: "Automated backup pipeline"
      details: |
        - Configure backup schedules per tier
        - Set up cross-region replication
        - Enable backup encryption
        - Implement backup verification
        - Configure retention policies

    5:
      step: "Create runbooks"
      skills: [disaster-recovery]
      output: "Executable DR runbooks"
      details: |
        - Document step-by-step failover procedures
        - Include decision trees for scenarios
        - Add verification checkpoints
        - Define communication templates
        - Assign roles and responsibilities

    6:
      step: "Test and validate"
      skills: [disaster-recovery, devops]
      output: "DR test results and gaps"
      details: |
        - Schedule tabletop exercises
        - Conduct game days with real failover
        - Measure actual RTO/RPO achieved
        - Document gaps and remediation
        - Update runbooks based on learnings

    7:
      step: "Establish ongoing program"
      skills: [disaster-recovery, compliance-automation]
      output: "Continuous DR program"
      details: |
        - Set testing cadence (quarterly minimum)
        - Integrate with change management
        - Configure monitoring and alerting
        - Establish evidence collection for audits
        - Create executive reporting dashboard

  decision_points:
    - question: "What RTO/RPO targets to set?"
      guidance: |
        RTO (Recovery Time Objective) - Max acceptable downtime:
        - Tier 1 (Revenue-critical): < 15 minutes
        - Tier 2 (Important): < 4 hours
        - Tier 3 (Standard): < 24 hours
        - Tier 4 (Low priority): < 72 hours

        RPO (Recovery Point Objective) - Max acceptable data loss:
        - Zero: Synchronous replication (expensive)
        - < 1 minute: Streaming replication
        - < 1 hour: Point-in-time recovery
        - < 24 hours: Daily backups

        Cost increases exponentially as RTO/RPO decrease.
        Validate targets with business stakeholders.

    - question: "Which DR strategy for each tier?"
      guidance: |
        Active-Active (RTO: ~0, RPO: ~0):
        - Traffic distributed across regions
        - Highest cost, highest availability
        - Complex data consistency
        - Use for: Payment processing, real-time systems

        Hot Standby (RTO: minutes, RPO: seconds):
        - Fully running secondary, no traffic
        - Synchronous or async replication
        - Fast failover, moderate cost
        - Use for: Core business applications

        Warm Standby (RTO: 10-30 min, RPO: minutes):
        - Scaled-down secondary
        - Needs scale-up on failover
        - Lower cost than hot
        - Use for: Important but not critical services

        Pilot Light (RTO: hours, RPO: minutes-hours):
        - Only core infrastructure running
        - Full deployment on failover
        - Lower ongoing cost
        - Use for: Seasonal or batch systems

        Backup & Restore (RTO: hours-days, RPO: hours):
        - No running infrastructure
        - Restore from backups
        - Lowest cost
        - Use for: Development, test, archive

    - question: "Single region vs multi-region?"
      guidance: |
        Single region (multi-AZ):
        - Protects against: Data center failure
        - Does NOT protect: Region-wide outage
        - Simpler, lower latency, lower cost
        - Use when: Regional compliance, cost-sensitive

        Multi-region active-passive:
        - Protects against: Full region failure
        - Adds latency for replication
        - Moderate cost increase
        - Use when: Business-critical, compliance requires

        Multi-region active-active:
        - Protects against: Full region failure
        - Serves traffic from both regions
        - Complex data consistency
        - Highest cost and complexity
        - Use when: Global users, zero-downtime required

    - question: "How to handle database failover?"
      guidance: |
        Managed services (RDS, Cloud SQL):
        - Built-in multi-AZ failover
        - Automatic promotion
        - Limited customization
        - Recommended for most cases

        Self-managed replication:
        - Streaming replication (PostgreSQL, MySQL)
        - Manual or automated promotion
        - More control, more responsibility
        - Use when: Managed service gaps

        Distributed databases (CockroachDB, Spanner):
        - Built-in multi-region
        - Automatic failover
        - Higher cost per node
        - Use when: Global consistency required

        Key decisions:
        - Synchronous vs async replication
        - Automatic vs manual failover
        - Read replicas vs standby
        - Connection string management

    - question: "How often to test DR?"
      guidance: |
        Tabletop exercises: Monthly
        - Walk through runbooks
        - No actual failover
        - Identify gaps in documentation

        Partial failover tests: Quarterly
        - Fail over non-critical services
        - Measure actual RTO/RPO
        - Test communication procedures

        Full DR tests: Annually minimum
        - Complete failover of all systems
        - Measure organization readiness
        - Required for compliance (SOC 2, ISO 22301)

        Chaos engineering: Ongoing
        - Random fault injection
        - Build resilience culture
        - Start small, increase scope

        After any significant change:
        - New service deployment
        - Architecture changes
        - Infrastructure updates

    - question: "How to handle stateful services?"
      guidance: |
        Databases:
        - Replication for RPO
        - Automated failover for RTO
        - Connection string abstraction

        Caches (Redis, Memcached):
        - Accept cache loss (rebuild on failover)
        - Or: Redis Cluster with replication
        - Warm cache strategy post-failover

        Message queues:
        - Replicated clusters (Kafka, RabbitMQ)
        - Idempotent consumers
        - Dead letter queues for failures

        File storage:
        - Cross-region replication (S3, GCS)
        - CDN for static assets
        - Verify replication lag

        Sessions:
        - Externalize to replicated store
        - Or: Accept session loss on failover

common_combinations:
  - name: DR Planning
    skills:
      - disaster-recovery
    workflow: |
      1. Define RTO/RPO per service tier
      2. Map all dependencies
      3. Select DR strategy per tier
      4. Design failover automation
      5. Document runbooks
      6. Schedule testing cadence

  - name: Backup and Restore Setup
    skills:
      - disaster-recovery
    workflow: |
      1. Define backup policies
      2. Configure backup automation
      3. Set up cross-region replication
      4. Enable backup encryption
      5. Implement restore procedures
      6. Test restore process
      7. Set retention policies

  - name: Chaos Engineering Program
    skills:
      - disaster-recovery
    workflow: |
      1. Define steady state hypotheses
      2. Design initial experiments
      3. Start with small blast radius
      4. Implement rollback triggers
      5. Run game days
      6. Expand to production chaos
      7. Document learnings

  - name: Active-Active Setup
    skills:
      - disaster-recovery
      - integration-patterns
    workflow: |
      1. Design multi-region architecture
      2. Configure database replication
      3. Implement conflict resolution
      4. Set up global load balancing
      5. Configure traffic distribution
      6. Test region isolation
      7. Document failover procedures

  - name: Database Failover Runbook
    skills:
      - disaster-recovery
    workflow: |
      1. Verify primary is truly unavailable
      2. Check replication lag
      3. Notify stakeholders
      4. Promote standby to primary
      5. Update connection strings
      6. Verify application functionality
      7. Begin RCA

  - name: Compliance-Ready DR
    skills:
      - disaster-recovery
      - compliance-automation
    workflow: |
      1. Map compliance requirements to DR
      2. Document DR controls
      3. Implement evidence collection
      4. Schedule compliant test frequency
      5. Generate audit-ready reports
      6. Maintain control documentation

ecosystem:
  backup:
    - "AWS Backup"
    - "Azure Backup"
    - "Veeam"
    - "Velero"

  replication:
    - "PostgreSQL Streaming Replication"
    - "MySQL Group Replication"
    - "MongoDB Atlas"
    - "CockroachDB"

  chaos:
    - "Gremlin"
    - "Chaos Monkey"
    - "Litmus Chaos"
    - "Chaos Mesh"
    - "AWS Fault Injection Simulator"

  dns:
    - "Route 53"
    - "Azure Traffic Manager"
    - "Cloudflare"
    - "NS1"

  orchestration:
    - "AWS Elastic Disaster Recovery"
    - "Azure Site Recovery"
    - "Zerto"
    - "Commvault"

references:
  frameworks:
    - "AWS Disaster Recovery Workloads"
    - "Azure Business Continuity"
    - "GCP DR Planning"

  standards:
    - "ISO 22301 Business Continuity"
    - "NIST Contingency Planning"
    - "SOC 2 Availability Criteria"

  books:
    - "Site Reliability Engineering"
    - "Chaos Engineering - O'Reilly"
    - "Release It! - Michael Nygard"
