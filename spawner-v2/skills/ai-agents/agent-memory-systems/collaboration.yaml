# Collaboration - Agent Memory Systems
# How this skill works with other skills

version: 1.0.0
skill_id: agent-memory-systems

prerequisites:
  required: []

  recommended:
    - skill: llm-architect
      reason: "RAG architecture and retrieval patterns"
      what_to_know:
        - "Embedding model selection"
        - "Context window management"
        - "Retrieval strategies"

    - skill: data-engineer
      reason: "Database operations at scale"
      what_to_know:
        - "Vector database operations"
        - "Data pipelines for embeddings"
        - "Backup and migration"

delegation_triggers:
  - trigger: "user needs vector database at scale"
    delegate_to: data-engineer
    context: "Production vector store operations"

  - trigger: "user needs embedding model optimization"
    delegate_to: ml-engineer
    context: "Custom embeddings, fine-tuning"

  - trigger: "user needs knowledge graph"
    delegate_to: knowledge-engineer
    context: "Graph-based memory structures"

  - trigger: "user needs RAG pipeline"
    delegate_to: llm-architect
    context: "End-to-end retrieval augmented generation"

  - trigger: "user needs multi-agent shared memory"
    delegate_to: multi-agent-orchestration
    context: "Memory sharing between agents"

receives_context_from:
  - skill: autonomous-agents
    receives:
      - "Memory requirements for the agent"
      - "Context window constraints"
      - "Memory access patterns"

  - skill: product-strategy
    receives:
      - "Personalization requirements"
      - "User data retention policies"
      - "Privacy constraints"

  - skill: llm-architect
    receives:
      - "Embedding model recommendations"
      - "Context management strategies"
      - "RAG architecture patterns"

provides_context_to:
  - skill: autonomous-agents
    provides:
      - "Memory infrastructure and APIs"
      - "Retrieval capabilities"
      - "Memory types available"

  - skill: multi-agent-orchestration
    provides:
      - "Shared memory architecture"
      - "Memory isolation patterns"
      - "Cross-agent retrieval"

  - skill: security-specialist
    provides:
      - "Data storage patterns for review"
      - "User isolation implementation"
      - "Encryption requirements"

escalation_paths:
  - situation: "Vector store performance issues"
    escalate_to: data-engineer
    context: "Index optimization, sharding"

  - situation: "Retrieval quality problems"
    escalate_to: llm-architect
    context: "Embedding, chunking, reranking"

  - situation: "Privacy/compliance concerns"
    escalate_to: security-specialist
    context: "Data handling, retention, encryption"

  - situation: "Complex knowledge structures"
    escalate_to: knowledge-engineer
    context: "Knowledge graphs, ontologies"

  - situation: "Scale beyond 100M vectors"
    escalate_to: data-engineer
    context: "Distributed vector stores, sharding"

workflow_integration:
  typical_sequence:
    1:
      step: "Define memory requirements"
      skills: [autonomous-agents, product-strategy]
      output: "Memory types needed, retention policies"

    2:
      step: "Choose storage architecture"
      skills: [agent-memory-systems, data-engineer]
      output: "Vector store selection, schema design"

    3:
      step: "Design chunking strategy"
      skills: [agent-memory-systems, llm-architect]
      output: "Chunking config, embedding model"

    4:
      step: "Implement memory layer"
      skills: [agent-memory-systems]
      output: "Working memory system"

    5:
      step: "Test retrieval quality"
      skills: [agent-memory-systems, agent-evaluation]
      output: "Retrieval benchmarks"

    6:
      step: "Integrate with agent"
      skills: [autonomous-agents]
      output: "Agent with memory"

  decision_points:
    - question: "Which vector store?"
      guidance: |
        Pinecone:
        - Best for: Enterprise scale, managed service
        - Tradeoffs: Higher cost, no hybrid search
        - Choose when: >10M vectors, need SLA

        Qdrant:
        - Best for: Complex filtering, open-source
        - Tradeoffs: Self-managed (or cloud)
        - Choose when: Need advanced metadata queries

        Weaviate:
        - Best for: Hybrid search, knowledge graphs
        - Tradeoffs: More complex setup
        - Choose when: Need semantic + keyword search

        ChromaDB:
        - Best for: Prototyping, small apps
        - Tradeoffs: Not for production scale
        - Choose when: <100K vectors, development

        pgvector:
        - Best for: Already using Postgres
        - Tradeoffs: Limited at scale
        - Choose when: <1M vectors, familiar tooling

    - question: "Which embedding model?"
      guidance: |
        OpenAI text-embedding-3-large (3072 dim):
        - Best quality, highest cost
        - $0.13/1M tokens
        - Use for: Premium applications

        OpenAI text-embedding-3-small (1536 dim):
        - Good quality, cost-effective
        - $0.02/1M tokens
        - Use for: Most production apps

        nomic-embed-text-v1.5 (768 dim):
        - Open source, local deployment
        - Good quality
        - Use for: Privacy, offline, cost control

        all-MiniLM-L6-v2 (384 dim):
        - Fastest, smallest
        - Good for short text
        - Use for: Real-time, edge deployment

    - question: "What chunk size?"
      guidance: |
        256-384 tokens:
        - Best for: Factual queries, short answers
        - Precise retrieval, may lose context

        512-768 tokens:
        - Best for: General purpose
        - Balance of precision and context

        1024+ tokens:
        - Best for: Complex concepts, code
        - More context, less precision

        Test multiple sizes with your queries!

collaboration_patterns:
  with_agents:
    when: "Integrating memory with autonomous agents"
    approach: |
      Memory integration patterns for agents:

      1. Pre-retrieval context:
         async def prepare_agent_context(query, user_id):
             profile = await memory.semantic.get_profile(user_id)
             relevant = await memory.search(query, user_id, k=5)
             skills = await memory.procedural.find_relevant(query)

             return {
                 "user_profile": profile,
                 "relevant_memories": relevant,
                 "applicable_skills": skills,
             }

      2. Post-action memory formation:
         async def learn_from_action(action, result, context):
             if result.success:
                 await memory.procedural.store(
                     task=context.task_type,
                     approach=action,
                     outcome=result
                 )

             await memory.episodic.log(
                 event=action,
                 result=result,
                 timestamp=datetime.now()
             )

      3. Memory as tool:
         tools = [
             Tool(
                 name="remember",
                 description="Store important information",
                 function=memory.store
             ),
             Tool(
                 name="recall",
                 description="Retrieve relevant memories",
                 function=memory.search
             )
         ]

  with_rag:
    when: "Building RAG systems with memory"
    approach: |
      Memory enhances RAG:

      1. User-aware retrieval:
         # Personalize document retrieval
         user_context = await memory.get_user_context(user_id)
         query_with_context = f"{user_context}\n{query}"
         docs = await rag.retrieve(query_with_context)

      2. Memory-augmented generation:
         # Inject memories into generation
         memories = await memory.search(query, user_id)
         docs = await rag.retrieve(query)

         context = format_context(memories, docs)
         response = await llm.generate(query, context)

      3. Learning from interactions:
         # Store successful retrievals
         if user_satisfied(feedback):
             await memory.store(
                 query=query,
                 useful_docs=[d.id for d in docs],
                 type="retrieval_success"
             )

  with_multi_agent:
    when: "Shared memory across agents"
    approach: |
      Multi-agent memory patterns:

      1. Shared knowledge base:
         # All agents can read
         shared_memory = MemoryStore(namespace="shared")

         # Write access controlled
         async def store_shared(agent_id, content, approval_required=True):
             if approval_required:
                 await request_approval(content)
             await shared_memory.store(content, source=agent_id)

      2. Agent-specific memories:
         # Each agent has private namespace
         agent_memory = MemoryStore(namespace=f"agent-{agent_id}")

      3. Handoff context:
         # When transferring between agents
         async def handoff(from_agent, to_agent, task):
             context = await from_agent.memory.get_task_context(task)
             await to_agent.memory.store_handoff(context)

platform_integration:
  langmem:
    setup: |
      pip install langmem

      from langmem import MemoryStore

      memory = MemoryStore(
          connection_string=os.environ["POSTGRES_URL"]
      )

      # Semantic memory
      await memory.semantic.upsert(namespace, key, content)

      # Episodic memory
      await memory.episodic.add(namespace, content, metadata)

      # Search
      results = await memory.search(query, namespace, k=5)
    considerations:
      - "Designed for LangGraph integration"
      - "Supports CoALA memory types"
      - "Background processing via subconscious"

  memgpt_letta:
    setup: |
      pip install letta

      from letta import Letta

      client = Letta(api_key=os.environ["LETTA_API_KEY"])

      # Create agent with memory
      agent = client.create_agent(
          name="my_agent",
          memory_config={
              "human": "User profile...",
              "persona": "Agent persona...",
          }
      )

      # Agent manages memory automatically
      response = agent.chat("Hello!")
    considerations:
      - "OS-style virtual context management"
      - "Automatic memory paging"
      - "Self-editing memory capabilities"

  mem0:
    setup: |
      pip install mem0ai

      from mem0 import Memory

      m = Memory()

      # Add memory
      m.add("User prefers dark mode", user_id="alice")

      # Search
      results = m.search("preferences", user_id="alice")
    considerations:
      - "Simple API for user memory"
      - "Good for personalization"
      - "Managed service available"

cost_optimization:
  embedding_costs:
    - "Batch embeddings (cheaper per token)"
    - "Use smaller models for simple content"
    - "Cache embeddings for repeated content"
    - "text-embedding-3-small is 6.5x cheaper than large"

  storage_costs:
    - "Archive old memories instead of keeping hot"
    - "Compress metadata"
    - "Use appropriate vector dimensions (don't over-dimension)"

  query_costs:
    - "Use metadata filters to reduce search space"
    - "Cache frequent queries"
    - "Batch similar queries"

migration_patterns:
  model_upgrade:
    - "Track embedding model in metadata"
    - "Re-embed in batches during low traffic"
    - "Use separate collection during migration"
    - "Validate retrieval quality before switchover"

  store_migration:
    - "Export all vectors with metadata"
    - "Import to new store"
    - "Run parallel reads during migration"
    - "Switch writes first, then reads"
