id: prompt-caching-collaboration
skill: prompt-caching
version: 1.0.0

receives_from:
  - skill: context-window-management
    context: "Need caching for performance"
    receives:
      - "Context patterns"
      - "Query patterns"
    provides: "Caching strategy"

  - skill: rag-implementation
    context: "RAG needs caching layer"
    receives:
      - "Retrieval patterns"
      - "Document stability"
    provides: "CAG vs RAG recommendation"

delegation_triggers:
  - trigger: "context window|token"
    delegate_to: context-window-management
    pattern: sequential
    context: "Need context optimization"

  - trigger: "rag|retrieval"
    delegate_to: rag-implementation
    pattern: sequential
    context: "Need retrieval system"

  - trigger: "memory"
    delegate_to: conversation-memory
    pattern: sequential
    context: "Need memory persistence"

feedback_loops:
  receives_feedback_from:
    - skill: rag-implementation
      signal: "Document update frequency"
      action: "Adjust cache TTL and invalidation"

  sends_feedback_to:
    - skill: context-window-management
      signal: "Cache effectiveness data"
      action: "Inform context strategy"

common_combinations:
  - name: High-Performance LLM System
    skills:
      - prompt-caching
      - context-window-management
      - rag-implementation
    workflow: |
      1. Analyze query patterns
      2. Implement prompt caching for stable prefixes
      3. Add response caching for frequent queries
      4. Consider CAG for stable document sets
      5. Monitor and optimize hit rates
