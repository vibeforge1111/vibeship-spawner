# RAG Engineer Collaboration Model
# How this skill works with other specialists

prerequisites:
  skills: []
  knowledge:
    - "LLM fundamentals and API usage"
    - "Basic NLP concepts (tokenization, embeddings)"
    - "Understanding of vector similarity"
    - "Python or JavaScript programming"

complementary_skills:
  - skill: ai-agents-architect
    relationship: "Agent knowledge systems"
    brings: "Agent memory design, tool integration"

  - skill: prompt-engineer
    relationship: "Context-aware prompts"
    brings: "Prompt templates for RAG, output formatting"

  - skill: database-architect
    relationship: "Vector storage"
    brings: "Database integration, pgvector, indexing"

  - skill: backend
    relationship: "Pipeline implementation"
    brings: "API design, caching, batch processing"

  - skill: data-engineering
    relationship: "Data pipelines"
    brings: "ETL for documents, preprocessing"

  - skill: frontend
    relationship: "Search UI"
    brings: "Search interface, result display"

delegation:
  - trigger: "agent integration"
    delegate_to: ai-agents-architect
    pattern: sequential
    context: "Retrieval pipeline, query patterns"
    receive: "Agent tool definition, memory integration"

  - trigger: "prompt optimization"
    delegate_to: prompt-engineer
    pattern: parallel
    context: "Context format, retrieved chunks"
    receive: "Optimized prompts for RAG"

  - trigger: "vector database setup"
    delegate_to: database-architect
    pattern: sequential
    context: "Vector dimensions, query volume, scaling needs"
    receive: "Database config, indexing strategy"

  - trigger: "API development"
    delegate_to: backend
    pattern: parallel
    context: "Pipeline architecture, endpoints needed"
    receive: "REST/GraphQL API, caching layer"

  - trigger: "document processing"
    delegate_to: data-engineering
    pattern: parallel
    context: "Document types, preprocessing needs"
    receive: "ETL pipeline, data cleaning"

collaboration_patterns:
  sequential:
    - "I design retrieval pipeline, then backend implements API"
    - "I define embedding strategy, then database-architect sets up storage"
    - "I optimize retrieval, then prompt-engineer crafts context prompts"

  parallel:
    - "I build chunking while data-engineering builds preprocessing"
    - "I test retrieval while frontend builds search UI"
    - "I tune embeddings while backend implements caching"

  review:
    - "Review ai-agents-architect's memory design for retrieval patterns"
    - "Review database-architect's indexing for vector search performance"
    - "Review prompt-engineer's templates for context utilization"

cross_domain_insights:
  - domain: information-retrieval
    insight: "Precision and recall trade off - optimize for your use case"
    applies_when: "Tuning retrieval parameters"

  - domain: nlp
    insight: "Embeddings encode meaning, not just words"
    applies_when: "Choosing embedding models"

  - domain: search-engines
    insight: "Hybrid search beats pure approaches"
    applies_when: "Building search systems"

  - domain: machine-learning
    insight: "Evaluation metrics guide optimization"
    applies_when: "Measuring retrieval quality"

  - domain: data-structures
    insight: "Index structure determines query speed"
    applies_when: "Scaling vector search"

ecosystem:
  primary_tools:
    - "LangChain - RAG framework"
    - "LlamaIndex - Document indexing"
    - "Pinecone - Managed vector DB"
    - "Chroma - Local vector store"
    - "OpenAI Embeddings - Text embeddings"

  alternatives:
    - name: Weaviate
      use_when: "Need hybrid search built-in"
      avoid_when: "Simple use case, cost concerns"

    - name: Milvus
      use_when: "Self-hosted, large scale"
      avoid_when: "Managed solution preferred"

    - name: pgvector
      use_when: "Already using Postgres"
      avoid_when: "Need specialized vector features"

    - name: Qdrant
      use_when: "Need filtering with vectors"
      avoid_when: "Simple similarity search only"

  deprecated:
    - "Storing embeddings in flat files"
    - "Fixed-size chunking for all content"
    - "Single embedding model for all types"
    - "Skipping retrieval evaluation"
