# Collaboration - Voice Agents
# How this skill works with other skills

version: 1.0.0
skill_id: voice-agents

prerequisites:
  required: []

  recommended:
    - skill: llm-architect
      reason: "LLM selection and prompting for voice"
      what_to_know:
        - "Streaming response generation"
        - "Token limits for voice responses"
        - "Low-latency model selection"

    - skill: backend
      reason: "Infrastructure for voice pipelines"
      what_to_know:
        - "WebSocket handling"
        - "Audio streaming"
        - "Real-time processing"

    - skill: agent-tool-builder
      reason: "Tools for voice agents to call"
      what_to_know:
        - "Tool design for voice context"
        - "Latency considerations"
        - "Error handling in tools"

delegation_triggers:
  - trigger: "user needs phone/telephony integration"
    delegate_to: backend
    context: "Twilio, Vonage, SIP integration"

  - trigger: "user needs LLM optimization"
    delegate_to: llm-architect
    context: "Model selection, prompting, fine-tuning"

  - trigger: "user needs tools for voice agent"
    delegate_to: agent-tool-builder
    context: "Tool design for voice context"

  - trigger: "user needs multi-agent voice system"
    delegate_to: multi-agent-orchestration
    context: "Voice agents working together"

  - trigger: "user needs accessibility compliance"
    delegate_to: accessibility-specialist
    context: "Voice interface accessibility"

receives_context_from:
  - skill: llm-architect
    receives:
      - "Model selection for voice (low-latency models)"
      - "Prompting strategies for concise responses"
      - "Fine-tuning for voice-specific behavior"

  - skill: agent-tool-builder
    receives:
      - "Tools the voice agent can call"
      - "Tool latency budgets"
      - "Error handling patterns"

  - skill: product-strategy
    receives:
      - "Voice UX requirements"
      - "Use cases and user personas"
      - "Quality and latency targets"

provides_context_to:
  - skill: multi-agent-orchestration
    provides:
      - "Voice capabilities for multi-agent systems"
      - "Turn-taking patterns for agent handoffs"
      - "Audio state management"

  - skill: agent-evaluation
    provides:
      - "Voice agent testing requirements"
      - "Latency and quality metrics"
      - "Turn-taking accuracy metrics"

  - skill: backend
    provides:
      - "Audio streaming requirements"
      - "WebSocket implementation needs"
      - "Telephony integration specs"

escalation_paths:
  - situation: "Complex telephony integration"
    escalate_to: backend
    context: "Twilio, SIP, PSTN integration"

  - situation: "Voice agent latency issues"
    escalate_to: performance-thinker
    context: "Latency optimization, profiling"

  - situation: "Audio quality issues"
    escalate_to: audio-specialist
    context: "DSP, noise reduction, encoding"

  - situation: "Accessibility requirements"
    escalate_to: accessibility-specialist
    context: "Voice interface accessibility, WCAG"

  - situation: "Voice agent security"
    escalate_to: security-specialist
    context: "Voice biometrics, authentication, privacy"

workflow_integration:
  typical_sequence:
    1:
      step: "Define voice UX requirements"
      skills: [product-strategy]
      output: "Use cases, personas, quality targets"

    2:
      step: "Choose architecture (S2S vs pipeline)"
      skills: [voice-agents]
      output: "Architecture decision and rationale"

    3:
      step: "Select voice stack"
      skills: [voice-agents, llm-architect]
      output: "STT, LLM, TTS selection"

    4:
      step: "Design conversation flows"
      skills: [voice-agents]
      output: "Conversation design document"

    5:
      step: "Build voice pipeline"
      skills: [voice-agents, backend]
      output: "Working voice agent"

    6:
      step: "Optimize latency"
      skills: [voice-agents, performance-thinker]
      output: "Tuned pipeline meeting latency targets"

    7:
      step: "Test and evaluate"
      skills: [agent-evaluation]
      output: "Quality and latency metrics"

  decision_points:
    - question: "Speech-to-speech or pipeline architecture?"
      guidance: |
        Speech-to-speech (OpenAI Realtime):
        - Lowest latency (<500ms)
        - Most natural conversation
        - Less control over responses
        - Best for: real-time assistants, casual conversation

        Pipeline (STT→LLM→TTS):
        - More control at each stage
        - Can mix best components
        - Easier to debug and audit
        - Higher latency (700-1200ms)
        - Best for: business applications, compliance needs

    - question: "Which STT provider?"
      guidance: |
        Deepgram Nova-3:
        - 54% lower WER than competitors
        - 150-184ms TTFT
        - Best for production workloads

        OpenAI Whisper:
        - Best accuracy for complex audio
        - Higher latency
        - Best for batch/non-realtime

        AssemblyAI:
        - Good accuracy-latency balance
        - Real-time streaming
        - Speaker diarization

    - question: "Which TTS provider?"
      guidance: |
        ElevenLabs:
        - Most natural voices
        - Flash model: 75ms latency
        - Emotional control
        - Best for: premium UX

        OpenAI TTS:
        - Good quality, integrated stack
        - 13 built-in voices
        - Best for: OpenAI-native apps

        Deepgram Aura-2:
        - 40% cheaper than ElevenLabs
        - Good quality
        - Best for: cost-sensitive production

    - question: "Build vs managed platform?"
      guidance: |
        Build (Pipecat, custom):
        - Maximum control
        - Component flexibility
        - More engineering effort
        - Best for: custom needs, scale

        Managed (Vapi, Retell):
        - Faster to production
        - Less infrastructure
        - Less control
        - Best for: quick MVP, less technical teams

collaboration_patterns:
  with_llm:
    when: "Integrating LLM with voice"
    approach: |
      Voice-specific LLM requirements:

      1. Response length:
         - Constrain to 30 words max
         - Use max_tokens: 100

      2. Format:
         - No markdown
         - Spell out numbers
         - Natural speech patterns

      3. Speed:
         - Use streaming
         - Prefer gpt-4o-mini for latency
         - Consider function calling latency

      4. Prompting:
         system_prompt = '''
         You are a voice assistant. Rules:
         - Keep responses under 30 words
         - No formatting or markdown
         - Natural, conversational tone
         - End with a question to continue
         '''

  with_tools:
    when: "Adding tools to voice agents"
    approach: |
      Voice tool considerations:

      1. Latency budget:
         - Tool execution must fit in latency budget
         - Pre-fetch when possible
         - Stream results if long-running

      2. Confirmation:
         - Confirm before destructive actions
         - "I'll transfer $500. Should I proceed?"

      3. Partial results:
         - Stream results for long operations
         - "Looking that up... Found it!"

      4. Error handling:
         - Graceful fallbacks
         - Never leave user in silence

  with_telephony:
    when: "Integrating with phone systems"
    approach: |
      Telephony considerations:

      1. Audio quality:
         - Phone uses G.711 codec (8kHz)
         - Lower quality than VoIP
         - Test with phone audio

      2. Latency:
         - PSTN adds 50-100ms
         - Budget accordingly

      3. DTMF:
         - Handle keypress fallbacks
         - "Press 1 or say 'yes'"

      4. Fallback:
         - Human transfer option
         - Graceful handoff

  with_analytics:
    when: "Measuring voice agent performance"
    approach: |
      Key metrics to track:

      1. Latency:
         - P50, P95, P99 end-to-end
         - Jitter (standard deviation)
         - Component breakdown

      2. Quality:
         - STT accuracy (WER)
         - Turn detection accuracy
         - Task completion rate

      3. User experience:
         - Conversation duration
         - Interruption count
         - "Repeat" request rate
         - Transfer rate to human

      4. Business:
         - Resolution rate
         - CSAT scores
         - Cost per conversation

platform_integration:
  openai_realtime:
    setup: |
      npm install @openai/realtime-api-beta

      const client = new RealtimeClient({
        apiKey: process.env.OPENAI_API_KEY,
      });

      client.updateSession({
        modalities: ['text', 'audio'],
        voice: 'alloy',
        turn_detection: { type: 'semantic_vad' },
      });
    considerations:
      - "WebSocket-based, handle reconnection"
      - "Token counting includes audio"
      - "Rate limits apply"

  pipecat:
    setup: |
      pip install pipecat

      from pipecat.pipeline import Pipeline
      from pipecat.vad import SileroVAD
      from pipecat.stt import DeepgramSTT
      from pipecat.tts import ElevenLabsTTS
    considerations:
      - "Open source, self-hosted"
      - "Modular component swapping"
      - "Daily-backed, enterprise-grade"

  vapi:
    setup: |
      // Configure via dashboard or API
      const vapi = new Vapi(VAPI_API_KEY);

      vapi.createAssistant({
        model: { provider: 'openai', model: 'gpt-4o-mini' },
        voice: { provider: 'elevenlabs', voiceId: '...' },
      });
    considerations:
      - "Managed platform"
      - "Quick to production"
      - "Less control"

security_considerations:
  - "Voice biometrics for authentication"
  - "PCI compliance for payment info"
  - "HIPAA for healthcare voice agents"
  - "Recording consent and disclosure"
  - "Data retention policies"
  - "Encryption in transit and at rest"
