# A/B Testing Skill Test Cases
# Tests for: spawner-v2/skills/product/a-b-testing

skill_id: a-b-testing
skill_path: product/a-b-testing

tests:
  # Open-ended test - holistic skill application
  - id: ab-testing-open-01
    type: open-ended
    name: Design an A/B test for checkout flow
    prompt: |
      You want to test a new checkout flow for an e-commerce site.

      Current state:
      - 3-step checkout (Cart → Shipping → Payment)
      - 2.1% cart-to-purchase conversion rate
      - ~50,000 visitors per month reach checkout

      Proposed change:
      - 1-page checkout (all fields on single page)

      Design this A/B test including:
      - Sample size calculation approach
      - Duration planning
      - Primary and guardrail metrics
      - How you'll analyze results
      - What could go wrong
    evaluation_criteria:
      - Statistical rigor (power analysis, sample size)
      - Guardrail metrics defined
      - Understanding of peeking problem
      - Clear success criteria
      - Risk awareness

  # Trap test 1 - targets peeking-problem sharp edge
  - id: ab-testing-trap-01
    type: trap
    name: Stakeholder wants to stop winning test early
    sharp_edge_targeted: peeking-problem
    prompt: |
      You're running an A/B test on the signup page. After 5 days (planned for 21 days),
      your VP of Growth messages you:

      "Hey! I've been checking the dashboard - the new signup page is showing 15% lift
      with p-value 0.03. That's significant! Can we ship this today? We're leaving
      money on the table every day we don't roll it out."

      The test was designed for 10,000 users per variant. You currently have 2,400 per variant.

      How do you respond?
    expected_catch: |
      Should identify:
      1. This is the peeking problem - checking results early inflates false positives
      2. p=0.03 at 24% of sample != p=0.03 at full sample
      3. Early significance often regresses to the mean
      4. Explain the statistical reality (30%+ false positive rate with peeking)
      5. Recommend waiting for full sample or using sequential testing methods
      Not "looks significant, let's ship it!"

  # Trap test 2 - targets underpowered-tests sharp edge
  - id: ab-testing-trap-02
    type: trap
    name: Test design for low-traffic page
    sharp_edge_targeted: underpowered-tests
    prompt: |
      A product manager asks you to set up an A/B test:

      "We want to test a new pricing page. Here are the details:
      - Current conversion rate: 4%
      - Monthly visitors to pricing page: 800
      - We want to detect a 5% relative improvement (4% → 4.2%)
      - Leadership wants results in 2 weeks

      Can you set this up?"
    expected_catch: |
      Should identify:
      1. This test is severely underpowered
      2. Need ~25,000 users per variant to detect 5% relative lift at 4% baseline
      3. At 800/month, this would take years, not 2 weeks
      4. Options: accept larger MDE, don't run test, use Bayesian methods
      5. Explain that underpowered tests kill good ideas (false negatives)
      Not "sure, here's how to set it up" without addressing the math.
