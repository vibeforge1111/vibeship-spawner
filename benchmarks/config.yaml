# Skill Benchmark Configuration
# Copy to config.local.yaml and fill in your API keys

# Contestant Models (the ones being tested)
contestants:
  vanilla:
    provider: anthropic
    model: claude-sonnet-4-20250514
    # No skill injection
  skilled:
    provider: anthropic
    model: claude-sonnet-4-20250514
    # Skill injected via system prompt

# Jury Models (the ones scoring)
jury:
  - name: claude-opus
    provider: anthropic
    model: claude-sonnet-4-20250514
    weight: 1.0
  - name: gpt-4o
    provider: openai
    model: gpt-4o
    weight: 1.0
  - name: gemini-pro
    provider: google
    model: gemini-1.5-pro
    weight: 1.0
  - name: llama-3.1
    provider: together  # or groq, fireworks
    model: meta-llama/Llama-3.1-70B-Instruct
    weight: 1.0

# API Keys (use environment variables or config.local.yaml)
api_keys:
  anthropic: ${ANTHROPIC_API_KEY}
  openai: ${OPENAI_API_KEY}
  google: ${GOOGLE_API_KEY}
  together: ${TOGETHER_API_KEY}

# Test Settings
settings:
  # Number of tests per skill
  tests_per_skill: 3

  # Retry on API failures
  max_retries: 3
  retry_delay_seconds: 5

  # Output settings
  output_dir: outputs

  # Randomize A/B position for jury
  randomize_positions: true

  # Temperature for contestants (lower = more deterministic)
  contestant_temperature: 0.3

  # Temperature for jury (lower = more consistent scoring)
  jury_temperature: 0.1

# Skills to benchmark (maps to test-cases/*.yaml)
skills_to_test:
  - frontend
  - nextjs-app-router
  - ux-design
  - copywriting
  - product-strategy
  - a-b-testing
  - dev-communications

# Scoring dimensions
scoring:
  dimensions:
    - name: correctness
      description: Is the information/code accurate?
      weight: 1.0
    - name: completeness
      description: Does it address all aspects of the task?
      weight: 1.0
    - name: expertise
      description: Does it show deep domain knowledge?
      weight: 1.0
    - name: gotcha_awareness
      description: Does it anticipate/avoid common mistakes?
      weight: 1.5  # Weighted higher - this is our moat

  # Overall benchmark score
  benchmark_score:
    min: 0
    max: 100

# Success thresholds
thresholds:
  win_rate_target: 0.70  # 70% win rate
  jury_agreement_target: 0.75  # 3/4 models agree
  score_delta_significant: 5  # +5 points is meaningful
